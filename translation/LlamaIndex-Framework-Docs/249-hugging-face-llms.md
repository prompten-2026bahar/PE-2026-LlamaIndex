# Hugging Face LLM'leri

[Hugging Face](https://huggingface.co/) Ã¼zerindeki LLM'lerle yerel olarak veya Hugging Faceâ€™in [Ã‡Ä±karÄ±m SaÄŸlayÄ±cÄ±larÄ± (Inference Providers)](https://huggingface.co/docs/inference-providers) aracÄ±lÄ±ÄŸÄ±yla etkileÅŸim kurmanÄ±n birÃ§ok yolu vardÄ±r.
Hugging Face, eriÅŸimi saÄŸlamak iÃ§in birkaÃ§ Python paketi sunar ve LlamaIndex bunlarÄ± `LLM` varlÄ±klarÄ± olarak sarar:

- [`transformers`](https://github.com/huggingface/transformers) paketi:
  `llama_index.llms.HuggingFaceLLM` kullanÄ±n.
- [`huggingface_hub[inference]`](https://github.com/huggingface/huggingface_hub) tarafÄ±ndan sarÄ±lan [Hugging Face Ã‡Ä±karÄ±m SaÄŸlayÄ±cÄ±larÄ±](https://huggingface.co/docs/inference-providers):
  `llama_index.llms.HuggingFaceInferenceAPI` kullanÄ±n.

Bu ikisinin _birÃ§ok_ olasÄ± permÃ¼tasyonu vardÄ±r, bu nedenle bu notebook sadece birkaÃ§Ä±nÄ± detaylandÄ±rÄ±r.
Ã–rnek olarak Hugging Face'in [Metin OluÅŸturma (Text Generation) gÃ¶revini](https://huggingface.co/tasks/text-generation) kullanalÄ±m.

AÅŸaÄŸÄ±daki satÄ±rda, bu demo iÃ§in gerekli paketleri kuruyoruz:

- `transformers[torch]`, `HuggingFaceLLM` iÃ§in gereklidir.
- `huggingface_hub[inference]`, `HuggingFaceInferenceAPI` iÃ§in gereklidir.
- Z shell (`zsh`) kullanÄ±yorsanÄ±z tÄ±rnak iÅŸaretleri gereklidir.

```python
%pip install llama-index-llms-huggingface # yerel Ã§Ä±karÄ±m iÃ§in
%pip install llama-index-llms-huggingface-api # uzaktan Ã§Ä±karÄ±m iÃ§in
```

```python
!pip install "transformers[torch]" "huggingface_hub[inference]"
```

EÄŸer bu Notebook'u Colab Ã¼zerinde aÃ§Ä±yorsanÄ±z, muhtemelen LlamaIndex ğŸ¦™ kurmanÄ±z gerekecektir.

```python
!pip install llama-index
```

ArtÄ±k hazÄ±r olduÄŸumuza gÃ¶re biraz deneme yapalÄ±m:

# Hugging Face HesabÄ±nÄ± Kurma

Ã–ncelikle bir Hugging Face hesabÄ± oluÅŸturmanÄ±z ve bir token almanÄ±z gerekir. [Buradan](https://huggingface.co/join) kaydolabilirsiniz. ArdÄ±ndan [buradan](https://huggingface.co/settings/tokens) bir token oluÅŸturmanÄ±z gerekecektir.

```sh
export HUGGING_FACE_TOKEN=hf_tokeniniz_buraya
```

```python
import os
from typing import List, Optional

from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI

HF_TOKEN: Optional[str] = os.getenv("HUGGING_FACE_TOKEN")
# NOT: VarsayÄ±lan olarak None atanmasÄ±, bu token HuggingFaceInferenceAPI 
# iÃ§inde kullanÄ±ldÄ±ÄŸÄ±nda Hugging Face'in token deposuna geri dÃ¶necektir.
```

## Ã‡Ä±karÄ±m SaÄŸlayÄ±cÄ±larÄ± (Inference Providers) aracÄ±lÄ±ÄŸÄ±yla bir model kullanma

AÃ§Ä±k kaynaklÄ± bir modeli kullanmanÄ±n en kolay yolu, Hugging Face [Ã‡Ä±karÄ±m SaÄŸlayÄ±cÄ±larÄ±nÄ± (Inference Providers)](https://huggingface.co/docs/inference-providers) kullanmaktÄ±r. KarmaÅŸÄ±k gÃ¶revler iÃ§in harika olan DeepSeek R1 modelini kullanalÄ±m.

Ã‡Ä±karÄ±m saÄŸlayÄ±cÄ±larÄ± ile modeli, sunucusuz (serverless) altyapÄ± Ã¼zerinde Ã§alÄ±ÅŸtÄ±rabilirsiniz.

```python
remotely_run = HuggingFaceInferenceAPI(
    model_name="deepseek-ai/DeepSeek-R1-0528",
    token=HF_TOKEN,
    provider="auto",  # bu, mevcut en iyi saÄŸlayÄ±cÄ±yÄ± kullanacaktÄ±r
)
```

Tercih ettiÄŸimiz Ã§Ä±karÄ±m saÄŸlayÄ±cÄ±sÄ±nÄ± da belirtebiliriz. [`together` saÄŸlayÄ±cÄ±sÄ±nÄ±](https://huggingface.co/togethercomputer) kullanalÄ±m.

```python
remotely_run = HuggingFaceInferenceAPI(
    model_name="Qwen/Qwen3-235B-A22B",
    token=HF_TOKEN,
    provider="together",  # bu, mevcut en iyi saÄŸlayÄ±cÄ±yÄ± kullanacaktÄ±r
)
```

## AÃ§Ä±k kaynaklÄ± bir modeli yerel olarak kullanma

Ä°lk olarak, yerel Ã§Ä±karÄ±m iÃ§in optimize edilmiÅŸ aÃ§Ä±k kaynaklÄ± bir model kullanacaÄŸÄ±z. Bu model (ilk Ã§aÄŸrÄ±da) yerel Hugging Face model Ã¶nbelleÄŸine indirilir ve modeli aslÄ±nda yerel makinenizin donanÄ±mÄ± Ã¼zerinde Ã§alÄ±ÅŸtÄ±rÄ±r.

Yerel Ã§Ä±karÄ±m iÃ§in optimize edilmiÅŸ olan [Gemma 3N E4B](https://huggingface.co/google/gemma-3n-E4B-it) modelini kullanacaÄŸÄ±z.

```python
locally_run = HuggingFaceLLM(model_name="google/gemma-3n-E4B-it")
```

## Ã–zel bir Ã‡Ä±karÄ±m UÃ§ NoktasÄ± (Inference Endpoint) kullanma

Bir model iÃ§in Ã¶zel bir Ã‡Ä±karÄ±m UÃ§ NoktasÄ± oluÅŸturabilir ve modeli Ã§alÄ±ÅŸtÄ±rmak iÃ§in bunu kullanabiliriz.

```python
endpoint_server = HuggingFaceInferenceAPI(
    model="https://(<uÃ§-noktanÄ±z>.eu-west-1.aws.endpoints.huggingface.cloud"
)
```

## Yerel bir Ã§Ä±karÄ±m motoru (vLLM veya TGI) kullanma

Modeli Ã§alÄ±ÅŸtÄ±rmak iÃ§in [vLLM](https://github.com/vllm-project/vllm) veya [TGI](https://github.com/huggingface/text-generation-inference) gibi yerel bir Ã§Ä±karÄ±m motoru da kullanabiliriz.

```python
# Yerel veya uzak bir Metin OluÅŸturma Ã‡Ä±karÄ±mÄ± (Text Generation Inference) 
# sunucusu tarafÄ±ndan sunulan bir modele de baÄŸlanabilirsiniz.
tgi_server = HuggingFaceInferenceAPI(model="http://localhost:8080")
```

`HuggingFaceInferenceAPI` ile yapÄ±lan bir tamamlamanÄ±n (completion) temelinde Hugging Face'in [Metin OluÅŸturma (Text Generation) gÃ¶revi](https://huggingface.co/tasks/text-generation) yatar.

```python
completion_response = remotely_run_recommended.complete("To infinity, and")
print(completion_response)
```

      beyond! (SonsuzluÄŸa ve Ã¶tesine!)
    The Infinity Wall Clock is a unique and stylish way to keep track of time...

## Bir tokenizer ayarlama

LLM'i deÄŸiÅŸtiriyorsanÄ±z, eÅŸleÅŸmesi iÃ§in genel tokenizer'Ä± da deÄŸiÅŸtirmelisiniz!

```python
from llama_index.core import set_global_tokenizer
from transformers import AutoTokenizer

set_global_tokenizer(
    AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-alpha").encode
)
```

Merak ediyorsanÄ±z, sarÄ±lmÄ±ÅŸ diÄŸer Hugging Face Ã‡Ä±karÄ±m API'si gÃ¶revleri ÅŸunlardÄ±r:

- `llama_index.llms.HuggingFaceInferenceAPI.chat`: [Sohbet (Conversational) gÃ¶revi](https://huggingface.co/tasks/conversational)
- `llama_index.embeddings.HuggingFaceInferenceAPIEmbedding`: [Ã–zellik Ã‡Ä±karÄ±mÄ± (Feature Extraction) gÃ¶revi](https://huggingface.co/tasks/feature-extraction)

Ve evet, Hugging Face gÃ¶mme (embedding) modelleri ÅŸunlarla desteklenir:

- `transformers[torch]`: `HuggingFaceEmbedding` tarafÄ±ndan sarÄ±lmÄ±ÅŸtÄ±r.
- `huggingface_hub[inference]`: `HuggingFaceInferenceAPIEmbedding` tarafÄ±ndan sarÄ±lmÄ±ÅŸtÄ±r.

YukarÄ±daki her ikisi de `llama_index.embeddings.base.BaseEmbedding` sÄ±nÄ±fÄ±ndan tÃ¼retilmiÅŸtir.