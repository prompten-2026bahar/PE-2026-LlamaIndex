# Metin G繹mme 覺kar覺m覺 (Text Embedding Inference)

Bu not defteri, `TextEmbeddingInference` g繹mmelerinin (embeddings) nas覺l yap覺land覺r覺laca覺n覺 g繹sterir.

襤lk ad覺m, g繹mme sunucusunu da覺tmakt覺r. Ayr覺nt覺l覺 talimatlar i癟in [Text Embeddings Inference resmi deposuna](https://github.com/huggingface/text-embeddings-inference) bak覺n. Eer Habana Gaudi/Gaudi 2 羹zerinde da覺t覺m yap覺yorsan覺z [tei-gaudi deposuna](https://github.com/huggingface/tei-gaudi) bak覺n.

Sunucu da覺t覺ld覺ktan sonra, aa覺daki kod 癟覺kar覺m (inference) i癟in sunucuya balanacak ve g繹mmeleri iletecektir.

Eer bu not defterini colab 羹zerinde a癟覺yorsan覺z, muhtemelen LlamaIndex'i  kurman覺z gerekecektir.

```python
%pip install llama-index-embeddings-text-embeddings-inference
```

```python
!pip install llama-index
```

```python
from llama_index.embeddings.text_embeddings_inference import (
    TextEmbeddingsInference,
)

embed_model = TextEmbeddingsInference(
    model_name="BAAI/bge-large-en-v1.5",  # 覺kar覺m metnini bi癟imlendirmek i癟in gereklidir,
    timeout=60,  # saniye cinsinden zaman a覺m覺
    embed_batch_size=10,  # g繹mme i癟in toplu ilem boyutu
)
```

```python
embeddings = embed_model.get_text_embedding("Merhaba D羹nya!")
print(len(embeddings))
print(embeddings[:5])
```

    1024
    [0.010597229, 0.05895996, 0.022445679, -0.012046814, -0.03164673]

```python
embeddings = await embed_model.aget_text_embedding("Merhaba D羹nya!")
print(len(embeddings))
print(embeddings[:5])
```

    1024
    [0.010597229, 0.05895996, 0.022445679, -0.012046814, -0.03164673]
