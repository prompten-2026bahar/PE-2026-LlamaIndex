# Ã‡ok Modlu (Multi-modal) Modeller

## Kavram

BÃ¼yÃ¼k dil modelleri (LLM'ler) "metin gir, metin al" ÅŸeklinde Ã§alÄ±ÅŸÄ±r. BÃ¼yÃ¼k Ã‡ok Modlu Modeller (LMM'ler) bunu metin haricindeki modaliteleri (tÃ¼rleri) de kapsayacak ÅŸekilde genelleÅŸtirir. Ã–rneÄŸin, GPT-4V gibi modeller hem gÃ¶rselleri hem de metni ortaklaÅŸa girdi olarak almanÄ±za ve metin Ã§Ä±ktÄ±sÄ± vermenize olanak tanÄ±r.

Metin+gÃ¶rsel modellerine izin vermek iÃ§in temel bir `MultiModalLLM` soyutlamasÄ± ekledik. **NOT**: Bu isimlendirme deÄŸiÅŸebilir!

## KullanÄ±m KalÄ±bÄ± (Usage Pattern)

1. AÅŸaÄŸÄ±daki kod parÃ§asÄ±, GPT-4V gibi LMM'leri kullanmaya nasÄ±l baÅŸlayabileceÄŸinizi gÃ¶sterir.

```python
from llama_index.multi_modal_llms.openai import OpenAIMultiModal
from llama_index.core.multi_modal_llms.generic_utils import load_image_urls
from llama_index.core import SimpleDirectoryReader

# gÃ¶rsel dÃ¶kÃ¼manlarÄ±nÄ± URL'lerden yÃ¼kle
image_documents = load_image_urls(image_urls)

# gÃ¶rsel dÃ¶kÃ¼manlarÄ±nÄ± yerel dizinden yÃ¼kle
image_documents = SimpleDirectoryReader(yerel_dizin).load_data()

# akÄ±ÅŸsÄ±z (non-streaming)
openai_mm_llm = OpenAIMultiModal(
    model="gpt-4-vision-preview", api_key=OPENAI_API_KEY, max_new_tokens=300
)
response = openai_mm_llm.complete(
    prompt="gÃ¶rselde ne var?", image_documents=image_documents
)
```

2. AÅŸaÄŸÄ±daki kod parÃ§asÄ±, Ã‡ok Modlu VektÃ¶r DepolarÄ±/Ä°ndeksleri nasÄ±l oluÅŸturabileceÄŸinizi gÃ¶sterir.

```python
from llama_index.core.indices import MultiModalVectorStoreIndex
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.core import SimpleDirectoryReader, StorageContext

import qdrant_client
from llama_index.core import SimpleDirectoryReader

# Yerel bir Qdrant vektÃ¶r deposu oluÅŸtur
client = qdrant_client.QdrantClient(path="qdrant_mm_db")

# eÄŸer gÃ¶rsel getirme iÃ§in sadece image_store'a ihtiyacÄ±nÄ±z varsa,
# text_store'u kaldÄ±rabilirsiniz
text_store = QdrantVectorStore(
    client=client, collection_name="metin_koleksiyonu"
)
image_store = QdrantVectorStore(
    client=client, collection_name="gorsel_koleksiyonu"
)

storage_context = StorageContext.from_defaults(
    vector_store=text_store, image_store=image_store
)

# Yerel klasÃ¶rden metin ve gÃ¶rsel dÃ¶kÃ¼manlarÄ±nÄ± yÃ¼kle
documents = SimpleDirectoryReader("./veri_klasoru/").load_data()
# Ã‡ok Modlu indeksi oluÅŸtur
index = MultiModalVectorStoreIndex.from_documents(
    documents,
    storage_context=storage_context,
)
```

3. AÅŸaÄŸÄ±daki kod parÃ§asÄ±, Ã‡ok Modlu Retriever ve Sorgu Motorunu (Query Engine) nasÄ±l kullanabileceÄŸinizi gÃ¶sterir.

```python
from llama_index.multi_modal_llms.openai import OpenAIMultiModal
from llama_index.core import PromptTemplate
from llama_index.core.query_engine import SimpleMultiModalQueryEngine

retriever_engine = index.as_retriever(
    similarity_top_k=3, image_similarity_top_k=3
)

# GPT4V yanÄ±tÄ±ndan daha fazla bilgi getir
retrieval_results = retriever_engine.retrieve(response)

# eÄŸer metin getirme olmadan sadece gÃ¶rsel getirmeye ihtiyacÄ±nÄ±z varsa
# `text_to_image_retrieve` kullanabilirsiniz
# retrieval_results = retriever_engine.text_to_image_retrieve(response)

qa_tmpl_str = (
    "BaÄŸlam bilgisi aÅŸaÄŸÄ±dadÄ±r.\n"
    "---------------------\n"
    "{context_str}\n"
    "---------------------\n"
    "BaÄŸlam bilgisini kullanarak ve Ã¶nceden bildiklerinize dayanmadan, "
    "sorguyu yanÄ±tlayÄ±n.\n"
    "Sorgu: {query_str}\n"
    "YanÄ±t: "
)
qa_tmpl = PromptTemplate(qa_tmpl_str)

query_engine = index.as_query_engine(
    multi_modal_llm=openai_mm_llm, text_qa_template=qa_tmpl
)

query_str = "Bana Porsche hakkÄ±nda daha fazla bilgi ver"
response = query_engine.query(query_str)
```

**GÃ¶sterge**

-   âœ… = sorunsuz Ã§alÄ±ÅŸmalÄ±
-   âš ï¸ = bazen gÃ¼venilmezdir, iyileÅŸtirilmesi iÃ§in daha fazla ayarlama gerekebilir
-   ğŸ›‘ = ÅŸu an iÃ§in mevcut deÄŸil.

### UÃ§tan Uca Ã‡ok Modlu Ä°ÅŸ AkÄ±ÅŸÄ±

AÅŸaÄŸÄ±daki tablolar, kendi Ã‡ok Modlu RAG'lerinizi (Getirme ile GÃ¼Ã§lendirilmiÅŸ Ãœretim) oluÅŸturmak iÃ§in Ã§eÅŸitli LlamaIndex Ã¶zelliklerini kullanan **baÅŸlangÄ±Ã§** adÄ±mlarÄ±nÄ± gÃ¶stermeyi amaÃ§lamaktadÄ±r. Kendi Ã‡ok Modlu RAG orkestrasyonunuzu oluÅŸturmak iÃ§in farklÄ± modÃ¼lleri/adÄ±mlarÄ± bir araya getirebilirsiniz.

| Sorgu TÃ¼rÃ¼ | Ã‡ok Modlu VektÃ¶r Deposu/Ä°ndeksi iÃ§in Veri KaynaklarÄ± | Ã‡ok Modlu Embedding | Retriever | Sorgu Motoru | Ã‡Ä±ktÄ± Veri TÃ¼rÃ¼ |
| :--- | :--- | :--- | :--- | :--- | :--- |
| Metin âœ… | Metin âœ… | Metin âœ… | Top-k getirme âœ…<br>Basit FÃ¼zyon (Simple Fusion) getirme âœ… | Basit Sorgu Motoru âœ… | Getirilen Metin âœ…<br>OluÅŸturulan Metin âœ… |
| GÃ¶rsel âœ… | GÃ¶rsel âœ… | GÃ¶rsel âœ…<br>GÃ¶rselden Metne Embedding âœ… | Top-k getirme âœ…<br>Basit FÃ¼zyon (Simple Fusion) getirme âœ… | Basit Sorgu Motoru âœ… | Getirilen GÃ¶rsel âœ…<br>OluÅŸturulan GÃ¶rsel ğŸ›‘ |
| Ses ğŸ›‘ | Ses ğŸ›‘ | Ses ğŸ›‘ | ğŸ›‘ | ğŸ›‘ | Ses ğŸ›‘ |
| Video ğŸ›‘ | Video ğŸ›‘ | Video ğŸ›‘ | ğŸ›‘ | ğŸ›‘ | Video ğŸ›‘ |

### Ã‡ok Modlu LLM Modelleri

Bu not defterleri; Ã‡ok Modlu RAG orkestrasyonu oluÅŸturmak iÃ§in Ã‡ok Modlu LLM modelini, Ã‡ok Modlu embedding'leri, Ã‡ok Modlu vektÃ¶r depolarÄ±nÄ±, Retriever'Ä± ve Sorgu motorunu nasÄ±l kullanabileceÄŸinize ve entegre edebileceÄŸinize dair Ã¶rnekler sunar.

| Ã‡ok Modlu GÃ¶rÃ¼ Modelleri | Tek GÃ¶rsel Muhakemesi | Ã‡oklu GÃ¶rsel Muhakemesi | GÃ¶rsel Embedding'leri | Basit Sorgu Motoru | Pydantic YapÄ±landÄ±rÄ±lmÄ±ÅŸ Ã‡Ä±ktÄ± |
| :--- | :--- | :--- | :--- | :--- | :--- |
| [GPT4V](/python/examples/multi_modal/gpt4v_multi_modal_retrieval)<br>(OpenAI API) | âœ… | âœ… | ğŸ›‘ | âœ… | âœ… |
| [GPT4V-Azure](/python/examples/multi_modal/azure_openai_multi_modal)<br>(Azure API) | âœ… | âœ… | ğŸ›‘ | âœ… | âœ… |
| [Gemini](/python/examples/multi_modal/gemini)<br>(Google) | âœ… | âœ… | ğŸ›‘ | âœ… | âœ… |
| [CLIP](/python/examples/multi_modal/image_to_image_retrieval)<br>(Yerel host) | ğŸ›‘ | ğŸ›‘ | âœ… | ğŸ›‘ | ğŸ›‘ |
| [LLaVa](/python/examples/multi_modal/llava_multi_modal_tesla_10q)<br>(replicate) | âœ… | ğŸ›‘ | ğŸ›‘ | âœ… | âš ï¸ |
| [Fuyu-8B](/python/examples/multi_modal/replicate_multi_modal)<br>(replicate) | âœ… | ğŸ›‘ | ğŸ›‘ | âœ… | âš ï¸ |
| [ImageBind<br>](https://imagebind.metademolab.com/)[Entegre edilecek] | ğŸ›‘ | ğŸ›‘ | âœ… | ğŸ›‘ | ğŸ›‘ |
| [MiniGPT-4<br>](/python/examples/multi_modal/replicate_multi_modal) | âœ… | ğŸ›‘ | ğŸ›‘ | âœ… | âš ï¸ |
| [CogVLM<br>](https://github.com/THUDM/CogVLM) | âœ… | ğŸ›‘ | ğŸ›‘ | âœ… | âš ï¸ |
| [Qwen-VL<br>](https://arxiv.org/abs/2308.12966)[Entegre edilecek] | âœ… | ğŸ›‘ | ğŸ›‘ | âœ… | âš ï¸ |

### Ã‡ok Modlu VektÃ¶r DepolarÄ±

AÅŸaÄŸÄ±daki tablo, Ã‡ok Modlu kullanÄ±m durumlarÄ±nÄ± destekleyen bazÄ± vektÃ¶r depolarÄ±nÄ± listeler. LlamaIndex bÃ¼nyesindeki `MultiModalVectorStoreIndex`, gÃ¶rsel ve metin embedding vektÃ¶r depolarÄ± iÃ§in ayrÄ± vektÃ¶r depolarÄ± oluÅŸturmayÄ± destekler. `MultiModalRetriever` ve `SimpleMultiModalQueryEngine`; metinden metne/gÃ¶rsele ve gÃ¶rselden gÃ¶rsele getirmeyi ve metin ile gÃ¶rsel getirme sonuÃ§larÄ±nÄ± birleÅŸtirmek iÃ§in basit sÄ±ralama fÃ¼zyon fonksiyonlarÄ±nÄ± destekler.

| Ã‡ok Modlu VektÃ¶r DepolarÄ± | Tek VektÃ¶r Deposu | Ã‡oklu VektÃ¶r Deposu | Metin Embedding | GÃ¶rsel Embedding |
| :--- | :--- | :--- | :--- | :--- |
| [LlamaIndex Kendi YapÄ±mÄ± Ã‡ok Modlu Ä°ndeks](/python/examples/multi_modal/gpt4v_multi_modal_retrieval) | ğŸ›‘ | âœ… | Herhangi bir metin embedding olabilir (VarsayÄ±lan GPT3.5) | Herhangi bir gÃ¶rsel embedding olabilir (VarsayÄ±lan CLIP) |
| [Chroma](/python/examples/multi_modal/chromamultimodaldemo) | âœ… | ğŸ›‘ | CLIP âœ… | CLIP âœ… |
| [Weaviate](https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/multi2vec-bind)<br>[Entegre edilecek] | âœ… | ğŸ›‘ | CLIP âœ…<br>ImageBind âœ… | CLIP âœ…<br>ImageBind âœ… |

## Ã‡ok Modlu LLM ModÃ¼lleri

GPT4-V, Anthropic (Opus, Sonnet), Gemini (Google), CLIP (OpenAI), BLIP (Salesforce) ve Replicate (LLaVA, Fuyu-8B, MiniGPT-4, CogVLM) ve daha fazlasÄ±yla entegrasyonlarÄ± destekliyoruz.

-   [OpenAI](/python/examples/multi_modal/openai_multi_modal)
-   [Gemini](/python/examples/multi_modal/gemini)
-   [Anthropic](/python/examples/multi_modal/anthropic_multi_modal)
-   [Replicate](/python/examples/multi_modal/replicate_multi_modal)
-   [Pydantic Ã‡ok Modlu](/python/examples/multi_modal/multi_modal_pydantic)
-   [GPT-4v COT Deneyleri](/python/examples/multi_modal/gpt4v_experiments_cot)
-   [Llava Tesla 10q](/python/examples/multi_modal/llava_multi_modal_tesla_10q)

## Ã‡ok Modlu Getirme ile GÃ¼Ã§lendirilmiÅŸ Ãœretim (RAG)

FarklÄ± Ã‡ok Modlu LLM'ler ve Ã‡ok Modlu vektÃ¶r depolarÄ± ile Ã‡ok Modlu Getirme ile GÃ¼Ã§lendirilmiÅŸ Ãœretim desteÄŸi sunuyoruz.

-   [GPT-4v Getirme](/python/examples/multi_modal/gpt4v_multi_modal_retrieval)
-   [Ã‡ok Modlu Getirme](/python/examples/multi_modal/multi_modal_retrieval)
-   [GÃ¶rselden GÃ¶rsele Getirme](/python/examples/multi_modal/image_to_image_retrieval)
-   [Chroma Ã‡ok Modlu](/python/examples/multi_modal/chromamultimodaldemo)

## DeÄŸerlendirme

Ã‡ok Modlu LLM ve Getirme ile GÃ¼Ã§lendirilmiÅŸ Ãœretim iÃ§in temel deÄŸerlendirme desteÄŸi sunuyoruz.

-   [Ã‡ok Modlu RAG DeÄŸerlendirmesi](/python/examples/evaluation/multi_modal/multi_modal_rag_evaluation)