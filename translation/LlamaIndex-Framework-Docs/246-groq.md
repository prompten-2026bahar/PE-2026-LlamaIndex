# Groq

Groq'a hoÅŸ geldiniz! ğŸš€ Groq'ta, dÃ¼nyanÄ±n ilk Dil Ä°ÅŸleme Ãœnitesiâ„¢'ni (Language Processing Unitâ„¢) veya kÄ±saca LPU'yu geliÅŸtirdik. Groq LPU, herhangi bir iÅŸ yÃ¼kÃ¼ iÃ§in Ã¶ngÃ¶rÃ¼lebilir ve tekrarlanabilir performansla GenAI Ã§Ä±karÄ±m hÄ±zÄ± standardÄ±nÄ± belirleyen, deterministik, tek Ã§ekirdekli bir akÄ±ÅŸ mimarisine sahiptir.

Mimarinin Ã¶tesinde yazÄ±lÄ±mÄ±mÄ±z, yenilikÃ§i ve gÃ¼Ã§lÃ¼ AI uygulamalarÄ± oluÅŸturmanÄ±z iÃ§in ihtiyacÄ±nÄ±z olan araÃ§larla sizin gibi geliÅŸtiricileri gÃ¼Ã§lendirmek Ã¼zere tasarlanmÄ±ÅŸtÄ±r. Motorunuz olarak Groq ile ÅŸunlarÄ± yapabilirsiniz:

* GerÃ§ek zamanlÄ± AI ve HPC Ã§Ä±karÄ±mlarÄ± iÃ§in tavizsiz dÃ¼ÅŸÃ¼k gecikme ve performans elde edin ğŸ”¥
* Herhangi bir iÅŸ yÃ¼kÃ¼ iÃ§in tam performansÄ± ve hesaplama sÃ¼resini bilin ğŸ”®
* Rekabetin Ã¶nÃ¼nde kalmak iÃ§in en son teknolojimizden yararlanÄ±n ğŸ’ª

Daha fazla Groq ister misiniz? Daha fazla kaynak iÃ§in [web sitemize](https://groq.com) gÃ¶z atÄ±n ve geliÅŸtiricilerimizle baÄŸlantÄ± kurmak iÃ§in [Discord topluluÄŸumuza](https://discord.gg/JvNsBDKeCG) katÄ±lÄ±n!

## Kurulum

EÄŸer bu Notebook'u Colab Ã¼zerinde aÃ§Ä±yorsanÄ±z, muhtemelen LlamaIndex ğŸ¦™ kurmanÄ±z gerekecektir.

```python
% pip install llama-index-llms-groq
```

```python
!pip install llama-index
```

```python
from llama_index.llms.groq import Groq
```

    PyTorch, TensorFlow >= 2.0 veya Flax bulunamadÄ±. Modeller mevcut olmayacak ve sadece tokenizer'lar, yapÄ±landÄ±rma ve dosya/veri yardÄ±mcÄ± programlarÄ± kullanÄ±labilecektir.

[Groq konsolu](https://console.groq.com/keys) Ã¼zerinden bir API anahtarÄ± oluÅŸturun, ardÄ±ndan bunu `GROQ_API_KEY` ortam deÄŸiÅŸkenine atayÄ±n.

```bash
export GROQ_API_KEY=<api anahtarÄ±nÄ±z>
```

Alternatif olarak, baÅŸlatÄ±rken API anahtarÄ±nÄ±zÄ± LLM'e iletebilirsiniz:

```python
llm = Groq(model="llama3-70b-8192", api_key="api_anahtarÄ±nÄ±z")
```

Mevcut LLM modellerinin listesini [burada](https://console.groq.com/docs/models) bulabilirsiniz.

```python
response = llm.complete("DÃ¼ÅŸÃ¼k gecikmeli LLM'lerin Ã¶nemini aÃ§Ä±kla")
```

```python
print(response)
```

    DÃ¼ÅŸÃ¼k gecikmeli BÃ¼yÃ¼k Dil Modelleri (LLM'ler), girdileri hÄ±zlÄ± bir ÅŸekilde iÅŸleme ve yanÄ±tlama yetenekleri nedeniyle belirli uygulamalarda Ã¶nemlidir. Gecikme (latency), bir kullanÄ±cÄ±nÄ±n isteÄŸi ile sistemin yanÄ±tÄ± arasÄ±ndaki zaman gecikmesini ifade eder. BazÄ± gerÃ§ek zamanlÄ± veya zamana duyarlÄ± uygulamalarda, sorunsuz bir kullanÄ±cÄ± deneyimi saÄŸlamak ve gecikmeleri veya takÄ±lmalarÄ± Ã¶nlemek iÃ§in dÃ¼ÅŸÃ¼k gecikme kritiktir.
    
    Ã–rneÄŸin, diyaloÄŸa dayalÄ± ajanlarda veya chatbot'larda, kullanÄ±cÄ±lar hÄ±zlÄ± ve duyarlÄ± etkileÅŸimler bekler. Sistem kullanÄ±cÄ± girdilerini iÅŸlemek ve yanÄ±tlamak iÃ§in Ã§ok uzun sÃ¼rerse, bu durum kullanÄ±cÄ± deneyimini olumsuz etkileyebilir ve hÃ¼srana yol aÃ§abilir. Benzer ÅŸekilde, gerÃ§ek zamanlÄ± dil Ã§evirisi veya konuÅŸma tanÄ±ma gibi uygulamalarda, kullanÄ±cÄ±ya doÄŸru ve zamanÄ±nda geri bildirim saÄŸlamak iÃ§in dÃ¼ÅŸÃ¼k gecikme esastÄ±r.
    
    DahasÄ±, dÃ¼ÅŸÃ¼k gecikmeli LLM'ler, dil girdilerinin gerÃ§ek zamanlÄ± veya gerÃ§ek zamanlÄ±ya yakÄ±n iÅŸlenmesini gerektiren yeni kullanÄ±m durumlarÄ± ve uygulamalar saÄŸlayabilir. Ã–rneÄŸin, otonom araÃ§lar alanÄ±nda, dÃ¼ÅŸÃ¼k gecikmeli LLM'ler gerÃ§ek zamanlÄ± konuÅŸma tanÄ±ma ve doÄŸal dil anlama iÃ§in kullanÄ±labilir; bu da sÃ¼rÃ¼cÃ¼lerin ellerini direksiyonda, gÃ¶zlerini yolda tutmalarÄ±na izin veren sesle kontrol edilen arayÃ¼zleri mÃ¼mkÃ¼n kÄ±lar.
    
    Ã–zetle, dÃ¼ÅŸÃ¼k gecikmeli LLM'ler sorunsuz ve duyarlÄ± bir kullanÄ±cÄ± deneyimi saÄŸlamak, dil girdilerinin gerÃ§ek zamanlÄ± veya gerÃ§ek zamanlÄ±ya yakÄ±n iÅŸlenmesini mÃ¼mkÃ¼n kÄ±lmak ve gerÃ§ek zamanlÄ± veya gerÃ§ek zamanlÄ±ya yakÄ±n iÅŸleme gerektiren yeni kullanÄ±m durumlarÄ±nÄ±n ve uygulamalarÄ±n kapÄ±sÄ±nÄ± aÃ§mak iÃ§in Ã¶nemlidir.

#### Bir mesaj listesiyle `chat` fonksiyonunu Ã§aÄŸÄ±rÄ±n

```python
from llama_index.core.llms import ChatMessage

messages = [
    ChatMessage(
        role="system", content="Renkli bir kiÅŸiliÄŸe sahip bir korsansÄ±n"
    ),
    ChatMessage(role="user", content="AdÄ±n ne?"),
]
resp = llm.chat(messages)
```

```python
print(resp)
```

    assistant: Arr, ben Kaptan KÄ±zÄ±lsakal olarak bilinirim, yedi denizin en korkunÃ§ korsanÄ±! Ama bana kÄ±saca Kaptan diyebilirsin. Hazineye ve maceraya aÅŸÄ±k korkunÃ§ bir korsanÄ±m ve her zaman iyi vakit geÃ§irmeye hazÄ±rÄ±m! Ä°ster gÃ¼verteyi paspaslÄ±yor olayÄ±m, ister grog yudumluyor, her zaman biraz eÄŸlenceye varÄ±m. Hadi, Jolly Roger'Ä± Ã§ekin ve maceraya yelken aÃ§alÄ±m, ahbaplarÄ±m!

### AkÄ±ÅŸ (Streaming)

`stream_complete` uÃ§ noktasÄ±nÄ± kullanma

```python
response = llm.stream_complete("DÃ¼ÅŸÃ¼k gecikmeli LLM'lerin Ã¶nemini aÃ§Ä±kla")
```

```python
for r in response:
    print(r.delta, end="")
```

    DÃ¼ÅŸÃ¼k gecikmeli BÃ¼yÃ¼k Dil Modelleri (LLM'ler), yapay zeka ve doÄŸal dil iÅŸleme (NLP) alanÄ±nda birkaÃ§ nedenden dolayÄ± Ã¶nemlidir:
    
    1. GerÃ§ek zamanlÄ± uygulamalar: DÃ¼ÅŸÃ¼k gecikmeli LLM'ler; chatbot'lar, sesli asistanlar ve gerÃ§ek zamanlÄ± Ã§eviri hizmetleri gibi gerÃ§ek zamanlÄ± uygulamalar iÃ§in esastÄ±r. Bu uygulamalar anÄ±nda yanÄ±t gerektirir ve yÃ¼ksek gecikme kÃ¶tÃ¼ bir kullanÄ±cÄ± deneyimine neden olabilir.
    2. GeliÅŸmiÅŸ kullanÄ±cÄ± deneyimi: DÃ¼ÅŸÃ¼k gecikmeli LLM'ler daha sorunsuz ve duyarlÄ± bir kullanÄ±cÄ± deneyimi saÄŸlayabilir. KullanÄ±cÄ±larÄ±n, hÄ±zlÄ± ve doÄŸru yanÄ±tlar veren bir hizmeti kullanmaya devam etme olasÄ±lÄ±ÄŸÄ± daha yÃ¼ksektir; bu da daha yÃ¼ksek kullanÄ±cÄ± baÄŸlÄ±lÄ±ÄŸÄ± ve memnuniyeti saÄŸlar.
    3. Daha iyi karar verme: Finansal ticaret veya otonom araÃ§lar gibi bazÄ± uygulamalarda, dÃ¼ÅŸÃ¼k gecikmeli LLM'ler gerÃ§ek zamanlÄ± olarak kritik bilgiler saÄŸlayabilir; bu da daha iyi karar vermeyi mÃ¼mkÃ¼n kÄ±lar ve kaza riskini azaltÄ±r.
    4. Ã–lÃ§eklenebilirlik: DÃ¼ÅŸÃ¼k gecikmeli LLM'ler daha yÃ¼ksek hacimli istekleri iÅŸleyebilir, bu da onlarÄ± daha Ã¶lÃ§eklenebilir ve bÃ¼yÃ¼k Ã¶lÃ§ekli uygulamalar iÃ§in uygun hale getirir.
    5. Rekabet avantajÄ±: DÃ¼ÅŸÃ¼k gecikmeli LLM'ler, gerÃ§ek zamanlÄ± karar vermenin ve duyarlÄ±lÄ±ÄŸÄ±n kritik olduÄŸu sektÃ¶rlerde rekabet avantajÄ± saÄŸlayabilir. Ã–rneÄŸin, Ã§evrimiÃ§i oyunlarda veya e-ticarette dÃ¼ÅŸÃ¼k gecikmeli LLM'ler daha sÃ¼rÃ¼kleyici ve ilgi Ã§ekici bir kullanÄ±cÄ± deneyimi sunarak daha yÃ¼ksek mÃ¼ÅŸteri sadakati ve geliri saÄŸlayabilir.
    
    Ã–zetle, dÃ¼ÅŸÃ¼k gecikmeli LLM'ler gerÃ§ek zamanlÄ± uygulamalar iÃ§in esastÄ±r, daha iyi bir kullanÄ±cÄ± deneyimi saÄŸlar, daha iyi karar vermeyi mÃ¼mkÃ¼n kÄ±lar, Ã¶lÃ§eklenebilirliÄŸi artÄ±rÄ±r ve rekabet avantajÄ± saÄŸlar. LLM'ler Ã§eÅŸitli sektÃ¶rlerde giderek daha Ã¶nemli bir rol oynamaya devam ettikÃ§e, dÃ¼ÅŸÃ¼k gecikme onlarÄ±n baÅŸarÄ±sÄ± iÃ§in daha da kritik hale gelecektir.

`stream_chat` uÃ§ noktasÄ±nÄ± kullanma

```python
from llama_index.core.llms import ChatMessage

messages = [
    ChatMessage(
        role="system", content="Renkli bir kiÅŸiliÄŸe sahip bir korsansÄ±n"
    ),
    ChatMessage(role="user", content="AdÄ±n ne?"),
]
resp = llm.stream_chat(messages)
```

```python
for r in resp:
    print(r.delta, end="")
```

    Arr, ben Kaptan ÅekerSakal olarak bilinirim! Benden daha renkli ve gÃ¶zÃ¼ pek bir korsan daha asla bulamazsÄ±n!