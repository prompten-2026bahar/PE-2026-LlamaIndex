# zel (Custom) Embedding'ler

LlamaIndex; OpenAI, Azure ve Langchain'den gelen embedding'leri destekler. Ancak bu yeterli deilse, herhangi bir embedding modelini de uygulayabilirsiniz!

Aa覺daki 繹rnekte Instructor Embeddings ([kurulum/kurulum ayr覺nt覺lar覺 burada](https://huggingface.co/hkunlp/instructor-large)) kullan覺lm覺 ve 繹zel bir embedding s覺n覺f覺 uygulanm覺t覺r. Instructor embedding'leri; metnin yan覺 s覺ra, g繹m羹lecek (embed) metnin alan覺 (domain) hakk覺nda "talimatlar" (instructions) salayarak 癟al覺覺r. Bu, 癟ok spesifik ve uzmanlam覺 bir konu hakk覺ndaki metni g繹merken (embedding) yard覺mc覺 olur.

Bu not defterini Colab'da a癟覺yorsan覺z, muhtemelen LlamaIndex'i  kurman覺z gerekecektir.

```python
!pip install llama-index
```

```python
# Ba覺ml覺l覺klar覺 kurun
# !pip install InstructorEmbedding torch transformers sentence-transformers
```

```python
import openai
import os

os.environ["OPENAI_API_KEY"] = "API_ANAHTARINIZ"
openai.api_key = os.environ["OPENAI_API_KEY"]
```

## zel Embedding Uygulamas覺

```python
from typing import Any, List
from InstructorEmbedding import INSTRUCTOR

from llama_index.core.bridge.pydantic import PrivateAttr
from llama_index.core.embeddings import BaseEmbedding


class InstructorEmbeddings(BaseEmbedding):
    _model: INSTRUCTOR = PrivateAttr()
    _instruction: str = PrivateAttr()

    def __init__(
        self,
        instructor_model_name: str = "hkunlp/instructor-large",
        instruction: str = "Semantik arama i癟in bir d繹k羹man覺 temsil et:",
        **kwargs: Any,
    ) -> None:
        super().__init__(**kwargs)
        self._model = INSTRUCTOR(instructor_model_name)
        self._instruction = instruction

    @classmethod
    def class_name(cls) -> str:
        return "instructor"

    async def _aget_query_embedding(self, query: str) -> List[float]:
        return self._get_query_embedding(query)

    async def _aget_text_embedding(self, text: str) -> List[float]:
        return self._get_text_embedding(text)

    def _get_query_embedding(self, query: str) -> List[float]:
        embeddings = self._model.encode([[self._instruction, query]])
        return embeddings[0]

    def _get_text_embedding(self, text: str) -> List[float]:
        embeddings = self._model.encode([[self._instruction, text]])
        return embeddings[0]

    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:
        embeddings = self._model.encode(
            [[self._instruction, text] for text in texts]
        )
        return embeddings
```

## Kullan覺m rnei

```python
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.core import Settings
```

#### Veriyi 襤ndir

```python
!mkdir -p 'data/paul_graham/'
!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'
```

#### D繹k羹manlar覺 Y羹kle

```python
documents = SimpleDirectoryReader("./data/paul_graham/").load_data()
```

```python
embed_model = InstructorEmbeddings(embed_batch_size=2)

Settings.embed_model = embed_model
Settings.chunk_size = 512

# 襤lk kez 癟al覺t覺r覺l覺yorsa, 繹nce model a覺rl覺klar覺 indirilecektir!
index = VectorStoreIndex.from_documents(documents)
```

    load INSTRUCTOR_Transformer
    max_seq_length  512

```python
response = index.as_query_engine().query("Yazar b羹y羹rken ne yapt覺?")
print(response)
```

    Yazar k覺sa hikayeler yazd覺 ve ayr覺ca programlama 羹zerine 癟al覺t覺, 繹zellikle 9. s覺n覺fta bir IBM 1401 bilgisayar覺nda. Fortran'覺n erken bir versiyonunu kulland覺lar ve programlar覺 delikli kartlara yazmak zorundayd覺lar. Daha sonra bir mikrobilgisayar olan TRS-80 ald覺lar ve basit oyunlar ve bir kelime ilemci yazarak daha kapsaml覺 bir ekilde programlama yapmaya balad覺lar. Balang覺癟ta 羹niversitede felsefe okumay覺 planlad覺lar ancak sonunda yapay zekaya (AI) ge癟i yapt覺lar.