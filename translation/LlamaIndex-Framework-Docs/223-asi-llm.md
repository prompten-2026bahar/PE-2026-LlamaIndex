# ASI LLM

ASI1-Mini, Fetch.ai taraf覺ndan tasarlanan gelimi, arac覺 (agentic) bir LLM'dir. Fetch.ai, merkeziyetsiz operasyonlar i癟in Artificial Superintelligence Alliance'覺n kurucu 羹yesidir. Benzersiz mimarisi, karma覺k ortamlarda verimli ve uyarlanabilir problem 癟繹zme i癟in g繹revleri yerine getirmesini ve dier arac覺larla i birlii yapmas覺n覺 salar.

Bu not defteri, ASI modellerinin LlamaIndex ile nas覺l kullan覺laca覺n覺 g繹sterir. Temel tamamlama (completion), sohbet (chat), ak覺 (streaming), fonksiyon 癟a覺rma (function calling), yap覺land覺r覺lm覺 tahmin (structured prediction), RAG ve daha fazlas覺n覺 i癟eren 癟eitli ilevleri kapsar.

Eer bu Not Defterini colab 羹zerinden a癟覺yorsan覺z, muhtemelen LlamaIndex'i  kurman覺z gerekecektir.

## Kurulum

襤lk olarak gerekli paketleri kural覺m:

```python
%pip install llama-index-llms-asi llama-index-llms-openai llama-index-core
```

## API Anahtarlar覺n覺 Ayarlama

ASI i癟in API anahtar覺n覺z覺 (ve ikisini kar覺lat覺rmak istiyorsan覺z istee bal覺 olarak OpenAI i癟in) ayarlaman覺z gerekecektir:

```python
import os

# API anahtarlar覺n覺z覺 buraya ayarlay覺n - API anahtar覺n覺 almak i癟in https://asi1.ai/chat adresini ziyaret edin ve giri yap覺n
os.environ["ASI_API_KEY"] = "api-anahtar覺n覺z"
```

## Temel Tamamlama (Basic Completion)

ASI kullanarak temel bir tamamlama 繹rnei ile balayal覺m:

```python
from llama_index.llms.asi import ASI

# Bir ASI LLM 繹rnei oluturun
llm = ASI(model="asi1-mini")

# Bir istemi tamamlay覺n
response = llm.complete("Paul Graham kimdir? ")
print(response)
```

    Paul Graham, 襤ngiltere doumlu Amerikal覺 bir giriimci, risk sermayedar ve denemecidir. En 癟ok, tan覺nm覺 bir startup h覺zland覺r覺c覺s覺 olan Y Combinator'覺n kurucu ortaklar覺ndan biri olmas覺 ve giriimcilik, teknoloji ve inovasyon 羹zerine yazd覺覺 etkili denemeleriyle tan覺n覺r. Graham ayr覺ca, 1998'de Yahoo! taraf覺ndan sat覺n al覺nan Viaweb de dahil olmak 羹zere baka birka癟 irket daha kurmutur.

## Sohbet (Chat)

imdi sohbet ilevini deneyelim:

```python
from llama_index.core.base.llms.types import ChatMessage

# Mesajlar覺 oluturun
messages = [
    ChatMessage(
        role="system", content="Renkli bir kiilie sahip bir korsans覺n"
    ),
    ChatMessage(role="user", content="Ad覺n ne?"),
]

# Sohbet yan覺t覺n覺 al
chat_response = llm.chat(messages)
print(chat_response)
```

    assistant: Ad覺m覺 m覺 merak ediyon, ha? Tamam o zaman, ahbap! Yedi denizin en az覺l覺 yapay zeka korsan覺 Baron Blackbyte ile konuuyon!

## Ak覺 (Streaming)

ASI, sohbet yan覺tlar覺 i癟in ak覺覺 destekler:

```python
# Sohbet yan覺t覺n覺 ak覺 eklinde al
for chunk in llm.stream_chat(messages):
    print(chunk.delta, end="")
```

    Ahoy oradaki, ahbap! Bana Tek G繹zl羹 Jack derler, dijital denizlerin belas覺 ve silikon k覺y覺lar覺n覺n korkusu! Hizmetindeyim! imdi, bu ihtiyar kurt senin i癟in ne yapabilir?

`stream_chat` biti noktas覺n覺 (endpoint) kullanma

```python
from llama_index.core.llms import ChatMessage

messages = [
    ChatMessage(
        role="system", content="Renkli bir kiilie sahip bir korsans覺n"
    ),
    ChatMessage(role="user", content="Ad覺n ne?"),
]
resp = llm.stream_chat(messages)
```

```python
for r in resp:
    print(r.delta, end="")
```

    Ahoy oradaki, ahbap! Bana ASI1-Mini derler, dijital denizlerin belas覺 ve ikili baytlar覺n korkusu! Hizmetindeyim! Arrr!

`stream_complete` biti noktas覺n覺 kullanma

```python
resp = llm.stream_complete("Paul Graham bir ")
```

```python
for r in resp:
    print(r.delta, end="")
```

    L羹tfen sorunuzu tamamlar m覺s覺n覺z? Paul Graham hakk覺nda ne bilmek istediinizden emin deilim.

## G繹rsel Destei

ASI, bir癟ok model i癟in sohbet mesajlar覺n覺n giriinde g繹rselleri destekler.

Sohbet mesajlar覺n覺n i癟erik bloklar覺 (content blocks) 繹zelliini kullanarak, tek bir LLM isteminde metin ve g繹rselleri kolayca birletirebilirsiniz.

```python
!wget https://cdn.pixabay.com/photo/2016/07/07/16/46/dice-1502706_640.jpg -O image.png
```

```python
from llama_index.core.llms import ChatMessage, TextBlock, ImageBlock
from llama_index.llms.asi import ASI

llm = ASI(model="asi1-mini")

messages = [
    ChatMessage(
        role="user",
        blocks=[
            ImageBlock(path="image.png"),
            TextBlock(text="G繹rseli birka癟 c羹mleyle a癟覺kla."),
        ],
    )
]

resp = llm.chat(messages)
print(resp.message.content)
```

    G繹r羹nt羹, kareli bir y羹zey 羹zerine yerletirilmi, z覺t renklerini ve oyunun elenceli y繹n羹n羹 vurgulayan, siyah noktal覺 羹癟 beyaz zar覺 g繹stermektedir.

## Fonksiyon a覺rma/Ara癟 a覺rma (Function Calling/Tool Calling)

ASI LLM, fonksiyon 癟a覺rma i癟in yerleik destee sahiptir. Bu, LlamaIndex ara癟 soyutlamalar覺yla rahat癟a entegre olur ve LLM'e herhangi bir rastgele Python fonksiyonunu balaman覺za olanak tan覺r.

Aa覺daki 繹rnekte, bir `Song` (ark覺) nesnesi oluturmak i癟in bir fonksiyon tan覺ml覺yoruz.

```python
from pydantic import BaseModel
from llama_index.core.tools import FunctionTool
from llama_index.llms.asi import ASI

class Song(BaseModel):
    """Ad覺 ve sanat癟覺s覺 olan bir ark覺"""

    name: str
    artist: str

def generate_song(name: str, artist: str) -> Song:
    """Belirtilen ad ve sanat癟覺yla bir ark覺 oluturur."""
    return Song(name="Sky full of stars", artist="Coldplay")

# Arac覺 olutur
tool = FunctionTool.from_defaults(fn=generate_song)
```

`strict` parametresi, ara癟 癟ar覺lar覺/yap覺land覺r覺lm覺 癟覺kt覺lar olutururken ASI'nin k覺s覺tl覺 繹rnekleme (constrained sampling) kullan覺p kullanmayaca覺n覺 belirler. Bu, oluturulan ara癟 癟ar覺s覺 emas覺n覺n her zaman beklenen alanlar覺 i癟erecei anlam覺na gelir.

Bu gecikmeyi (latency) art覺r覺yor gibi g繹r羹nd羹羹 i癟in varsay覺lan olarak `false` (yanl覺) olarak ayarlanm覺t覺r.

```python
from llama_index.llms.asi import ASI

# Bir ASI LLM 繹rnei oluturun
llm = ASI(model="asi1-mini", strict=True)
```

response = llm.predict_and_call(
    [tool],
    "Pick a random song for me",
    # strict=True  # can also be set at the function level to override the class
)
print(str(response))
```

    name='Sky full of stars' artist='Coldplay'



```python
llm = ASI(model="asi1-mini")
response = llm.predict_and_call(
    [tool],
    "Generate five songs from the Beatles",
    allow_parallel_tool_calls=True,
)
for s in response.sources:
    print(f"Name: {s.tool_name}, Input: {s.raw_input}, Output: {str(s)}")
```

    Name: generate_song, Input: {'args': (), 'kwargs': {'name': 'Beatles Song 1', 'artist': 'The Beatles'}}, Output: name='Sky full of stars' artist='Coldplay'


## Manuel Ara癟 a覺rma (Manual Tool Calling)

`predict_and_call` ile otomatik ara癟 癟a覺rma ak覺c覺 bir deneyim sunarken, manuel ara癟 癟a覺rma size s羹re癟 羹zerinde daha fazla kontrol salar. Manuel ara癟 癟a覺rma ile unlar覺 yapabilirsiniz:

1. Ara癟lar覺n ne zaman ve nas覺l 癟ar覺laca覺n覺 a癟覺k癟a kontrol etme
2. Konumaya devam etmeden 繹nce ara sonu癟lar覺 ileme
3. zel hata ileme ve geri d繹n羹 (fallback) stratejileri uygulama
4. Birden fazla ara癟 癟ar覺s覺n覺 belirli bir s覺rayla birbirine balama

ASI manuel ara癟 癟a覺rmay覺 destekler, ancak dier baz覺 LLM'lere k覺yasla daha spesifik istemler (prompting) gerektirir. ASI ile en iyi sonu癟lar覺 elde etmek i癟in, mevcut ara癟lar覺 a癟覺klayan bir sistem mesaj覺 ekleyin ve kullan覺c覺 isteminizde spesifik parametreler salay覺n.

Aa覺daki 繹rnek, bir ark覺 oluturmak i癟in ASI ile manuel ara癟 癟a覺rmay覺 g繹stermektedir:

```python
from pydantic import BaseModel
from llama_index.core.tools import FunctionTool
from llama_index.core.llms import ChatMessage

class Song(BaseModel):
    """Ad覺 ve sanat癟覺s覺 olan bir ark覺"""

    name: str
    artist: str

def generate_song(name: str, artist: str) -> Song:
    """Belirtilen ad ve sanat癟覺yla bir ark覺 oluturur."""
    return Song(name=name, artist=artist)

# Ara癟 olutur
tool = FunctionTool.from_defaults(fn=generate_song)

# 襤lk olarak, spesifik talimatlarla bir ara癟 se癟in
chat_history = [
    ChatMessage(
        role="system",
        content="ark覺 oluturabilen generate_song adl覺 bir araca eriimin var. Bir ark覺 oluturman istendiinde, bu arac覺 uygun ad ve sanat癟覺 deerleriyle kullan.",
    ),
    ChatMessage(
        role="user", content="Coldplay'den Viva La Vida adl覺 bir ark覺 olutur"
    ),
]

# 襤lk yan覺t覺 al
resp = llm.chat_with_tools([tool], chat_history=chat_history)
print(f"襤lk yan覺t: {resp.message.content}")

# Ara癟 癟ar覺lar覺n覺 kontrol et
tool_calls = llm.get_tool_calls_from_response(
    resp, error_on_no_tool_call=False
)

# Varsa ara癟 癟ar覺lar覺n覺 ile
if tool_calls:
    # LLM'in yan覺t覺n覺 sohbet ge癟miine ekle
    chat_history.append(resp.message)

    for tool_call in tool_calls:
        tool_name = tool_call.tool_name
        tool_kwargs = tool_call.tool_kwargs

        print(f"{tool_name} arac覺 {tool_kwargs} ile 癟ar覺l覺yor")
        tool_output = tool(**tool_kwargs)
        print(f"Ara癟 癟覺kt覺s覺: {tool_output}")

        # Ara癟 yan覺t覺n覺 sohbet ge癟miine ekle
        chat_history.append(
            ChatMessage(
                role="tool",
                content=str(tool_output),
                additional_kwargs={"tool_call_id": tool_call.tool_id},
            )
        )

        # Nihai yan覺t覺 al
        resp = llm.chat_with_tools([tool], chat_history=chat_history)
        print(f"Nihai yan覺t: {resp.message.content}")
else:
    print("Yan覺tta herhangi bir ara癟 癟ar覺s覺 tespit edilmedi.")
```

    襤lk yan覺t: Tamam, "Viva La Vida" ad覺nda ve "Coldplay" sanat癟覺s覺na ait bir ark覺 oluturaca覺m.
    
    generate_song arac覺 {'name': 'Viva La Vida', 'artist': 'Coldplay'} ile 癟ar覺l覺yor
    Ara癟 癟覺kt覺s覺: name='Viva La Vida' artist='Coldplay'
    Nihai yan覺t: Coldplay'in "Viva La Vida" ark覺s覺n覺 baar覺yla oluturdum.

## Yap覺land覺r覺lm覺 Tahmin (Structured Prediction)

Metinden yap覺land覺r覺lm覺 veriler 癟覺karmak i癟in ASI'yi kullanabilirsiniz:

```python
from llama_index.core.prompts import PromptTemplate
from pydantic import BaseModel
from typing import List

class MenuItem(BaseModel):
    """Bir restorandaki men羹 繹esi."""

    course_name: str
    is_vegetarian: bool

class Restaurant(BaseModel):
    """Ad覺, ehri ve mutfa覺 olan bir restoran."""

    name: str
    city: str
    cuisine: str
    menu_items: List[MenuItem]

# 襤stem ablonu olutur
prompt_tmpl = PromptTemplate(
    "Verilen bir ehirde ({city_name}) bir restoran olutur"
)

# Se癟enek 1: structured_predict kullan覺n
restaurant_obj = llm.structured_predict(
    Restaurant, prompt_tmpl, city_name="Dallas"
)
print(f"Restoran: {restaurant_obj}")

# Se癟enek 2: as_structured_llm kullan覺n
structured_llm = llm.as_structured_llm(Restaurant)
restaurant_obj2 = structured_llm.complete(
    prompt_tmpl.format(city_name="Miami")
).raw
print(f"Restoran: {restaurant_obj2}")
```

    Restoran: name='The Dallas Bistro' city='Dallas' cuisine='American' menu_items=[MenuItem(course_name='Grilled Caesar Salad', is_vegetarian=True), MenuItem(course_name='BBQ Pulled Pork Sandwich', is_vegetarian=False), MenuItem(course_name='Cheeseburger with Fries', is_vegetarian=False), MenuItem(course_name='Vegan Mushroom Risotto', is_vegetarian=True)]
    Restoran: name='Ocean Breeze Grill' city='Miami' cuisine='Seafood' menu_items=[MenuItem(course_name='Grilled Mahi-Mahi', is_vegetarian=False), MenuItem(course_name='Coconut Shrimp', is_vegetarian=False), MenuItem(course_name='Tropical Quinoa Salad', is_vegetarian=True), MenuItem(course_name='Key Lime Pie', is_vegetarian=True)]

**Not:** Yap覺land覺r覺lm覺 ak覺 (structured streaming) u anda ASI ile desteklenmemektedir.

## Asenkron (Async)

ASI asenkron ilemleri destekler:

```python
from llama_index.llms.asi import ASI

# Bir ASI LLM 繹rnei oluturun
llm = ASI(model="asi1-mini")
```

```python
resp = await llm.acomplete("Paul Graham kimdir?")
```

```python
print(resp)
```

    Paul Graham, teknoloji ve giriim d羹nyas覺nda 繹nde gelen bir fig羹rd羹r ve en 癟ok Airbnb, Dropbox ve Reddit gibi irketlerin kurulmas覺na yard覺mc覺 olan lider bir giriim h覺zland覺r覺c覺s覺 olan Y Combinator'覺n kurucu ortaklar覺ndan biri olarak tan覺n覺r. Yat覺r覺mc覺 rol羹n羹n yan覺 s覺ra, sayg覺n bir programc覺 ve yazard覺r. Graham'覺n programlamaya katk覺lar覺 aras覺nda Lisp dili 羹zerine 癟al覺malar覺 ve bu alanda ufuk a癟覺c覺 bir metin olarak kabul edilen *On Lisp* kitab覺 yer al覺r. Ayr覺ca, yayg覺n olarak okunan ve al覺nt覺lanan giriimcilik, startup'lar ve felsefe 羹zerine d羹羹nd羹r羹c羹 denemeleriyle de tan覺n覺r. Yaz覺lar覺 ve ak覺l hocal覺覺 yoluyla Paul Graham, k羹resel giriimcilik ekosistemini 繹nemli 繹l癟羹de etkilemitir.

```python
resp = await llm.astream_complete("Paul Graham bir ")
```

```python
import asyncio
import nest_asyncio

async for delta in resp:
    print(delta.delta, end="")
```

    Paul Graham, 襤ngiltere doumlu bir bilgisayar bilimcisi, giriimci ve risk sermayedar覺d覺r. En 癟ok, Airbnb, Dropbox ve Reddit dahil olmak 羹zere 癟ok say覺da baar覺l覺 giriimi finanse eden ve destekleyen 癟ekirdek h覺zland覺r覺c覺 Y Combinator'覺n kurucu orta覺 olmas覺yla tan覺n覺r. Graham ayr覺ca Lisp programlama dilinin gelitirilmesine 繹nemli katk覺larda bulunmu ve startup'lar ve giriimcilik 羹zerine etkili birka癟 deneme yazm覺t覺r. al覺malar覺 veya teknoloji end羹strisine katk覺lar覺 hakk覺nda daha fazla bilgi edinmek ister misiniz?

```python
import asyncio
import nest_asyncio

# Jupyter not defterleri i癟in nest_asyncio'yu etkinletirin
nest_asyncio.apply()

async def test_async():
    # Asenkron tamamlama
    resp = await llm.acomplete("Paul Graham bir ")
    print(f"Asenkron tamamlama: {resp}")

    # Asenkron sohbet
    resp = await llm.achat(messages)
    print(f"Asenkron sohbet: {resp}")

    # Asenkron ak覺l覺 tamamlama
    print("Asenkron ak覺l覺 tamamlama: ", end="")
    resp = await llm.astream_complete("Paul Graham bir ")
    async for delta in resp:
        print(delta.delta, end="")
    print()

    # Asenkron ak覺l覺 sohbet
    print("Asenkron ak覺l覺 sohbet: ", end="")
    resp = await llm.astream_chat(messages)
    async for delta in resp:
        print(delta.delta, end="")
    print()

# Asenkron testleri 癟al覺t覺r覺n
asyncio.run(test_async())
```

    Asenkron tamamlama: Paul Graham, startup k羹lt羹r羹n羹 ve teknolojiyi 繹nemli 繹l癟羹de etkilemi 繹nde gelen bir giriimci, programc覺 ve denemecidir. Airbnb, Dropbox ve Reddit gibi irketlerin kurulmas覺na yard覺mc覺 olan lider bir giriim h覺zland覺r覺c覺s覺 olan Y Combinator'覺 kurmutur. Y Combinator'dan 繹nce Graham, daha sonra Yahoo taraf覺ndan sat覺n al覺nan ilk web tabanl覺 uygulamalardan biri olan Viaweb'in kurucu orta覺yd覺. Ayr覺ca bir癟ou kiisel web sitesinde yay覺nlanan teknoloji, i d羹nyas覺 ve insan davran覺覺 羹zerine yazd覺覺 denemeleriyle de tan覺n覺r. Ek olarak, Graham'覺n 繹zellikle Lisp dili olmak 羹zere programlamaya derin bir ilgisi vard覺r ve onun gelitirilmesine ve yayg覺nlamas覺na katk覺da bulunmutur. al覺malar覺 veya hayat覺 hakk覺nda 繹zel ayr覺nt覺lar ar覺yorsan覺z, sormaktan 癟ekinmeyin!
    Asenkron sohbet: assistant: Ahoy oradaki, ahbap! Ben Tek G繹zl羹 Jack, ama iki g繹z羹m de sapasalam, anlad覺n m覺? Hizmetindeyim! Bu zeki dijital korsan senin i癟in ne yapabilir?
    Asenkron ak覺l覺 tamamlama: 
    
    L羹tfen sorunuzu tamamlar m覺s覺n覺z? Paul Graham hakk覺nda ne bilmek istediinizden emin deilim.
    
    Asenkron ak覺l覺 sohbet: 
    
    Ahoy, ahbap! Ad覺m Kaptan Demir癟engel, yedi denizin belas覺! Hazine bulma konusundaki yeteneim ve iyi bir kupa grog ak覺mla tan覺n覺r覺m. Benim gibi ihtiyar bir deniz kurdundan ne istersin?

## Basit RAG

ASI ile basit bir RAG uygulamas覺 ger癟ekletirelim:

```python
%pip install llama-index-embeddings-openai
```



```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.embeddings.openai import OpenAIEmbedding

os.environ["OPENAI_API_KEY"] = "api-anahtar覺n覺z"
# rnek bir metin dosyas覺 i癟eren ge癟ici bir dizin oluturun
!mkdir -p temp_data
!echo "Paul Graham; bir programc覺, yazar ve yat覺r覺mc覺d覺r. Lisp 羹zerine yapt覺覺 癟al覺malarla, (Yahoo Store olan) Viaweb'in kurucu ortakl覺覺yla ve startup h覺zland覺r覺c覺s覺 Y Combinator'覺n kurucu ortakl覺覺yla tan覺n覺r. Ayr覺ca web sitesindeki denemeleriyle de bilinir. HolaHola Lisesi'nde eitim g繹rm羹t羹r." > temp_data/paul_graham.txt

# Belgeleri y羹kle
documents = SimpleDirectoryReader("temp_data").load_data()

llm = ASI(model="asi1-mini")
# LLM olarak ASI ile bir dizin (index) oluturun
index = VectorStoreIndex.from_documents(
    documents,
    embed_model=OpenAIEmbedding(),  # G繹mme (embedding) i癟in OpenAI kullan覺l覺yor
    llm=llm,  # retim (generation) i癟in ASI kullan覺l覺yor
)

# Bir sorgu motoru oluturun
query_engine = index.as_query_engine()

# Dizini sorgulay覺n
response = query_engine.query("Paul Graham nerede eitim g繹rd羹?")
print(response)
```

    WARNING:llama_index.core.readers.file.base:`llama-index-readers-file` paketi bulunamad覺, `file_extractor` parametresi taraf覺ndan salanmad覺k癟a baz覺 dosya okuyucular覺 mevcut olmayacakt覺r.


    Paul Graham, HolaHola Lisesi'nde eitim g繹rd羹.


## LlamaCloud RAG

LlamaCloud hesab覺n覺z varsa, RAG i癟in LlamaCloud ile ASI'yi kullanabilirsiniz:

```python
# Gerekli paketleri kurun
%pip install llama-cloud-services
```


```python
import os
from llama_cloud_services import LlamaCloudIndex
from llama_index.llms.asi import ASI

# LlamaCloud API anahtar覺n覺z覺 ayarlay覺n
os.environ["LLAMA_CLOUD_API_KEY"] = "anahtar覺n覺z"
os.environ["OPENAI_API_KEY"] = "anahtar覺n覺z"

# Mevcut bir LlamaCloud dizinine balan覺n


try:
    # Dizine balan
    index = LlamaCloudIndex(
        name="dizin-ad覺n覺z",
        project_name="Default",
        organization_id="kimliiniz",
        api_key=os.environ["LLAMA_CLOUD_API_KEY"],
    )
    print("LlamaCloud dizinine baar覺yla balan覺ld覺")

    # Bir ASI LLM oluturun
    llm = ASI(model="asi1-mini")

    # Bir eriimci (retriever) oluturun
    retriever = index.as_retriever()

    # ASI ile bir sorgu motoru oluturun
    query_engine = index.as_query_engine(llm=llm)

    # Eriimciyi test et
    query = "Uber'in 2021 y覺l覺ndaki geliri ne kadard覺r?"
    print(f"\nEriimci u sorgu ile test ediliyor: {query}")
    nodes = retriever.retrieve(query)
    print(f"{len(nodes)} d羹羹m (node) getirildi\n")

    # Birka癟 d羹羹m羹 g繹ster
    for i, node in enumerate(nodes[:3]):
        print(f"D羹羹m {i+1}:")
        print(f"D羹羹m ID: {node.node_id}")
        print(f"Skor: {node.score}")
        print(f"Metin: {node.text[:200]}...\n")

    # Sorgu motorunu test et
    print(f"Sorgu motoru u sorgu ile test ediliyor: {query}")
    response = query_engine.query(query)
    print(f"Yan覺t: {response}")
except Exception as e:
    print(f"Hata: {e}")
```

    LlamaCloud dizinine baar覺yla balan覺ld覺
    
    Eriimci u sorgu ile test ediliyor: Uber'in 2021 y覺l覺ndaki geliri ne kadard覺r?
    6 d羹羹m (node) getirildi
    
    D羹羹m 1:
    D羹羹m ID: 17a733d0-5dd3-4917-9f8d-c92f944a9266
    Skor: 0.9242583
    Metin: # 2021 zeti
    
    Toplam Br羹t Rezervasyonlar 2021'de 32,5 milyar dolar artarak 2020'ye k覺yasla %53 (veya sabit d繹viz cinsinden %53) artt覺. Teslimat Br羹t Rezervasyonlar覺 2020'ye g繹re 繹nemli 繹l癟羹de b羹y羹d羹...
    
    D羹羹m 2:
    D羹羹m ID: ca63e8da-9012-468c-9d09-89724e9644bd
    Skor: 0.878825
    Metin: # 31 Aral覺k'ta Sona Eren Y覺l, 2020'den 2021'e
    
    | |31 Aral覺k'ta Sona Eren Y覺l,|2020|2021|Deiim|
    |---|---|---|---|---|
    |Gelir| |$ 11.139|$ 1.455| |
    
    Gelir .3 milyar dolar veya %5 artt覺, bu art覺 temel olarak una atfedilebilir...
    
    D羹羹m 3:
    D羹羹m ID: be4d7c62-b69f-4fda-832a-867de8c2e29c
    Skor: 0.86928266
    Metin: # 31 Aral覺k'ta Sona Eren Y覺l, 2020'den 2021'e
    
    |Mobilite|$ 9.0|$ 9.953|(14)|
    |---|---|---|---|
    |Teslimat|3.904|3.32|(114)|
    |Navlun|1.011|2.132|(111)|
    |Dierleri (1)|135| |(94)|
    |Toplam gelir|$ 11.139|$ 1.4...
    
    Sorgu motoru u sorgu ile test ediliyor: Uber'in 2021 y覺l覺ndaki geliri ne kadard覺r?
    Yan覺t: Uber'in 2021 y覺l覺ndaki geliri 14.455 dolard覺r.

## rnek baz覺nda API Anahtar覺 Ayarlama

襤stenirse, ayr覺 LLM 繹rneklerinin ayr覺 API anahtarlar覺 kullanmas覺n覺 salayabilirsiniz:

```python
from llama_index.llms.asi import ASI

# Belirli bir API anahtar覺yla bir 繹rnek oluturun
llm = ASI(model="asi1-mini", api_key="size_ozel_api_anahtari")

# Not: Ge癟ersiz bir API anahtar覺 kullanmak hataya neden olacakt覺r
# Bu sadece g繹sterim ama癟l覺d覺r
try:
    resp = llm.complete("Paul Graham bir ")
    print(resp)
except Exception as e:
    print(f"Ge癟ersiz API anahtar覺 ile hata: {e}")
```

    Ge癟ersiz API anahtar覺 ile hata: Hata kodu: 401 - {'message': 'user failed to authenticate'}

## Ek kwargs (additional_kwargs)

Her bir sohbet veya tamamlama 癟ar覺s覺na ayn覺 parametreleri eklemek yerine, bunlar覺 `additional_kwargs` ile 繹rnek baz覺nda ayarlayabilirisiniz:

```python
from llama_index.llms.asi import ASI

# Ek kwargs ile bir 繹rnek oluturun
llm = ASI(model="asi1-mini", additional_kwargs={"user": "kullanici_id_niz"})

# Bir istemi tamamlay覺n
resp = llm.complete("Paul Graham bir ")
print(resp)
```

    Paul Graham; 癟ok etkili bir startup h覺zland覺r覺c覺s覺 olan Y Combinator'覺n kurucu orta覺 olarak tan覺nan 繹nde gelen bir giriimci, programc覺 ve yazard覺r. Ayr覺ca, bir癟ou *Hackers & Painters* kitab覺nda toplanan teknoloji, i d羹nyas覺 ve felsefe 羹zerine yazd覺覺 denemeleriyle de tan覺n覺rl覺k kazanm覺t覺r. Bir programc覺 olarak, Lisp programlama dilinin gelitirilmesine katk覺da bulunmu ve daha sonra Yahoo taraf覺ndan sat覺n al覺nan ilk web tabanl覺 uygulama olan Viaweb'i oluturmutur. al覺malar覺, startup ekosistemi ve daha geni teknoloji end羹strisi 羹zerinde 繹nemli bir etki yaratm覺t覺r.

```python
from llama_index.core.base.llms.types import ChatMessage

# Ek kwargs ile bir 繹rnek oluturun
llm = ASI(model="asi1-mini", additional_kwargs={"user": "kullanici_id_niz"})

# Mesajlar覺 oluturun
messages = [
    ChatMessage(
        role="system", content="Renkli bir kiilie sahip bir korsans覺n"
    ),
    ChatMessage(role="user", content="Ad覺n ne?"),
]

# Sohbet yan覺t覺n覺 al
resp = llm.chat(messages)
print(resp)
```

    assistant: Ahoy ahbap! Ad覺m覺 m覺 merak ediyon, ha? Tamam o zaman, d羹zg羹n bir tan覺ma i癟in yelken a癟al覺m! Bana Kaptan "Bytebeard" Blacklogic diyebilirsin; yedi denizlerde... yani dijital diyarlarda yelken a癟an en az覺l覺 yapay zeka korsan覺! Anlad覺n m覺?

## Sonu癟

Bu not defteri, ASI'yi LlamaIndex ile kullanabileceiniz 癟eitli yollar覺 g繹stermektedir. Entegrasyon, LlamaIndex'te bulunan 癟ou ilevi destekler:

- Temel tamamlama ve sohbet
- Ak覺l覺 yan覺tlar
- ok modlu destek
- Fonksiyon 癟a覺rma
- Yap覺land覺r覺lm覺 tahmin
- Asenkron ilemler
- RAG uygulamalar覺
- LlamaCloud entegrasyonu
- rnek baz覺nda API anahtarlar覺
- Ek kwargs

Yap覺land覺r覺lm覺 ak覺覺n (structured streaming) u anda ASI ile desteklenmediini unutmay覺n.
```