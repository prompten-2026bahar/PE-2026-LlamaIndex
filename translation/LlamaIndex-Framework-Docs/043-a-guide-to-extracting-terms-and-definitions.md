# Terim ve Tan覺mlar覺 覺karma K覺lavuzu

LlamaIndex'in iyi d繹k羹mante edilmi bir癟ok kullan覺m durumu (anlamsal arama, 繹zetleme vb.) vard覺r. Ancak bu, LlamaIndex'i 癟ok spesifik kullan覺m durumlar覺na uygulayamayaca覺m覺z anlam覺na gelmez!

Bu eitimde, metinden terimleri ve tan覺mlar覺 癟覺karmak i癟in LlamaIndex'i kullanma ve kullan覺c覺lar覺n daha sonra bu terimleri sorgulamas覺na olanak tan覺ma tasar覺m s羹recini inceleyeceiz. [Streamlit](https://streamlit.io/) kullanarak, t羹m bunlar覺 癟al覺t覺rmak ve test etmek i癟in kolayca bir 繹n y羹z oluturabilir ve tasar覺m覺m覺zla h覺zl覺 bir ekilde yineleme yapabiliriz.

Bu eitim, Python 3.9+ s羹r羹m羹ne ve aa覺daki paketlerin y羹kl羹 olduuna sahip olduunuzu varsayar:

-   llama-index
-   streamlit

Temel d羹zeyde amac覺m覺z; bir d繹k羹mandan metin almak, terimleri ve tan覺mlar覺 癟覺karmak ve ard覺ndan kullan覺c覺lar覺n bu terim ve tan覺m bilgi taban覺n覺 sorgulamas覺 i癟in bir yol salamakt覺r. Eitim, hem LlamaIndex hem de Streamlit 繹zelliklerini inceleyecek ve umar覺z ortaya 癟覺kan yayg覺n sorunlar i癟in ilgin癟 癟繹z羹mler sunacakt覺r.

Bu eitimin final s羹r羹m羹 [burada](https://github.com/abdulasiraj/A-Guide-to-Extracting-Terms-and-Definitions) bulunabilir ve canl覺 olarak bar覺nd覺r覺lan bir demo [Huggingface Spaces](https://huggingface.co/spaces/Nobody4591/Llama_Index_Term_Extractor) 羹zerinde mevcuttur.

## Metin Y羹kleme

襤lk ad覺m, kullan覺c覺lara metni manuel olarak girme yolu vermektir. Bunun i癟in aray羹z salayacak Streamlit kodunu yazal覺m! Aa覺daki kodu kullan覺n ve uygulamay覺 `streamlit run app.py` ile balat覺n.

```python
import streamlit as st

st.title(" Llama Index Terim 覺kar覺c覺 ")

document_text = st.text_area("Ham metni girin")
if st.button("Terimleri ve Tan覺mlar覺 覺kar") and document_text:
    with st.spinner("覺kar覺l覺yor..."):
        extracted_terms = document_text  # bu bir yer tutucudur!
    st.write(extracted_terms)
```

S羹per basit, deil mi! Ancak uygulaman覺n hen羹z yararl覺 bir ey yapmad覺覺n覺 fark edeceksiniz. LlamaIndex'i kullanmak i癟in OpenAI LLM'imizi de kurmam覺z gerekiyor. LLM i癟in bir dizi olas覺 ayar vard覺r, bu y羹zden hangisinin en iyi olduunu kullan覺c覺n覺n belirlemesine izin verebiliriz. Ayr覺ca kullan覺c覺n覺n terimleri 癟覺karacak istemi (prompt) ayarlamas覺na da izin vermeliyiz (bu, neyin en iyi 癟al覺t覺覺n覺 hata ay覺klamam覺za da yard覺mc覺 olacakt覺r).

## LLM Ayarlar覺

Bu sonraki ad覺m, farkl覺 繹zellikler sunan farkl覺 b繹lmelere ay覺rmak i癟in uygulamam覺za baz覺 sekmeler (tabs) ekler. LLM ayarlar覺 ve metin y羹kleme i癟in birer sekme olutural覺m:

```python
import os
import streamlit as st

DEFAULT_TERM_STR = (
    "Balamda tan覺mlanan terimlerin ve tan覺mlar覺n bir listesini yap覺n, "
    "her sat覺rda bir 癟ift olsun. "
    "Bir terimin tan覺m覺 eksikse, en iyi tahmininizi kullan覺n. "
    "Her sat覺r覺 u ekilde yaz覺n:\nTerim: <terim> Tan覺m: <tan覺m>"
)

st.title(" Llama Index Terim 覺kar覺c覺 ")

setup_tab, upload_tab = st.tabs(["Kurulum", "Y羹kle/Terimleri 覺kar"])

with setup_tab:
    st.subheader("LLM Kurulumu")
    api_key = st.text_input("OpenAI API anahtar覺n覺z覺 buraya girin", type="password")
    llm_name = st.selectbox("Hangi LLM?", ["gpt-3.5-turbo", "gpt-4"])
    model_temperature = st.slider(
        "LLM S覺cakl覺覺 (Temperature)", min_value=0.0, max_value=1.0, step=0.1
    )
    term_extract_str = st.text_area(
        "Terimleri ve tan覺mlar覺 癟覺karmak i癟in kullan覺lacak sorgu.",
        value=DEFAULT_TERM_STR,
    )

with upload_tab:
    st.subheader("Tan覺mlar覺 覺kar ve Sorgula")
    document_text = st.text_area("Ham metni girin")
    if st.button("Terimleri ve Tan覺mlar覺 覺kar") and document_text:
        with st.spinner("覺kar覺l覺yor..."):
            extracted_terms = document_text  # bu bir yer tutucudur!
        st.write(extracted_terms)
```

Art覺k uygulamam覺z覺n iki sekmesi var, bu da organizasyona ger癟ekten yard覺mc覺 oluyor. Ayr覺ca terimleri 癟覺karmak i癟in varsay覺lan bir istem eklediimi fark edeceksiniz -- baz覺 terimleri 癟覺karmay覺 denedikten sonra bunu deitirebilirsiniz, bu sadece biraz deneme yapt覺ktan sonra ulat覺覺m istemdir.

Terimleri 癟覺karmaktan bahsetmiken, tam olarak bunu yapacak baz覺 fonksiyonlar eklemenin zaman覺 geldi!

## Terimleri 覺karma ve Saklama

Art覺k LLM ayarlar覺n覺 ve girdi metnini tan覺mlayabildiimize g繹re, terimleri bizim i癟in metinden 癟覺karmak i癟in LlamaIndex'i kullanmay覺 deneyebiliriz!

Hem LLM'imizi balatmak hem de giri metninden terimleri 癟覺karmak i癟in aa覺daki fonksiyonlar覺 ekleyebiliriz.

```python
from llama_index.core import Document, SummaryIndex, load_index_from_storage
from llama_index.llms.openai import OpenAI
from llama_index.core import Settings


def get_llm(llm_name, model_temperature, api_key, max_tokens=256):
    os.environ["OPENAI_API_KEY"] = api_key
    return OpenAI(
        temperature=model_temperature, model=llm_name, max_tokens=max_tokens
    )


def extract_terms(
    documents, term_extract_str, llm_name, model_temperature, api_key
):
    llm = get_llm(llm_name, model_temperature, api_key, max_tokens=1024)

    temp_index = SummaryIndex.from_documents(
        documents,
    )
    query_engine = temp_index.as_query_engine(
        response_mode="tree_summarize", llm=llm
    )
    terms_definitions = str(query_engine.query(term_extract_str))
    terms_definitions = [
        x
        for x in terms_definitions.split("\n")
        if x and "Terim:" in x and "Tan覺m:" in x
    ]
    # metni bir s繹zl羹e d繹n羹t羹r
    terms_to_definition = {
        x.split("Tan覺m:")[0]
        .split("Terim:")[-1]
        .strip(): x.split("Tan覺m:")[-1]
        .strip()
        for x in terms_definitions
    }
    return terms_to_definition
```

imdi yeni fonksiyonlar覺 kullanarak nihayet terimlerimizi 癟覺karabiliriz!

```python
...
with upload_tab:
    st.subheader("Tan覺mlar覺 覺kar ve Sorgula")
    document_text = st.text_area("Ham metni girin")
    if st.button("Terimleri ve Tan覺mlar覺 覺kar") and document_text:
        with st.spinner("覺kar覺l覺yor..."):
            extracted_terms = extract_terms(
                [Document(text=document_text)],
                term_extract_str,
                llm_name,
                model_temperature,
                api_key,
            )
        st.write(extracted_terms)
```

u anda 癟ok ey oluyor, neler bittiini incelemek i癟in bir dakikan覺z覺 ay覺ral覺m.

`get_llm()`, kurulum sekmesindeki kullan覺c覺 yap覺land覺rmas覺na dayanarak LLM'i somutlat覺r覺yor.

`extract_terms()`, t羹m g羹zel eylerin olduu yerdir. 襤lk olarak, terimlerimizi ve tan覺mlar覺m覺z覺 癟覺kar覺rken modeli 癟ok fazla k覺s覺tlamak istemediimiz i癟in `max_tokens=1024` ile `get_llm()` 癟a覺r覺yoruz (ayarlanmazsa varsay覺lan deer 256'd覺r). Ard覺ndan, `Settings` nesnemizi tan覺ml覺yoruz; `num_output` deerini `max_tokens` deerimizle hizal覺yoruz ve chunk boyutunu 癟覺kt覺dan daha b羹y羹k olmayacak ekilde ayarl覺yoruz. D繹k羹manlar LlamaIndex taraf覺ndan indekslendiinde, b羹y羹klerse par癟alara (node olarak da adland覺r覺l覺r) b繹l羹n羹rler ve `chunk_size` bu par癟alar覺n boyutunu belirler.

S覺rada, ge癟ici bir 繹zet indeksi (summary index) oluturuyoruz ve LLM'imizi ge癟iyoruz. Bir 繹zet indeksi, indeksimizdeki her bir metin par癟as覺n覺 okuyacakt覺r; bu da terimleri 癟覺karmak i癟in m羹kemmeldir. Son olarak, `response_mode="tree_summarize"` kullanarak terimleri 癟覺karmak i癟in 繹nceden tan覺mlanm覺 sorgu metnimizi kullan覺yoruz. Bu yan覺t modu, aa覺dan yukar覺ya doru bir 繹zet aac覺 oluturacakt覺r; burada her 羹st 繹e (parent) kendi alt 繹elerini (children) 繹zetler. Son olarak aac覺n tepesi d繹nd羹r羹l羹r; bu tepe t羹m 癟覺kar覺lan terimlerimizi ve tan覺mlar覺m覺z覺 i癟erecektir.

Son olarak, k羹癟羹k bir ilem sonras覺 (post processing) yap覺yoruz. Modelin talimatlar覺 takip ettiini ve her sat覺ra bir terim/tan覺m 癟ifti koyduunu varsay覺yoruz. Eer bir sat覺rda `Terim:` veya `Tan覺m:` etiketleri eksikse, o sat覺r覺 atl覺yoruz. Ard覺ndan kolay saklama i癟in bunu bir s繹zl羹e d繹n羹t羹r羹yoruz!

## 覺kar覺lan Terimleri Kaydetme

Art覺k terimleri 癟覺karabildiimize g繹re, onlar覺 daha sonra sorgulayabilmemiz i癟in bir yere koymam覺z gerekiyor. Bir `VectorStoreIndex` u an i癟in m羹kemmel bir se癟im olacakt覺r! Ancak ek olarak, uygulamam覺z daha sonra inceleyebilmemiz i癟in hangi terimlerin indekse eklendiini de takip etmelidir. `st.session_state` kullanarak, her kullan覺c覺ya 繹zel bir oturum s繹zl羹羹nde mevcut terim listesini saklayabiliriz!

ncelikle k羹resel bir vekt繹r indeksini balatmak i癟in bir 繹zellik ve 癟覺kar覺lan terimleri eklemek i癟in baka bir fonksiyon ekleyelim.

```python
from llama_index.core import Settings, VectorStoreIndex

...
if "all_terms" not in st.session_state:
    st.session_state["all_terms"] = DEFAULT_TERMS
...


def insert_terms(terms_to_definition):
    for term, definition in terms_to_definition.items():
        doc = Document(text=f"Terim: {term}\nTan覺m: {definition}")
        st.session_state["llama_index"].insert(doc)


@st.cache_resource
def initialize_index(llm_name, model_temperature, api_key):
    """VectorStoreIndex nesnesini oluturur."""
    Settings.llm = get_llm(llm_name, model_temperature, api_key)

    index = VectorStoreIndex([])

    return index


...

with upload_tab:
    st.subheader("Tan覺mlar覺 覺kar ve Sorgula")
    if st.button("襤ndeksi Balat ve Terimleri S覺f覺rla"):
        st.session_state["llama_index"] = initialize_index(
            llm_name, model_temperature, api_key
        )
        st.session_state["all_terms"] = {}

    if "llama_index" in st.session_state:
        st.markdown(
            "Bir d繹k羹man覺n g繹r羹nt羹s羹n羹/ekran g繹r羹nt羹s羹n羹 y羹kleyin veya metni manuel olarak girin."
        )
        document_text = st.text_area("Veya ham metni girin")
        if st.button("Terimleri ve Tan覺mlar覺 覺kar") and (
            uploaded_file or document_text
        ):
            st.session_state["terms"] = {}
            terms_docs = {}
            with st.spinner("覺kar覺l覺yor..."):
                terms_docs.update(
                    extract_terms(
                        [Document(text=document_text)],
                        term_extract_str,
                        llm_name,
                        model_temperature,
                        api_key,
                    )
                )
            st.session_state["terms"].update(terms_docs)

        if "terms" in st.session_state and st.session_state["terms"]:
            st.markdown("覺kar覺lan terimler")
            st.json(st.session_state["terms"])

            if st.button("Terimler eklensin mi?"):
                with st.spinner("Terimler ekleniyor"):
                    insert_terms(st.session_state["terms"])
                st.session_state["all_terms"].update(st.session_state["terms"])
                st.session_state["terms"] = {}
                st.experimental_rerun()
```

imdi Streamlit'in g羹c羹nden ger癟ekten yararlanmaya bal覺yorsunuz! Y羹kleme sekmesinin alt覺ndaki kodla balayal覺m. Vekt繹r indeksini balatmak i癟in bir buton ekledik ve onu k羹resel Streamlit durum s繹zl羹羹nde (session state) sakl覺yoruz; ayr覺ca mevcut 癟覺kar覺lan terimleri s覺f覺rl覺yoruz. Ard覺ndan, girdi metninden terimleri 癟覺kard覺ktan sonra, 癟覺kar覺lan terimleri tekrar k羹resel durumda sakl覺yoruz ve kullan覺c覺ya eklemeden 繹nce onlar覺 inceleme ans覺 veriyoruz. Eer ekleme butonuna bas覺l覺rsa, terim ekleme fonksiyonumuzu 癟a覺r覺yoruz, eklenen terimlerin k羹resel takibini g羹ncelliyoruz ve en son 癟覺kar覺lan terimleri oturum durumundan kald覺r覺yoruz.

## 覺kar覺lan Terimler/Tan覺mlar 襤癟in Sorgulama

Terimler ve tan覺mlar 癟覺kar覺l覺p kaydedildikten sonra onlar覺 nas覺l kullanabiliriz? Ve kullan覺c覺 daha 繹nce nelerin kaydedildiini nas覺l hat覺rlayacak?? Bu 繹zellikleri y繹netmek i癟in uygulamaya birka癟 sekme daha ekleyebiliriz.

```python
...
setup_tab, terms_tab, upload_tab, query_tab = st.tabs(
    ["Kurulum", "T羹m Terimler", "Y羹kle/Terimleri 覺kar", "Terimleri Sorgula"]
)
...
with terms_tab:
    st.subheader("Mevcut 覺kar覺lan Terimler ve Tan覺mlar")
    st.json(st.session_state["all_terms"])
...
with query_tab:
    st.subheader("Terimleri/Tan覺mlar覺 Sorgulay覺n!")
    st.markdown(
        (
            "LLM sorgunuzu yan覺tlamaya 癟al覺acak ve eklediiniz terimleri/tan覺mlar覺 kullanarak cevaplar覺n覺 zenginletirecektir. "
            "Bir terim indekste yoksa, kendi i癟sel bilgisiyle yan覺t verecektir."
        )
    )
    if st.button("襤ndeksi Balat ve Terimleri S覺f覺rla", key="init_index_2"):
        st.session_state["llama_index"] = initialize_index(
            llm_name, model_temperature, api_key
        )
        st.session_state["all_terms"] = {}

    if "llama_index" in st.session_state:
        query_text = st.text_input("Bir terim veya tan覺m hakk覺nda soru sorun:")
        if query_text:
            query_text = (
                query_text
                + "\nCevab覺 bulamazsan覺z, sorguyu en iyi bildiiniz ekilde yan覺tlay覺n."
            )
            with st.spinner("Cevap oluturuluyor..."):
                response = (
                    st.session_state["llama_index"]
                    .as_query_engine(
                        similarity_top_k=5,
                        response_mode="compact",
                        text_qa_template=TEXT_QA_TEMPLATE,
                        refine_template=DEFAULT_REFINE_PROMPT,
                    )
                    .query(query_text)
                )
            st.markdown(str(response))
```

Bu k覺s覺m 癟ounlukla basit olsa da dikkat edilmesi gereken baz覺 繹nemli noktalar:

-   Balatma butonumuzun dier butonumuzla ayn覺 metni var. Streamlit buna itiraz edecektir, bu y羹zden bunun yerine benzersiz bir anahtar (key) sal覺yoruz.
-   Sorguya ek metin eklendi! Bu, indeksin cevaba sahip olmad覺覺 zamanlar覺 telafi etmeye 癟al覺mak i癟indir.
-   襤ndeks sorgumuzda iki se癟enek belirledik:
    -   `similarity_top_k=5`, indeksin sorguya en yak覺n eleen en iyi 5 terimi/tan覺m覺 getirecei anlam覺na gelir.
    -   `response_mode="compact"`, her LLM 癟ar覺s覺nda 5 eleen terimden/tan覺mdan m羹mk羹n olduunca fazla metnin kullan覺laca覺 anlam覺na gelir. Bu olmasayd覺, indeks LLM'e en az 5 癟ar覺 yapard覺; bu da kullan覺c覺 i癟in ileri yavalatabilir.

## Deneme Testi (Dry Run Test)

Asl覺nda biz ilerlerken sizin test ettiinizi umuyorum. Ama imdi, tam bir testi deneyelim.

1.  Uygulamay覺 yenileyin.
2.  LLM ayarlar覺n覺z覺 girin.
3.  Sorgu sekmesine gidin.
4.  unu sorun: `Bunnyhug nedir?`
5.  Uygulama sa癟ma sapan bir cevap vermelidir. Bilmiyorsan覺z, bunnyhug Kanada Prairies'inden (K覺rlar覺) insanlar覺n kulland覺覺 bir kap羹onlu (hoodie) s繹zc羹羹d羹r!
6.  Bu tan覺m覺 uygulamaya ekleyelim. Y羹kleme sekmesini a癟覺n ve u metni girin: `A bunnyhug is a common term used to describe a hoodie. This term is used by people from the Canadian Prairies.`
7.  覺kar butonuna t覺klay覺n. Birka癟 saniye sonra uygulama doru ekilde 癟覺kar覺lan terimi/tan覺m覺 g繹r羹nt羹lemelidir. Kaydetmek i癟in terimi ekle butonuna t覺klay覺n!
8.  Terimler sekmesini a癟arsak, az 繹nce 癟覺kard覺覺m覺z terim ve tan覺m g繹r羹nt羹lenmelidir.
9.  Sorgu sekmesine geri d繹n羹n ve bir bunnyhug'覺n ne olduunu sormay覺 deneyin. imdi cevap doru olmal覺d覺r!

## 襤yiletirme #1 - Bir Balang覺癟 襤ndeksi Oluturun

Temel uygulamam覺z 癟al覺覺rken, yararl覺 bir indeks oluturmak i癟in 癟ok 癟aba harcamak gerekiyormu gibi gelebilir. Ya kullan覺c覺ya uygulaman覺n sorgu yeteneklerini sergilemek i癟in bir t羹r balang覺癟 noktas覺 verseydik? Bunu yapabiliriz! 襤lk olarak, her y羹klemeden sonra indeksi diske kaydetmemiz i癟in uygulamam覺zda k羹癟羹k bir deiiklik yapal覺m:

```python
def insert_terms(terms_to_definition):
    for term, definition in terms_to_definition.items():
        doc = Document(text=f"Terim: {term}\nTan覺m: {definition}")
        st.session_state["llama_index"].insert(doc)
    # GE襤C襤 - diske kaydet
    st.session_state["llama_index"].storage_context.persist()
```

imdi, 癟覺kar覺m yapacak bir d繹k羹mana ihtiyac覺m覺z var! Bu projenin deposu New York City hakk覺ndaki Wikipedia sayfas覺n覺 kulland覺 ve metni [burada](https://github.com/jerryjliu/llama_index/blob/main/examples/test_wiki/data/nyc_text.txt) bulabilirsiniz.

Metni y羹kleme sekmesine yap覺t覺r覺p 癟al覺t覺r覺rsan覺z (biraz zaman alabilir), 癟覺kar覺lan terimleri ekleyebiliriz. 襤ndekse eklemeden 繹nce 癟覺kar覺lan terimlerin metnini bir not defterine veya benzeri bir yere kopyalad覺覺n覺zdan emin olun! Bunlara bir saniye i癟inde ihtiyac覺m覺z olacak.

Ekledikten sonra, indeksi diske kaydetmek i癟in kulland覺覺m覺z kod sat覺r覺n覺 kald覺r覺n. Art覺k kaydedilmi bir balang覺癟 indeksi ile `initialize_index` fonksiyonumuzu u ekilde g繹r羹necek ekilde deitirebiliriz:

```python
@st.cache_resource
def initialize_index(llm_name, model_temperature, api_key):
    """Index nesnesini y羹kler."""
    Settings.llm = get_llm(llm_name, model_temperature, api_key)

    index = load_index_from_storage(storage_context)

    return index
```

Not defterine o devasa 癟覺kar覺lan terim listesini kaydetmeyi hat覺rlad覺n覺z m覺? imdi uygulamam覺z balad覺覺nda, indeksteki varsay覺lan terimleri k羹resel terimler durumumuza aktarmak istiyoruz:

```python
...
if "all_terms" not in st.session_state:
    st.session_state["all_terms"] = DEFAULT_TERMS
...
```

Bunu daha 繹nce `all_terms` deerlerini s覺f覺rlad覺覺m覺z her yerde tekrarlay覺n.

## 襤yiletirme #2 - (Rafine Etme) Daha 襤yi 襤stemler (Prompts)

u anda uygulama ile biraz oynarsan覺z, istemimizi takip etmeyi b覺rakt覺覺n覺 fark edebilirsiniz! Hat覺rlarsan覺z, `query_str` deikenimize terim/tan覺m bulunamazsa en iyi bilgisiyle yan覺t vermesini eklemitik. Ancak imdi rastgele terimler (bunnyhug gibi!) sormay覺 denerseniz, bu talimatlar覺 takip edebilir veya etmeyebilir.

Bu durum, LlamaIndex'teki cevaplar覺 "rafine etme" (refining) konseptinden kaynaklanmaktad覺r. En iyi 5 eleen sonu癟 aras覺nda sorgulama yapt覺覺m覺z i癟in, bazen t羹m sonu癟lar tek bir isteme s覺maz! OpenAI modelleri genellikle 4097 token'l覺k bir maksimum giri boyutuna sahiptir. Bu nedenle LlamaIndex, eleen sonu癟lar覺 isteme s覺acak par癟alara ay覺rarak bunu hesaba katar. LlamaIndex ilk API 癟ar覺s覺ndan ilk cevab覺 ald覺ktan sonra, bir sonraki par癟ay覺 API'a bir 繹nceki cevapla birlikte g繹nderir ve modelden bu cevab覺 rafine etmesini ister.

G繹r羹n羹e g繹re rafine etme s羹reci sonu癟lar覺m覺z覺 bozuyor! `query_str`'ye fazladan talimatlar eklemek yerine bunu kald覺r覺n; LlamaIndex kendi 繹zel istemlerimizi salamam覺za izin verecektir! imdi [varsay覺lan istemleri](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/prompts/default_prompts.py) ve [sohbete 繹zel istemleri](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/prompts/chat_prompts.py) k覺lavuz olarak kullanarak bunlar覺 olutural覺m. Yeni bir `constants.py` dosyas覺 kullanarak baz覺 yeni sorgu ablonlar覺 olutural覺m:

```python
from llama_index.core import (
    PromptTemplate,
    SelectorPromptTemplate,
    ChatPromptTemplate,
)
from llama_index.core.prompts.utils import is_chat_model
from llama_index.core.llms import ChatMessage, MessageRole

# Metin Soru-Cevap (QA) ablonlar覺
DEFAULT_TEXT_QA_PROMPT_TMPL = (
    "Balam bilgisi aa覺dad覺r. \n"
    "---------------------\n"
    "{context_str}"
    "\n---------------------\n"
    "Balam bilgisini dikkate alarak u soruyu yan覺tlay覺n "
    "(cevab覺 bilmiyorsan覺z, en iyi bilginizi kullan覺n): {query_str}\n"
)
TEXT_QA_TEMPLATE = PromptTemplate(DEFAULT_TEXT_QA_PROMPT_TMPL)

# Rafine Etme (Refine) ablonlar覺
DEFAULT_REFINE_PROMPT_TMPL = (
    "As覺l soru u ekildedir: {query_str}\n"
    "Mevcut bir cevap salad覺k: {existing_answer}\n"
    "Aa覺daki biraz daha balamla mevcut cevab覺 rafine etme "
    "(sadece gerekiyorsa) f覺rsat覺m覺z var.\n"
    "------------\n"
    "{context_msg}\n"
    "------------\n"
    "Yeni balam覺 ve en iyi bilginizi kullanarak mevcut cevab覺 iyiletirin. "
    "Mevcut cevab覺 iyiletiremiyorsan覺z, aynen tekrarlay覺n."
)
DEFAULT_REFINE_PROMPT = PromptTemplate(DEFAULT_REFINE_PROMPT_TMPL)

CHAT_REFINE_PROMPT_TMPL_MSGS = [
    ChatMessage(content="{query_str}", role=MessageRole.USER),
    ChatMessage(content="{existing_answer}", role=MessageRole.ASSISTANT),
    ChatMessage(
        content="Yukar覺daki cevab覺 aa覺daki biraz daha balamla rafine etme "
        "(sadece gerekiyorsa) f覺rsat覺m覺z var.\n"
        "------------\n"
        "{context_msg}\n"
        "------------\n"
        "Yeni balam覺 ve en iyi bilginizi kullanarak mevcut cevab覺 iyiletirin. "
        "Mevcut cevab覺 iyiletiremiyorsan覺z, aynen tekrarlay覺n.",
        role=MessageRole.USER,
    ),
]

CHAT_REFINE_PROMPT = ChatPromptTemplate(CHAT_REFINE_PROMPT_TMPL_MSGS)

# rafine etme istemi se癟icisi (refine prompt selector)
REFINE_TEMPLATE = SelectorPromptTemplate(
    default_template=DEFAULT_REFINE_PROMPT,
    conditionals=[(is_chat_model, CHAT_REFINE_PROMPT)],
)
```

Bu 癟ok fazla kod gibi g繹r羹nebilir ama o kadar da k繹t羹 deil! Varsay覺lan istemlere bakt覺ysan覺z, varsay覺lan istemlerin ve sohbet modellerine 繹zel istemlerin olduunu fark etmi olabilirsiniz. Bu eilimi devam ettirerek, 繹zel istemlerimiz i癟in de ayn覺s覺n覺 yap覺yoruz. Ard覺ndan, bir istem se癟ici kullanarak her iki istemi tek bir nesnede birletirebiliriz. Kullan覺lan LLM bir sohbet modeliyse (ChatGPT, GPT-4), sohbet istemleri kullan覺l覺r. Aksi takdirde normal istem ablonlar覺 kullan覺l覺r.

Dikkat edilmesi gereken bir dier husus da sadece bir tane Soru-Cevap (QA) ablonu tan覺mlam覺 olmam覺zd覺r. Bir sohbet modelinde bu, tek bir "insan" (human) mesaj覺na d繹n羹t羹r羹lecektir.

imdi bu istemleri uygulamam覺za aktarabilir ve sorgu s覺ras覺nda kullanabiliriz.

```python
from constants import REFINE_TEMPLATE, TEXT_QA_TEMPLATE

...
if "llama_index" in st.session_state:
    query_text = st.text_input("Bir terim veya tan覺m hakk覺nda soru sorun:")
    if query_text:
        query_text = query_text  # Eski talimatlar覺 kald覺rd覺覺m覺za dikkat edin
        with st.spinner("Cevap oluturuluyor..."):
            response = (
                st.session_state["llama_index"]
                .as_query_engine(
                    similarity_top_k=5,
                    response_mode="compact",
                    text_qa_template=TEXT_QA_TEMPLATE,
                    refine_template=DEFAULT_REFINE_PROMPT,
                )
                .query(query_text)
            )
        st.markdown(str(response))
...
```

Sorgularla biraz daha deneme yaparsan覺z, cevaplar覺n art覺k talimatlar覺m覺z覺 biraz daha iyi takip ettiini fark edersiniz umar覺m!

## 襤yiletirme #3 - G繹r羹nt羹 (Image) Destei

LlamaIndex g繹r羹nt羹leri de destekler! LlamaIndex kullanarak d繹k羹manlar覺n (makaleler, mektuplar vb.) g繹r羹nt羹lerini y羹kleyebiliriz ve LlamaIndex metni 癟覺karma ilemini halleder. Kullan覺c覺lar覺n d繹k羹manlar覺n覺n g繹r羹nt羹lerini y羹klemelerine ve onlardan terim ile tan覺mlar覺 癟覺karmalar覺na olanak tan覺mak i癟in bundan yararlanabiliriz.

PIL hakk覺nda bir i癟e aktarma hatas覺 al覺rsan覺z, 繹nce `pip install Pillow` kullanarak y羹kleyin.

```python
from PIL import Image
from llama_index.readers.file import ImageReader


@st.cache_resource
def get_file_extractor():
    image_parser = ImageReader(keep_image=True, parse_text=True)
    file_extractor = {
        ".jpg": image_parser,
        ".png": image_parser,
        ".jpeg": image_parser,
    }
    return file_extractor


file_extractor = get_file_extractor()
...
with upload_tab:
    st.subheader("Tan覺mlar覺 覺kar ve Sorgula")
    if st.button("襤ndeksi Balat ve Terimleri S覺f覺rla", key="init_index_1"):
        st.session_state["llama_index"] = initialize_index(
            llm_name, model_temperature, api_key
        )
        st.session_state["all_terms"] = DEFAULT_TERMS

    if "llama_index" in st.session_state:
        st.markdown(
            "Bir d繹k羹man覺n g繹r羹nt羹s羹n羹/ekran g繹r羹nt羹s羹n羹 y羹kleyin veya metni manuel olarak girin."
        )
        uploaded_file = st.file_uploader(
            "Bir d繹k羹man覺n g繹r羹nt羹s羹n羹/ekran g繹r羹nt羹s羹n羹 y羹kleyin:",
            type=["png", "jpg", "jpeg"],
        )
        document_text = st.text_area("Veya ham metni girin")
        if st.button("Terimleri ve Tan覺mlar覺 覺kar") and (
            uploaded_file or document_text
        ):
            st.session_state["terms"] = {}
            terms_docs = {}
            with st.spinner("覺kar覺l覺yor (g繹r羹nt羹ler yava olabilir)..."):
                if document_text:
                    terms_docs.update(
                        extract_terms(
                            [Document(text=document_text)],
                            term_extract_str,
                            llm_name,
                            model_temperature,
                            api_key,
                        )
                    )
                if uploaded_file:
                    Image.open(uploaded_file).convert("RGB").save("temp.png")
                    img_reader = SimpleDirectoryReader(
                        input_files=["temp.png"], file_extractor=file_extractor
                    )
                    img_docs = img_reader.load_data()
                    os.remove("temp.png")
                    terms_docs.update(
                        extract_terms(
                            img_docs,
                            term_extract_str,
                            llm_name,
                            model_temperature,
                            api_key,
                        )
                    )
            st.session_state["terms"].update(terms_docs)

        if "terms" in st.session_state and st.session_state["terms"]:
            st.markdown("覺kar覺lan terimler")
            st.json(st.session_state["terms"])

            if st.button("Terimler eklensin mi?"):
                with st.spinner("Terimler ekleniyor"):
                    insert_terms(st.session_state["terms"])
                st.session_state["all_terms"].update(st.session_state["terms"])
                st.session_state["terms"] = {}
                st.experimental_rerun()
```

Burada Streamlit kullanarak dosya y羹kleme se癟eneini ekledik. Ard覺ndan g繹r羹nt羹 a癟覺l覺r ve diske kaydedilir (bu biraz dolamba癟l覺 g繹r羹nebilir ama ileri basit tutar). Ard覺ndan g繹r羹nt羹 yolunu okuyucuya (reader) ge癟eriz, d繹k羹manlar覺/metni 癟覺kar覺r覺z ve ge癟ici g繹r羹nt羹 dosyam覺z覺 kald覺r覺r覺z.

Art覺k d繹k羹manlara sahip olduumuza g繹re, `extract_terms()` fonksiyonunu daha 繹nce olduu gibi 癟a覺rabiliriz.

## Sonu癟 / zet (TLDR)

Bu eitimde, yolda kar覺m覺za 癟覺kan baz覺 yayg覺n sorunlar覺 ve problemleri 癟繹zerken bir ton bilgiye deindik:

-   Farkl覺 kullan覺m durumlar覺 i癟in farkl覺 indeksler kullanma (Liste ve Vekt繹r indeksi kar覺lat覺rmas覺)
-   Streamlit'in `session_state` konsepti ile k羹resel durum deerlerini saklama
-   LlamaIndex ile dahili istemleri (prompts) 繹zelletirme
-   LlamaIndex ile g繹r羹nt羹lerden metin okuma

Bu eitimin final s羹r羹m羹 [burada](https://github.com/abdulasiraj/A-Guide-to-Extracting-Terms-and-Definitions) bulunabilir ve canl覺 olarak bar覺nd覺r覺lan bir demo [Huggingface Spaces](https://huggingface.co/spaces/Nobody4591/Llama_Index_Term_Extractor) 羹zerinde mevcuttur.