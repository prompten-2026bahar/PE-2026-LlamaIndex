# HuggingFace ile Yerel (Local) Embedding'ler

LlamaIndex; BGE, Mixedbread, Nomic, Jina, E5 gibi Sentence Transformer modelleri dahil olmak Ã¼zere HuggingFace embedding modellerini destekler. Bu modelleri dÃ¶kÃ¼manlarÄ±mÄ±z ve eriÅŸim sorgularÄ±mÄ±z iÃ§in embedding oluÅŸturmada kullanabiliriz.

AyrÄ±ca, HuggingFace'in [Optimum kÃ¼tÃ¼phanesini](https://huggingface.co/docs/optimum) kullanarak ONNX ve OpenVINO modelleri oluÅŸturmak ve kullanmak iÃ§in yardÄ±mcÄ± araÃ§lar saÄŸlÄ±yoruz.

## HuggingFaceEmbedding

Temel `HuggingFaceEmbedding` sÄ±nÄ±fÄ±, embedding iÅŸlemleri iÃ§in herhangi bir HuggingFace modeli etrafÄ±nda oluÅŸturulmuÅŸ genel bir sarmalayÄ±cÄ±dÄ±r. Hugging Face Ã¼zerindeki tÃ¼m [embedding modelleri](https://huggingface.co/models?library=sentence-transformers) Ã§alÄ±ÅŸmalÄ±dÄ±r. Daha fazla Ã¶neri iÃ§in [embedding liderlik tablosuna (leaderboard)](https://huggingface.co/spaces/mteb/leaderboard) bakabilirsiniz.

Bu sÄ±nÄ±f, `pip install sentence-transformers` komutuyla kurabileceÄŸiniz `sentence-transformers` paketine baÄŸlÄ±dÄ±r.

NOT: Daha Ã¶nce LangChain'den `HuggingFaceEmbeddings` kullanÄ±yorsanÄ±z, bu sÄ±nÄ±f size eÅŸdeÄŸer sonuÃ§lar verecektir.

Bu not defterini Colab'da aÃ§Ä±yorsanÄ±z, muhtemelen LlamaIndex'i ğŸ¦™ kurmanÄ±z gerekecektir.

```python
%pip install llama-index-embeddings-huggingface
```

```python
!pip install llama-index
```

```python
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# https://huggingface.co/BAAI/bge-small-en-v1.5 adresini yÃ¼kler
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
```

```python
embeddings = embed_model.get_text_embedding("Merhaba DÃ¼nya!")
print(len(embeddings))
print(embeddings[:5])
```

    384
    [-0.003275700844824314, -0.011690810322761536, 0.041559211909770966, -0.03814814239740372, 0.024183044210076332]

## KÄ±yaslama (Benchmarking)

Klasik ve bÃ¼yÃ¼k bir dÃ¶kÃ¼man olan IPCC iklim raporu, bÃ¶lÃ¼m 3'Ã¼ kullanarak karÅŸÄ±laÅŸtÄ±rma yapmayÄ± deneyelim.

```python
!curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf
```

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core import Settings

documents = SimpleDirectoryReader(
    input_files=["IPCC_AR6_WGII_Chapter03.pdf"]
).load_data()
```

### Temel HuggingFace Embedding'leri

```python
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# VarsayÄ±lan torch arka ucu (backend) ile BAAI/bge-small-en-v1.5'i yÃ¼kler
embed_model = HuggingFaceEmbedding(
    model_name="BAAI/bge-small-en-v1.5",
    device="cpu",
    embed_batch_size=8,
)
test_embeds = embed_model.get_text_embedding("Merhaba DÃ¼nya!")

Settings.embed_model = embed_model
```

```python
%%timeit -r 1 -n 1
index = VectorStoreIndex.from_documents(documents, show_progress=True)
```

    Parsing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 172/172 [00:00<00:00, 428.44it/s]
    Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 459/459 [00:19<00:00, 23.32it/s]

    20.2 s Â± 0 ns per loop (mean Â± std. dev. of 1 run, 1 loop each)

### ONNX Embedding'leri

```python
# pip install sentence-transformers[onnx]

# Onnx arka ucu (backend) ile BAAI/bge-small-en-v1.5'i yÃ¼kler
embed_model = HuggingFaceEmbedding(
    model_name="BAAI/bge-small-en-v1.5",
    device="cpu",
    backend="onnx",
    model_kwargs={
        "provider": "CPUExecutionProvider"
    },  # ONNX iÃ§in saÄŸlayÄ±cÄ±yÄ± (provider) belirtebilirsiniz, bkz: https://sbert.net/docs/sentence_transformer/usage/efficiency.html
)
test_embeds = embed_model.get_text_embedding("Merhaba DÃ¼nya!")

Settings.embed_model = embed_model
```

```python
%%timeit -r 1 -n 1
index = VectorStoreIndex.from_documents(documents, show_progress=True)
```

    Parsing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 172/172 [00:00<00:00, 421.63it/s]
    Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 459/459 [00:31<00:00, 14.53it/s]

    32.1 s Â± 0 ns per loop (mean Â± std. dev. of 1 run, 1 loop each)

### OpenVINO Embedding'leri

```python
# pip install sentence-transformers[openvino]

# OpenVINO arka ucu (backend) ile BAAI/bge-small-en-v1.5'i yÃ¼kler
embed_model = HuggingFaceEmbedding(
    model_name="BAAI/bge-small-en-v1.5",
    device="cpu",
    backend="openvino",  # OpenVINO, CPU'larda Ã§ok gÃ¼Ã§lÃ¼dÃ¼r
    revision="refs/pr/16",  # BAAI/bge-small-en-v1.5'in kendisinin ÅŸu an bir OpenVINO modeli yok, ancak yÃ¼kleyebileceÄŸimiz bir PR (pull request) var: https://huggingface.co/BAAI/bge-small-en-v1.5/discussions/16
    model_kwargs={
        "file_name": "openvino_model_qint8_quantized.xml"
    },  # Optimize edilmiÅŸ/kuantize edilmiÅŸ (quantized) bir model kullanÄ±yorsak, dosya adÄ±nÄ± bu ÅŸekilde belirtmemiz gerekir
)
test_embeds = embed_model.get_text_embedding("Merhaba DÃ¼nya!")

Settings.embed_model = embed_model
```

```python
%%timeit -r 1 -n 1
index = VectorStoreIndex.from_documents(documents, show_progress=True)
```

    Parsing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 172/172 [00:00<00:00, 403.15it/s]
    Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 459/459 [00:08<00:00, 53.83it/s]

    9.03 s Â± 0 ns per loop (mean Â± std. dev. of 1 run, 1 loop each)

### Referanslar

-   [Yerel Embedding Modelleri](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/#local-embedding-models), bunlar gibi yerel modellerin kullanÄ±mÄ± hakkÄ±nda daha fazla bilgi verir.
-   [Sentence Transformers > Ã‡Ä±karÄ±mÄ± HÄ±zlandÄ±rma](https://sbert.net/docs/sentence_transformer/usage/efficiency.html), ONNX ve OpenVINO iÃ§in optimizasyon ve kuantizasyon dahil olmak Ã¼zere arka uÃ§ seÃ§eneklerinin nasÄ±l etkili bir ÅŸekilde kullanÄ±lacaÄŸÄ±na dair kapsamlÄ± dÃ¶kÃ¼mantasyon iÃ§erir.