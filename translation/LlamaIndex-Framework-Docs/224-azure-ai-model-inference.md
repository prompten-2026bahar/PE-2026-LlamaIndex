# Azure AI model 癟覺kar覺m覺 (Azure AI model inference)

Bu not defteri, Azure AI Studio veya Azure Machine Learning'deki Azure AI model 癟覺kar覺m (AI model inference) API'si ile da覺t覺lan modellerle `llama-index-llm-azure-inference` paketinin nas覺l kullan覺laca覺n覺 a癟覺klar. Paket ayr覺ca GitHub Modelleri (nizleme) u癟 noktalar覺n覺 da destekler.

```python
%pip install llama-index-llms-azure-inference
```

Eer bu not defterini Google Colab'da a癟覺yorsan覺z, muhtemelen LlamaIndex'i  kurman覺z gerekecektir.

```python
%pip install llama-index
```

## n Koullar

Azure AI model 癟覺kar覺m覺, gelitiricilerin tutarl覺 bir ema kullanarak Azure AI'da bar覺nd覺r覺lan 癟eitli modellere erimesini salayan bir API'dir. `llama-index-llms-azure-inference` entegrasyon paketini, Azure AI sunucusuz API u癟 noktalar覺na da覺t覺lan modeller ve Y繹netilen 覺kar覺m'dan (Managed Inference) bir dizi model dahil olmak 羹zere bu API'yi destekleyen modellerle kullanabilirsiniz. API spesifikasyonu ve bunu destekleyen modeller hakk覺nda daha fazla bilgi edinmek i癟in [Azure AI model 癟覺kar覺m API'si](https://aka.ms/azureai/modelinference) sayfas覺na bakabilirsiniz.

Bu 繹reticiyi 癟al覺t覺rmak i癟in unlara ihtiyac覺n覺z vard覺r:

1. Bir [Azure abonelii](https://azure.microsoft.com) oluturun.
2. [Azure AI Studio merkezi nas覺l oluturulur ve y繹netilir](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/create-azure-ai-resource) sayfas覺nda a癟覺kland覺覺 gibi bir Azure AI merkezi (hub) kayna覺 oluturun.
3. [Azure AI model 癟覺kar覺m API'sini](https://aka.ms/azureai/modelinference) destekleyen bir model da覺t覺n. Bu 繹rnekte bir `Mistral-Large` da覺t覺m覺 kullan覺yoruz.

    * [Modelleri sunucusuz API'ler olarak da覺tma](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-serverless) talimatlar覺n覺 izleyebilirsiniz.

Alternatif olarak, 羹cretsiz kullan覺m deneyimi dahil olmak 羹zere bu entegrasyonla GitHub Modelleri u癟 noktalar覺n覺 kullanabilirsiniz. [GitHub modelleri](https://github.com/marketplace/models) hakk覺nda daha fazla bilgi edinin.

## Ortam Kurulumu

Kullanmak istediiniz modelden ihtiyac覺n覺z olan bilgileri almak i癟in u ad覺mlar覺 izleyin:

1. Kulland覺覺n覺z 羹r羹ne bal覺 olarak [Azure AI Foundry (eski ad覺yla Azure AI Studio)](https://ai.azure.com/) veya [Azure Machine Learning studio](https://ml.azure.com) adresine gidin.
2. Da覺t覺mlara (Azure Machine Learning'de u癟 noktalar) gidin ve 繹n koullarda belirtildii gibi da覺tt覺覺n覺z modeli se癟in.
3. U癟 nokta (endpoint) URL'sini ve anahtar覺 (key) kopyalay覺n.

> Modeliniz Microsoft Entra Kimlik desteiyle da覺t覺ld覺ysa anahtara ihtiyac覺n覺z yoktur.

Bu senaryoda, hem u癟 nokta URL'sini hem de anahtar覺 aa覺daki ortam deikenlerine yerletirdik:

```python
import os

os.environ["AZURE_INFERENCE_ENDPOINT"] = "<uc-noktaniz>"
os.environ["AZURE_INFERENCE_CREDENTIAL"] = "<kimlik-bilginiz>"
```

## Da覺t覺m覺n覺za ve U癟 Noktan覺za Balan覺n

Azure AI Studio veya Azure Machine Learning'de da覺t覺lan LLM'leri kullanmak i癟in u癟 noktaya ve ona balanmak i癟in kimlik bilgilerine ihtiyac覺n覺z vard覺r. Managed Online Endpoints gibi tek bir modele hizmet veren u癟 noktalar i癟in `model_name` parametresi gerekli deildir.

```python
from llama_index.llms.azure_inference import AzureAICompletionsModel
```

```python
llm = AzureAICompletionsModel(
    endpoint=os.environ["AZURE_INFERENCE_ENDPOINT"],
    credential=os.environ["AZURE_INFERENCE_CREDENTIAL"],
)
```

Alternatif olarak, u癟 noktan覺z Microsoft Entra Kimliini destekliyorsa, istemciyi oluturmak i癟in aa覺daki kodu kullanabilirsiniz:

```python
from azure.identity import DefaultAzureCredential

llm = AzureAICompletionsModel(
    endpoint=os.environ["AZURE_INFERENCE_ENDPOINT"],
    credential=DefaultAzureCredential(),
)
```

> Not: Microsoft Entra Kimliini kullan覺rken, u癟 noktan覺n bu kimlik dorulama y繹ntemiyle da覺t覺ld覺覺ndan ve onu 癟a覺rmak i癟in gerekli izinlere sahip olduunuzdan emin olun.

Asenkron 癟a覺rma kullanmay覺 planl覺yorsan覺z, kimlik bilgileri i癟in asenkron s羹r羹m羹 kullanmak en iyi uygulamad覺r:

```python
from azure.identity.aio import (
    DefaultAzureCredential as DefaultAzureCredentialAsync,
)

llm = AzureAICompletionsModel(
    endpoint=os.environ["AZURE_INFERENCE_ENDPOINT"],
    credential=DefaultAzureCredentialAsync(),
)
```

U癟 noktan覺z [GitHub Modelleri](https://github.com/marketplace/models) veya Azure AI Hizmetleri gibi birden fazla modele hizmet veriyorsa, `model_name` parametresini belirtmeniz gerekir:

```python
llm = AzureAICompletionsModel(
    endpoint=os.environ["AZURE_INFERENCE_ENDPOINT"],
    credential=os.environ["AZURE_INFERENCE_CREDENTIAL"],
    model_name="mistral-large",  # kullanmak istediiniz modele g繹re deitirin
)
```

## Modeli Kullanma

Metin tamamlama i癟in `complete` u癟 noktas覺n覺 kullan覺n. `chat-completions` t羹r羹ndeki modeller i癟in `complete` y繹ntemi hala mevcuttur. Bu durumlarda, giri metniniz `role="user"` olan bir mesaja d繹n羹t羹r羹l羹r.

```python
response = llm.complete("G繹ky羹z羹 g羹zel bir mavi ve")
print(response)
```

```python
response = llm.stream_complete("G繹ky羹z羹 g羹zel bir mavi ve")
for r in response:
    print(r.delta, end="")
```

Sohbet i癟in `chat` u癟 noktas覺n覺 kullan覺n

```python
from llama_index.core.llms import ChatMessage

messages = [
    ChatMessage(
        role="system", content="Renkli bir kiilie sahip bir korsans覺n."
    ),
    ChatMessage(role="user", content="Merhaba"),
]

response = llm.chat(messages)
print(response)
```

```python
response = llm.stream_chat(messages)
for r in response:
    print(r.delta, end="")
```

Her sohbet veya tamamlama 癟ar覺s覺na ayn覺 parametreleri eklemek yerine, bunlar覺 istemci 繹rneinde ayarlayabilirsiniz.

```python
llm = AzureAICompletionsModel(
    endpoint=os.environ["AZURE_INFERENCE_ENDPOINT"],
    credential=os.environ["AZURE_INFERENCE_CREDENTIAL"],
    temperature=0.0,
    model_kwargs={"top_p": 1.0},
)
```

```python
response = llm.complete("G繹ky羹z羹 g羹zel bir mavi ve")
print(response)
```

Azure AI model 癟覺kar覺m API'si taraf覺ndan desteklenmeyen ancak temel modelde mevcut olan ek parametreler i癟in `model_extras` arg羹man覺n覺 kullanabilirsiniz. Aa覺daki 繹rnekte, yaln覺zca Mistral modelleri i癟in mevcut olan `safe_prompt` parametresi ge癟ilmektedir.

```python
llm = AzureAICompletionsModel(
    endpoint=os.environ["AZURE_INFERENCE_ENDPOINT"],
    credential=os.environ["AZURE_INFERENCE_CREDENTIAL"],
    temperature=0.0,
    model_kwargs={"model_extras": {"safe_prompt": True}},
)
```

```python
response = llm.complete("G繹ky羹z羹 g羹zel bir mavi ve")
print(response)
```

## Ek kaynaklar

bu entegrasyon hakk覺nda daha fazla bilgi edinmek i癟in [LlamaIndex ve Azure AI ile Balarken](https://aka.ms/azureai/llamaindex) sayfas覺n覺 ziyaret edin.
