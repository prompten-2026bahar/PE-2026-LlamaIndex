# CohereAI Embedding'leri

Cohere Embed; float, int8, binary ve ubinary embedding'lerini yerel olarak destekleyen ilk embedding modelidir.

1. v3 modelleri tÃ¼m embedding tÃ¼rlerini desteklerken, v2 modelleri yalnÄ±zca `float` embedding tÃ¼rÃ¼nÃ¼ destekler.
2. `LlamaIndex` ile varsayÄ±lan `embedding_type` (embedding tÃ¼rÃ¼) `float`'tur. v3 modelleri iÃ§in `embedding_type` parametresini kullanarak bunu Ã¶zelleÅŸtirebilirsiniz.

Bu not defterinde, farklÄ± `models`, `input_types` ve `embedding_types` ile `Cohere Embeddings` kullanÄ±mÄ±nÄ± gÃ¶stereceÄŸiz.

Cohere int8 ve binary Embedding'leri hakkÄ±nda daha fazla ayrÄ±ntÄ± iÃ§in [ana blog yazÄ±larÄ±na](https://txt.cohere.com/int8-binary-embeddings/) bakÄ±n.

Bu not defterini Colab'da aÃ§Ä±yorsanÄ±z, muhtemelen LlamaIndex'i ğŸ¦™ kurmanÄ±z gerekecektir.

```python
%pip install llama-index-llms-cohere
%pip install llama-index-embeddings-cohere
```

```python
!pip install llama-index
```

```python
# API anahtarÄ±nÄ±zla baÅŸlatÄ±n
import os

cohere_api_key = "COHERE API ANAHTARINIZ"
os.environ["COHERE_API_KEY"] = cohere_api_key
```

#### En yeni `embed-english-v3.0` embedding'leri ile.

-   input_type="search_document": Bunu vektÃ¶r veritabanÄ±nÄ±zda saklamak istediÄŸiniz metinler (dÃ¶kÃ¼manlar) iÃ§in kullanÄ±n.
-   input_type="search_query": Bunu vektÃ¶r veritabanÄ±nÄ±zdaki en alakalÄ± dÃ¶kÃ¼manlarÄ± bulmak amacÄ±yla kullanÄ±lan arama sorgularÄ± iÃ§in kullanÄ±n.

VarsayÄ±lan `embedding_type` `float`'tur.

```python
from llama_index.embeddings.cohere import CohereEmbedding

# input_type='search_query' ile
embed_model = CohereEmbedding(
    api_key=cohere_api_key,
    model_name="embed-english-v3.0",
    input_type="search_query",
)

embeddings = embed_model.get_text_embedding("Merhaba CohereAI!")

print(len(embeddings))
print(embeddings[:5])
```

    1024
    [-0.041931152, -0.022384644, -0.07067871, -0.011886597, -0.019210815]

```python
# input_type = 'search_document' ile
embed_model = CohereEmbedding(
    api_key=cohere_api_key,
    model_name="embed-english-v3.0",
    input_type="search_document",
)

embeddings = embed_model.get_text_embedding("Merhaba CohereAI!")

print(len(embeddings))
print(embeddings[:5])
```

    1024
    [-0.03074646, -0.0029201508, -0.058044434, -0.015457153, -0.02331543]

##### `int8` embedding_type ile kontrol edelim

```python
embed_model = CohereEmbedding(
    api_key=cohere_api_key,
    model_name="embed-english-v3.0",
    input_type="search_query",
    embedding_type="int8",
)

embeddings = embed_model.get_text_embedding("Merhaba CohereAI!")

print(len(embeddings))
print(embeddings[:5])
```

    1024
    [-54, -29, -90, -16, -25]

##### `binary` embedding_type ile

```python
embed_model = CohereEmbedding(
    api_key=cohere_api_key,
    model_name="embed-english-v3.0",
    input_type="search_query",
    embedding_type="binary",
)

embeddings = embed_model.get_text_embedding("Merhaba CohereAI!")

print(len(embeddings))
print(embeddings[:5])
```

    128
    [-127, -38, 66, 83, 89]

#### Eski `embed-english-v2.0` embedding'leri ile.

v2 modelleri varsayÄ±lan olarak `float` embedding_type'Ä±nÄ± destekler.

```python
embed_model = CohereEmbedding(
    api_key=cohere_api_key, model_name="embed-english-v2.0"
)

embeddings = embed_model.get_text_embedding("Merhaba CohereAI!")

print(len(embeddings))
print(embeddings[:5])
```

    4096
    [0.65771484, 0.7998047, 2.3769531, -2.3105469, -1.6044922]

#### Åimdi en yeni `embed-english-v3.0` embedding'leri ile,

ÅunlarÄ± kullanalÄ±m:
1. Ä°ndeks oluÅŸturmak iÃ§in input_type=`search_document`
2. Ä°lgili baÄŸlamÄ± getirmek iÃ§in input_type=`search_query`

`int8` embedding_type'Ä± ile deneme yapacaÄŸÄ±z.

```python
import logging
import sys

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

from llama_index.llms.cohere import Cohere
from llama_index.core.response.notebook_utils import display_source_node

from IPython.display import Markdown, display
```

#### Veriyi Ä°ndir

```python
!mkdir -p 'data/paul_graham/'
!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'
```

#### Veriyi YÃ¼kle

```python
documents = SimpleDirectoryReader("./data/paul_graham/").load_data()
```

### `int8` embedding_type ile

#### input_type = 'search_document' ile indeks oluÅŸturma

```python
llm = Cohere(model="command-nightly", api_key=cohere_api_key)
embed_model = CohereEmbedding(
    api_key=cohere_api_key,
    model_name="embed-english-v3.0",
    input_type="search_document",
    embedding_type="int8",
)

index = VectorStoreIndex.from_documents(
    documents=documents, embed_model=embed_model
)
```

#### input_type = 'search_query' ile eriÅŸici (retriever) oluÅŸturma

```python
embed_model = CohereEmbedding(
    api_key=cohere_api_key,
    model_name="embed-english-v3.0",
    input_type="search_query",
    embedding_type="int8",
)

search_query_retriever = index.as_retriever()

search_query_retrieved_nodes = search_query_retriever.retrieve(
    "1995 yazÄ±nda ne oldu?"
)
```

```python
for n in search_query_retrieved_nodes:
    display_source_node(n, source_length=2000)
```

**DÃ¼ÄŸÃ¼m (Node) ID:** 0f821a16-5242-4284-86ba-23b16069e071<br>**Benzerlik:** 0.30740912992211505<br>**Metin:** Cambridge'de sahip olduÄŸum binayÄ± merkezimiz olarak kullanacaktÄ±k. Haftada bir kez â€” SalÄ± gÃ¼nleri, Ã§Ã¼nkÃ¼ PerÅŸembe akÅŸamÄ± yemek yiyenler iÃ§in zaten PerÅŸembe gÃ¼nleri yemek piÅŸiriyordum â€” orada hep birlikte akÅŸam yemeÄŸi yiyecektik ve yemekten sonra konuÅŸma yapmalarÄ± iÃ§in giriÅŸim uzmanlarÄ± getirecektik.

Ãœniversite Ã¶ÄŸrencilerinin o zamanlar yaz iÅŸleri hakkÄ±nda karar verdiklerini biliyorduk, bu yÃ¼zden birkaÃ§ gÃ¼n iÃ§inde Summer Founders Program adÄ±nÄ± verdiÄŸimiz bir ÅŸey hazÄ±rladÄ±k ve sitemde Ã¶ÄŸrencileri baÅŸvurmaya davet eden bir duyuru yayÄ±nladÄ±m. Makale yazmanÄ±n, yatÄ±rÄ±mcÄ±larÄ±n deyimiyle "anlaÅŸma akÄ±ÅŸÄ±" (deal flow) elde etmenin bir yolu olacaÄŸÄ±nÄ± hiÃ§ hayal etmemiÅŸtim ama mÃ¼kemmel bir kaynak olduÄŸu ortaya Ã§Ä±ktÄ±. [15] Summer Founders Program iÃ§in 225 baÅŸvuru aldÄ±k ve birÃ§oÄŸunun zaten mezun olmuÅŸ veya o bahar mezun olmak Ã¼zere olan kiÅŸilerden geldiÄŸini gÃ¶rÃ¼nce ÅŸaÅŸÄ±rdÄ±k. Zaten bu SFP iÅŸi niyetlediÄŸimizden daha ciddi bir hal almaya baÅŸlamÄ±ÅŸtÄ±.

225 gruptan yaklaÅŸÄ±k 20'sini ÅŸahsen gÃ¶rÃ¼ÅŸmeye davet ettik ve bunlar arasÄ±ndan fon saÄŸlamak iÃ§in 8'ini seÃ§tik. Etkileyici bir gruptular. O ilk grupta reddit, daha sonra Twitch'i kuracak olan Justin Kan ve Emmett Shear, RSS Ã¶zelliÄŸinin yazÄ±lmasÄ±na Ã§oktan yardÄ±m etmiÅŸ olan ve birkaÃ§ yÄ±l sonra aÃ§Ä±k eriÅŸim iÃ§in bir ÅŸehit haline gelecek olan Aaron Swartz ve daha sonra YC'nin ikinci baÅŸkanÄ± olacak olan Sam Altman vardÄ±. Ä°lk grubun bu kadar iyi olmasÄ±nÄ±n tamamen ÅŸans olduÄŸunu dÃ¼ÅŸÃ¼nmÃ¼yorum. Microsoft veya Goldman Sachs gibi meÅŸru bir yerdeki yaz iÅŸi yerine Summer Founders Program gibi tuhaf bir ÅŸeye kaydolmak iÃ§in oldukÃ§a cesur olmanÄ±z gerekiyordu.

GiriÅŸimler iÃ§in yapÄ±lan anlaÅŸma, Julian ile yaptÄ±ÄŸÄ±mÄ±z anlaÅŸmanÄ±n bir kombinasyonuna ($10 bin karÅŸÄ±lÄ±ÄŸÄ±nda %10) ve Robert'Ä±n MIT yÃ¼ksek lisans Ã¶ÄŸrencilerinin yaz iÃ§in ne aldÄ±ÄŸÄ±nÄ± sÃ¶ylediÄŸi miktara ($6 bin) dayanÄ±yordu. Kurucu baÅŸÄ±na $6 bin yatÄ±rÄ±m yaptÄ±k, bu da tipik bir iki kuruculu vaka iÃ§in %6 karÅŸÄ±lÄ±ÄŸÄ±nda $12 bin demekti. Bu adil olmalÄ±ydÄ± Ã§Ã¼nkÃ¼ kendi aldÄ±ÄŸÄ±mÄ±z anlaÅŸmadan iki kat daha iyiydi. AyrÄ±ca o gerÃ§ekten sÄ±cak geÃ§en ilk yaz boyunca Jessica kuruculara Ã¼cretsiz klimalar getirdi. [16]

OldukÃ§a hÄ±zlÄ± bir ÅŸekilde fark ettim ki...<br>

**DÃ¼ÄŸÃ¼m (Node) ID:** 15e1050d-38f1-4c7c-a169-ef9fe4ab1249<br>**Benzerlik:** 0.3000104724138056<br>**Metin:** Sadece bir avuÃ§ dolusu Ã§alÄ±ÅŸanÄ± olan bir ÅŸirket amatÃ¶rce gÃ¶rÃ¼nÃ¼rdÃ¼. Bu yÃ¼zden, 1998 yazÄ±nda Yahoo bizi satÄ±n alana kadar baÅŸa baÅŸ noktasÄ±na (breakeven) ulaÅŸamadÄ±k. Bu da ÅŸirketin tÃ¼m Ã¶mrÃ¼ boyunca yatÄ±rÄ±mcÄ±larÄ±n insafÄ±na kaldÄ±ÄŸÄ±mÄ±z anlamÄ±na geliyordu. Hem biz hem de yatÄ±rÄ±mcÄ±larÄ±mÄ±z giriÅŸimler konusunda acemi olduÄŸumuz iÃ§in, sonuÃ§ giriÅŸim standartlarÄ±na gÃ¶re bile bir karmaÅŸaydÄ±.

Yahoo bizi satÄ±n aldÄ±ÄŸÄ±nda bÃ¼yÃ¼k bir rahatlama oldu. Prensipte Viaweb hisselerimiz deÄŸerliydi. KÃ¢rlÄ± ve hÄ±zla bÃ¼yÃ¼yen bir iÅŸletmedeki hisselerdi. Ama bana pek deÄŸerli gelmiyordu; bir iÅŸletmeye nasÄ±l deÄŸer biÃ§ileceÄŸi hakkÄ±nda hiÃ§bir fikrim yoktu ama birkaÃ§ ayda bir yaÅŸadÄ±ÄŸÄ±mÄ±z Ã¶lÃ¼me yakÄ±n deneyimlerin fazlasÄ±yla farkÄ±ndaydÄ±m. BaÅŸladÄ±ÄŸÄ±mÄ±zdan beri yÃ¼ksek lisans Ã¶ÄŸrencisi yaÅŸam tarzÄ±mÄ± da Ã¶nemli Ã¶lÃ§Ã¼de deÄŸiÅŸtirmemiÅŸtim. Bu yÃ¼zden Yahoo bizi satÄ±n aldÄ±ÄŸÄ±nda kendimi fakirlikten zenginliÄŸe geÃ§miÅŸ gibi hissettim. Kaliforniya'ya gideceÄŸimiz iÃ§in bir araba satÄ±n aldÄ±m, sarÄ± bir 1998 VW GTI. Sadece deri koltuklarÄ±nÄ±n bile o zamana kadar sahip olduÄŸum en lÃ¼ks ÅŸey olduÄŸunu dÃ¼ÅŸÃ¼ndÃ¼ÄŸÃ¼mÃ¼ hatÄ±rlÄ±yorum.

Ertesi yÄ±l, 1998 yazÄ±ndan 1999 yazÄ±na kadar olan sÃ¼re, hayatÄ±mÄ±n muhtemelen en verimsiz dÃ¶nemiydi. O zaman fark etmemiÅŸtim ama Viaweb'i yÃ¶netmenin Ã§abasÄ±ndan ve stresinden tÃ¼kenmiÅŸtim. Kaliforniya'ya gittikten bir sÃ¼re sonra, sabaha karÅŸÄ± 3'e kadar programlama yapma ÅŸeklindeki olaÄŸan Ã§alÄ±ÅŸma tarzÄ±mÄ± sÃ¼rdÃ¼rmeye Ã§alÄ±ÅŸtÄ±m ama yorgunluk, Yahoo'nun erkenden yaÅŸlanmÄ±ÅŸ kÃ¼ltÃ¼rÃ¼ ve Santa Clara'daki kasvetli ofis bÃ¶lmeleriyle (cube farm) birleÅŸince yavaÅŸ yavaÅŸ beni aÅŸaÄŸÄ± Ã§ekti. BirkaÃ§ ay sonra kendimi huzursuz edici bir ÅŸekilde Interleaf'te Ã§alÄ±ÅŸÄ±yormuÅŸ gibi hissettim.

Yahoo bizi satÄ±n aldÄ±ÄŸÄ±nda bize Ã§ok sayÄ±da opsiyon vermiÅŸti. O zamanlar Yahoo'nun o kadar aÅŸÄ±rÄ± deÄŸerli olduÄŸunu dÃ¼ÅŸÃ¼nÃ¼yordum ki asla bir deÄŸeri olmayacaktÄ± ama hayretle gÃ¶rdÃ¼m ki hisse senetli takip eden yÄ±l iÃ§inde 5 kat arttÄ±. OpsiyonlarÄ±n ilk kÄ±smÄ± hak edilene (vest) kadar bekledim, sonra 1999 yazÄ±nda ayrÄ±ldÄ±m. Bir ÅŸeyler boyamayalÄ± o kadar uzun zaman olmuÅŸtu ki bunu neden yaptÄ±ÄŸÄ±mÄ± yarÄ± yarÄ±ya unutmuÅŸtum. Beynim 4 yÄ±l boyunca tamamen yazÄ±lÄ±m ve erkek gÃ¶mlekleriyle doluydu. Ama bunu zengin olmak iÃ§in yapmÄ±ÅŸtÄ±m bu yÃ¼zden...<br>

### `float` embedding_type ile

#### input_type = 'search_document' ile indeks oluÅŸturma

```python
llm = Cohere(model="command-nightly", api_key=cohere_api_key)
embed_model = CohereEmbedding(
    api_key=cohere_api_key,
    model_name="embed-english-v3.0",
    input_type="search_document",
    embedding_type="float",
)

index = VectorStoreIndex.from_documents(
    documents=documents, embed_model=embed_model
)
```

#### input_type = 'search_query' ile eriÅŸici (retriever) oluÅŸturma

```python
embed_model = CohereEmbedding(
    api_key=cohere_api_key,
    model_name="embed-english-v3.0",
    input_type="search_query",
    embedding_type="float",
)

search_query_retriever = index.as_retriever()

search_query_retrieved_nodes = search_query_retriever.retrieve(
    "1995 yazÄ±nda ne oldu?"
)
```

```python
for n in search_query_retrieved_nodes:
    display_source_node(n, source_length=2000)
```

**DÃ¼ÄŸÃ¼m (Node) ID:** cff8a942-2e1a-4921-ac08-8355b49fde85<br>**Benzerlik:** 0.3051793987443398<br>**Metin:** Cambridge'de sahip olduÄŸum binayÄ± merkezimiz olarak kullanacaktÄ±k. Haftada bir kez â€” SalÄ± gÃ¼nleri, Ã§Ã¼nkÃ¼ PerÅŸembe akÅŸamÄ± yemek yiyenler iÃ§in zaten PerÅŸembe gÃ¼nleri yemek piÅŸiriyordum â€” orada hep birlikte akÅŸam yemeÄŸi yiyecektik ve yemekten sonra konuÅŸma yapmalarÄ± iÃ§in giriÅŸim uzmanlarÄ± getirecektik.

... (yukarÄ±daki metinle aynÄ±) ...

**DÃ¼ÄŸÃ¼m (Node) ID:** 1810afad-3817-447c-a194-859601437923<br>**Benzerlik:** 0.2959499578848539<br>**Metin:** Sadece bir avuÃ§ dolusu Ã§alÄ±ÅŸanÄ± olan bir ÅŸirket amatÃ¶rce gÃ¶rÃ¼nÃ¼rdÃ¼. Bu yÃ¼zden, 1998 yazÄ±nda Yahoo bizi satÄ±n alana kadar baÅŸa baÅŸ noktasÄ±na ulaÅŸamadÄ±k.

... (yukarÄ±daki metinle aynÄ±) ...

### `binary` embedding_type ile.

#### input_type = 'search_document' ile indeks oluÅŸturma

```python
embed_model = CohereEmbedding(
    api_key=cohere_api_key,
    model_name="embed-english-v3.0",
    input_type="search_document",
    embedding_type="binary",
)

index = VectorStoreIndex.from_documents(
    documents=documents, embed_model=embed_model
)
```

#### input_type = 'search_query' ile eriÅŸici (retriever) oluÅŸturma

```python
embed_model = CohereEmbedding(
    api_key=cohere_api_key,
    model_name="embed-english-v3.0",
    input_type="search_query",
    embedding_type="binary",
)

search_query_retriever = index.as_retriever()

search_query_retrieved_nodes = search_query_retriever.retrieve(
    "1995 yazÄ±nda ne oldu?"
)
```

```python
for n in search_query_retrieved_nodes:
    display_source_node(n, source_length=2000)
```

**DÃ¼ÄŸÃ¼m (Node) ID:** fd8e185d-7c9e-40de-8d3e-09a76ae85e18<br>**Benzerlik:** 0.3498979255746315<br>**Metin:** ZamanÄ±nda editÃ¶r, en iyi genel amaÃ§lÄ± site oluÅŸturuculardan biriydi. Kodun bÃ¼tÃ¼nlÃ¼ÄŸÃ¼nÃ¼ korudum ve Robert ile Trevor'Ä±nkiler dÄ±ÅŸÄ±ndaki hiÃ§bir yazÄ±lÄ±mla entegre etmek zorunda kalmadÄ±m, bu yÃ¼zden Ã¼zerinde Ã§alÄ±ÅŸmak oldukÃ§a eÄŸlenceliydi. Tek yapmam gereken bu yazÄ±lÄ±m Ã¼zerinde Ã§alÄ±ÅŸmak olsaydÄ±, sonraki 3 yÄ±l hayatÄ±mÄ±n en kolay yÄ±llarÄ± olurdu. Ne yazÄ±k ki Ã§ok daha fazlasÄ±nÄ± yapmak zorundaydÄ±m, bunlarÄ±n hepsi programlamadan daha kÃ¶tÃ¼ olduÄŸum ÅŸeylerdi ve sonraki 3 yÄ±l bunun yerine en stresli yÄ±llarÄ±m oldu.

90'larÄ±n ikinci yarÄ±sÄ±nda e-ticaret yazÄ±lÄ±mÄ± yapan bir sÃ¼rÃ¼ giriÅŸim vardÄ±. Biz Interleaf deÄŸil, Microsoft Word olmaya kararlÄ±ydÄ±k. Bu da kullanÄ±mÄ± kolay ve ucuz olmak anlamÄ±na geliyordu. Fakir olmamÄ±z bizim iÃ§in bir ÅŸanstÄ± Ã§Ã¼nkÃ¼ bu Viaweb'i fark ettiÄŸimizden bile daha ucuz yapmamÄ±za neden oldu. KÃ¼Ã§Ã¼k bir maÄŸaza iÃ§in ayda $100, bÃ¼yÃ¼k bir maÄŸaza iÃ§inse ayda $300 Ã¼cret alÄ±yorduk. Bu dÃ¼ÅŸÃ¼k fiyat bÃ¼yÃ¼k bir ilgi odaÄŸÄ±ydÄ± ve rakiplerin ayaklarÄ±na takÄ±lan sÃ¼rekli bir dikendi, ancak fiyatÄ± dÃ¼ÅŸÃ¼k belirlememiz akÄ±llÄ±ca bir iÃ§gÃ¶rÃ¼ sayesinde olmamÄ±ÅŸtÄ±. Ä°ÅŸletmelerin bir ÅŸeyler iÃ§in ne Ã¶dediÄŸi hakkÄ±nda hiÃ§bir fikrimiz yoktu. Ayda $300 bize Ã§ok para gibi gÃ¶rÃ¼nÃ¼yordu.

Bunun gibi pek Ã§ok ÅŸeyi yanlÄ±ÅŸlÄ±kla doÄŸru yaptÄ±k. Ã–rneÄŸin, ÅŸimdi "Ã¶lÃ§eklenmeyen ÅŸeyleri yapmak" (doing things that don't scale) olarak adlandÄ±rÄ±lan ÅŸeyi yaptÄ±k, ancak o zamanlar bunu "kullanÄ±cÄ± edinmek iÃ§in en Ã§aresiz Ã¶nlemlere sÃ¼rÃ¼klenecek kadar ezik olmak" olarak tanÄ±mlardÄ±k. BunlarÄ±n en yaygÄ±nÄ± onlar iÃ§in maÄŸazalar inÅŸa etmekti. Bu Ã¶zellikle aÅŸaÄŸÄ±layÄ±cÄ± gÃ¶rÃ¼nÃ¼yordu Ã§Ã¼nkÃ¼ yazÄ±lÄ±mÄ±mÄ±zÄ±n tÃ¼m varlÄ±k nedeni insanlarÄ±n onu kendi maÄŸazalarÄ±nÄ± yapmak iÃ§in kullanabilmesiydi. Ama kullanÄ±cÄ± edinmek iÃ§in her ÅŸeyi yapardÄ±k.

Perakende hakkÄ±nda bilmek istediÄŸimizden Ã§ok daha fazlasÄ±nÄ± Ã¶ÄŸrendik. Ã–rneÄŸin, bir erkek gÃ¶mleÄŸinin yalnÄ±zca kÃ¼Ã§Ã¼k bir gÃ¶rÃ¼ntÃ¼sÃ¼ne sahip olabiliyorsanÄ±z (ve o zamanlar tÃ¼m gÃ¶rÃ¼ntÃ¼ler bugÃ¼nkÃ¼ standartlara gÃ¶re kÃ¼Ã§Ã¼ktÃ¼), tÃ¼m gÃ¶mleÄŸin resmindense yakanÄ±n yakÄ±n Ã§ekimine sahip olmanÄ±n daha iyi olduÄŸunu Ã¶ÄŸrendik. Bunu Ã¶ÄŸrendiÄŸimi hatÄ±rlamamÄ±n sebebi, erkek gÃ¶mleklerine ait yaklaÅŸÄ±k 30 resmi yeniden taramam gerektiÄŸi anlamÄ±na gelmesiydi. Ä°lk tarama setim de Ã§ok gÃ¼zeldi.

...<br>

**DÃ¼ÄŸÃ¼m (Node) ID:** b013216a-1c23-46b6-ba78-aaeed21b2fe2<br>**Benzerlik:** 0.3376224194936838<br>**Metin:** Ancak yazÄ±n yaklaÅŸÄ±k yarÄ±sÄ±nda bir ÅŸirket yÃ¶netmeyi gerÃ§ekten istemediÄŸimi fark ettim â€” Ã¶zellikle de bunun olmasÄ± gerektiÄŸi gibi gÃ¶rÃ¼nen bÃ¼yÃ¼k bir ÅŸirket. Viaweb'e sadece paraya ihtiyacÄ±m olduÄŸu iÃ§in baÅŸlamÄ±ÅŸtÄ±m. ArtÄ±k paraya ihtiyacÄ±m olmadÄ±ÄŸÄ±na gÃ¶re bunu neden yapÄ±yordum? EÄŸer bu vizyonun bir ÅŸirket olarak gerÃ§ekleÅŸtirilmesi gerekiyorsa, o vizyon batsÄ±n (screw the vision). AÃ§Ä±k kaynaklÄ± bir proje olarak yapÄ±labilecek bir alt kÃ¼me inÅŸa ederdim.

ÅaÅŸÄ±rtÄ±cÄ± bir ÅŸekilde, bu ÅŸeyler Ã¼zerinde harcadÄ±ÄŸÄ±m zaman boÅŸa gitmemiÅŸti. Y Combinator'a baÅŸladÄ±ktan sonra, bu yeni mimarinin bÃ¶lÃ¼mleri Ã¼zerinde Ã§alÄ±ÅŸan giriÅŸimlerle sÄ±k sÄ±k karÅŸÄ±laÅŸÄ±rdÄ±m ve bunun Ã¼zerinde bu kadar Ã§ok zaman dÃ¼ÅŸÃ¼nmÃ¼ÅŸ olmak ve hatta bir kÄ±smÄ±nÄ± yazmaya Ã§alÄ±ÅŸmÄ±ÅŸ olmak Ã§ok yararlÄ±ydÄ±.

AÃ§Ä±k kaynaklÄ± bir proje olarak inÅŸa edeceÄŸim alt kÃ¼me, parantezlerini artÄ±k saklamak zorunda bile kalmayacaÄŸÄ±m yeni Lisp'ti. Pek Ã§ok Lisp uzmanÄ± yeni bir Lisp inÅŸa etmeyi hayal eder, kÄ±smen dilin ayÄ±rt edici Ã¶zelliklerinden biri lehÃ§elerinin olmasÄ±dÄ±r ve kÄ±smen de, sanÄ±rÄ±m, zihnimizde mevcut tÃ¼m lehÃ§elerin gerisinde kaldÄ±ÄŸÄ± Platonik bir Lisp formuna sahip olmamÄ±zdÄ±r. Bende kesinlikle vardÄ±. Bu yÃ¼zden yazÄ±n sonunda Dan ve ben, Cambridge'de satÄ±n aldÄ±ÄŸÄ±m bir evde Arc adÄ±nÄ± verdiÄŸim bu yeni Lisp lehÃ§esi Ã¼zerinde Ã§alÄ±ÅŸmaya baÅŸladÄ±k.

Ertesi bahar, ÅŸans yÃ¼zÃ¼me gÃ¼ldÃ¼. Bir Lisp konferansÄ±nda bir konuÅŸma yapmaya davet edildim, bu yÃ¼zden Lisp'i Viaweb'de nasÄ±l kullandÄ±ÄŸÄ±mÄ±z hakkÄ±nda bir konuÅŸma yaptÄ±m. Daha sonra bu konuÅŸmanÄ±n bir postscript dosyasÄ±nÄ± paulgraham.com'a koydum; orayÄ± yÄ±llar Ã¶nce Viaweb kullanarak oluÅŸturmuÅŸtum ama hiÃ§bir ÅŸey iÃ§in kullanmamÄ±ÅŸtÄ±m. Bir gÃ¼nde 30.000 sayfa gÃ¶rÃ¼ntÃ¼leme aldÄ±. DÃ¼nyada ne olmuÅŸtu? YÃ¶nlendiren URL'ler birinin onu Slashdot'ta paylaÅŸtÄ±ÄŸÄ±nÄ± gÃ¶steriyordu. [10]

Vay canÄ±na, diye dÃ¼ÅŸÃ¼ndÃ¼m bir kitle var. EÄŸer bir ÅŸey yazar ve web'e koyarsam, herkes okuyabilir. Bu ÅŸimdi bariz gÃ¶rÃ¼nebilir ama o zaman ÅŸaÅŸÄ±rtÄ±cÄ±ydÄ±. BasÄ±lÄ± yayÄ±n dÃ¶neminde okuyuculara ulaÅŸan dar bir kanal vardÄ± ve bu kanal editÃ¶rler olarak bilinen hÄ±rÃ§Ä±n canavarlar tarafÄ±ndan korunuyordu. YazdÄ±ÄŸÄ±nÄ±z herhangi bir ÅŸey iÃ§in kitle edinmenin tek yolu...<br>

##### `binary` embedding tÃ¼rÃ¼ ile getirilen parÃ§alar, `float` ve `int8` ile karÅŸÄ±laÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda kesinlikle farklÄ±dÄ±r. RAG boru hattÄ±nÄ±zda `float`/`int8`/`binary`/`ubinary` embedding kullanÄ±mÄ± iÃ§in [eriÅŸim deÄŸerlendirmesi](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/usage_pattern_retrieval/) yapmak ilginÃ§ olacaktÄ±r.

### Metin-GÃ¶rÃ¼ntÃ¼ (Text-Image) Embedding'leri

[Cohere artÄ±k hem metnin hem de gÃ¶rÃ¼ntÃ¼nÃ¼n aynÄ± embedding alanÄ±nda bulunduÄŸu Ã§ok modlu (multi-modal) embedding modelini destekliyor.](https://cohere.com/blog/multimodal-embed-3)

```python
from PIL import Image
import matplotlib.pyplot as plt

img = Image.open("../data/images/prometheus_paper_card.png")
plt.imshow(img)
```

    <matplotlib.image.AxesImage at 0x2c7323af0>

![png](output_41_1.png)

```python
from llama_index.embeddings.cohere import CohereEmbedding

embed_model = CohereEmbedding(
    api_key=cohere_api_key,
    model_name="embed-english-v3.0",
)
```

##### GÃ¶rÃ¼ntÃ¼ Embedding'leri

```python
embeddings = embed_model.get_image_embedding(
    "../data/images/prometheus_paper_card.png"
)

print(len(embeddings))
print(embeddings[:5])
```

    1024
    [0.01171875, -0.014503479, 0.014205933, -0.022949219, -0.040374756]

##### Metin Embedding'leri

```python
embeddings = embed_model.get_text_embedding("prometheus evaluation model")

print(len(embeddings))
print(embeddings[:5])
```

    1024
    [0.0044403076, 0.01737976, -0.023345947, 0.028182983, -0.036499023]