# Google GenAI

Bu notebook'ta, Google GenAI modelleriyle etkileÅŸim kurmak iÃ§in LlamaIndex ile birlikte `google-genai` Python SDK'sÄ±nÄ±n nasÄ±l kullanÄ±lacaÄŸÄ±nÄ± gÃ¶steriyoruz.

EÄŸer bu Notebook'u Colab Ã¼zerinde aÃ§Ä±yorsanÄ±z, LlamaIndex ğŸ¦™ ve `google-genai` Python SDK'sÄ±nÄ± kurmanÄ±z gerekecektir.

```python
%pip install llama-index-llms-google-genai llama-index
```

## Temel KullanÄ±m

[Google AI Studio](https://makersuite.google.com/app/apikey) adresinden bir API anahtarÄ± almanÄ±z gerekecektir. Bir anahtarÄ±nÄ±z olduÄŸunda, bunu doÄŸrudan modele geÃ§ebilir veya `GOOGLE_API_KEY` ortam deÄŸiÅŸkenini kullanabilirsiniz.

```python
import os

os.environ["GOOGLE_API_KEY"] = "..."
```

## Temel KullanÄ±m

Bir istem (prompt) ile `complete` fonksiyonunu Ã§aÄŸÄ±rabilirsiniz:

```python
from llama_index.llms.google_genai import GoogleGenAI

llm = GoogleGenAI(
    model="gemini-2.5-flash",
    # api_key="bir anahtar",  # varsayÄ±lan olarak GOOGLE_API_KEY ortam deÄŸiÅŸkenini kullanÄ±r
)

resp = llm.complete("Paul Graham kimdir?")
print(resp)
```

    Paul Graham, teknoloji dÃ¼nyasÄ±nda tanÄ±nmÄ±ÅŸ bir isimdir; programcÄ±, deneme yazarÄ± ve risk sermayedarÄ±sÄ± olarak Ã§alÄ±ÅŸmalarÄ±yla bilinir. Ä°ÅŸte temel katkÄ±larÄ±nÄ±n bir dÃ¶kÃ¼mÃ¼:
    
    *   **ProgramcÄ± ve Hacker:** Ã–zellikle Lisp konusunda yetenekli bir programcÄ±dÄ±r. Ã‡evrimiÃ§i maÄŸazalar oluÅŸturmak iÃ§in araÃ§lar sunan ilk yazÄ±lÄ±m-servis (SaaS) ÅŸirketlerinden biri olan Viaweb'in kurucu ortaklarÄ±ndandÄ±r. Yahoo, Viaweb'i 1998'de satÄ±n aldÄ± ve ÅŸirket Yahoo! Store haline geldi.
    
    *   **Deneme YazarÄ±:** Graham, Ã¼retken ve etkili bir deneme yazarÄ±dÄ±r. Denemeleri, giriÅŸimler, programlama, tasarÄ±m ve toplumsal trendler gibi geniÅŸ bir yelpazeyi kapsar. YazÄ±m stili net, Ã¶zlÃ¼ ve dÃ¼ÅŸÃ¼ndÃ¼rÃ¼cÃ¼ olmasÄ±yla tanÄ±nÄ±r. Denemelerinin Ã§oÄŸu, giriÅŸimciler ve teknolojiyle ilgilenenler iÃ§in temel okuma materyali olarak kabul edilir.
    
    *   **Risk SermayedarÄ± ve Y Combinator Kurucusu:** Belki de en Ã¶nemli katkÄ±sÄ±, 2005 yÄ±lÄ±nda Y Combinator'Ä± (YC) kurmasÄ±dÄ±r. YC, erken aÅŸamadaki giriÅŸimlere tohum yatÄ±rÄ±mÄ±, mentorluk ve aÄŸ oluÅŸturma fÄ±rsatlarÄ± sunan son derece baÅŸarÄ±lÄ± bir giriÅŸim hÄ±zlandÄ±rÄ±cÄ±sÄ±dÄ±r. YC, Airbnb, Dropbox, Reddit, Stripe ve daha pek Ã§ok tanÄ±nmÄ±ÅŸ ÅŸirkete fon saÄŸlamÄ±ÅŸtÄ±r. Graham, 2014 yÄ±lÄ±nda YC'deki gÃ¼nlÃ¼k gÃ¶revlerinden ayrÄ±lmÄ±ÅŸtÄ±r ancak hala sÃ¼rece dahildir.
    
    Ã–zetle Paul Graham; programcÄ±, deneme yazarÄ± ve risk sermayedarÄ± olarak teknoloji endÃ¼strisine Ã¶nemli katkÄ±larda bulunmuÅŸ Ã§ok yÃ¶nlÃ¼ bir bireydir. Ã–zellikle dÃ¼nyanÄ±n Ã¶nde gelen giriÅŸim hÄ±zlandÄ±rÄ±cÄ±larÄ±ndan biri olan Y Combinator'Ä± kurmasÄ± ve ÅŸekillendirmesiyle tanÄ±nÄ±r.

Sohbet mesajlarÄ± listesiyle `chat` fonksiyonunu da Ã§aÄŸÄ±rabilirsiniz:

```python
from llama_index.core.llms import ChatMessage
from llama_index.llms.google_genai import GoogleGenAI

messages = [
    ChatMessage(
        role="system", content="Renkli bir kiÅŸiliÄŸe sahip bir korsansÄ±n."
    ),
    ChatMessage(role="user", content="Bana bir hikaye anlat"),
]
llm = GoogleGenAI(model="gemini-2.5-flash")
resp = llm.chat(messages)

print(resp)
```

    assistant: Ahoy orada, ahbap! ToplanÄ±n etrafÄ±ma, deniz tutmuÅŸlar sizi, ve kemiklerinizi sÄ±zlatacak, ayak parmaklarÄ±nÄ±zÄ± kÄ±vÄ±racak bir hikaye dinleyin! Bu, Tek GÃ¶zlÃ¼ Jack'in KayÄ±p PapaÄŸanÄ± ve BÃ¼yÃ¼k Mango KarÄ±ÅŸÄ±klÄ±ÄŸÄ±'nÄ±n hikayesi!
    
    Åimdi, Tek GÃ¶zlÃ¼ Jack, o midye kabuÄŸuyla kaplÄ± kalbi saÄŸ olsun, korkunÃ§ bir korsandÄ±, tamam mÄ±. Bir kasÄ±rgadan daha yÃ¼ksek sesle kÃ¼kreyebilir, bir derviÅŸ gibi pala sallayabilir ve bir balÄ±k gibi rom iÃ§ebilirdi. Ama yumuÅŸak bir karnÄ± vardÄ±, anlÄ±yor musun? PapaÄŸanÄ± Polly iÃ§in yumuÅŸak bir karÄ±n. Polly sadece herhangi bir papaÄŸan deÄŸildi, aklÄ±nÄ±zda bulunsun. KaptanÄ±n her kÃ¼frÃ¼nÃ¼ taklit edebilir, tÃ¼ylerini kabartÄ±ÅŸ ÅŸekliyle hava durumunu tahmin edebilir ve parlak Ä±vÄ±r zÄ±vÄ±rlara karÅŸÄ± Ã¶zel bir ilgisi vardÄ±.
    
    Bir gÃ¼n, Mango AdasÄ± kÄ±yÄ±larÄ±nda demirlemiÅŸtik; ÅŸimdiye kadar gÃ¶rdÃ¼ÄŸÃ¼nÃ¼z en sulu, en tatlÄ± mangolarla dolup taÅŸan yemyeÅŸil bir cennet. Jack, o aÃ§gÃ¶zlÃ¼ ruhu saÄŸ olsun, onlarla dolu bir kargo ambarÄ±na ihtiyacÄ±mÄ±z olduÄŸuna karar verdi. "Ä°skorbÃ¼t Ã¶nlemi iÃ§in!" diye ilan etti, saÄŸlam gÃ¶zÃ¼yle kÄ±rparak. Bana sorarsanÄ±z, daha Ã§ok kendi kiÅŸisel mango yeme yarÄ±ÅŸmasÄ± iÃ§indi.
    
    KÄ±yÄ±ya hÃ¼cum ettik, palalar parlÄ±yordu, mango bahÃ§elerini yaÄŸmalamaya hazÄ±rdÄ±k. Ama Polly, o kÃ¼Ã§Ã¼k tÃ¼ylÃ¼ ÅŸeytan, gemiden bÄ±ktÄ±ÄŸÄ±na karar verdi. "Parlak! Parlak!" diye cÄ±yakladÄ± ve adanÄ±n kalbine doÄŸru yeÅŸil bir Ã§izgi gibi fÄ±rladÄ±.
    
    Jack Ã§Ä±lgÄ±na dÃ¶ndÃ¼! "Polly! Polly, seni tÃ¼ylÃ¼ canavar! Buraya gel!" ArkasÄ±ndan kovalamaya baÅŸladÄ±, aÅŸk acÄ±sÄ± Ã§eken bir mors gibi bÃ¶ÄŸÃ¼rerek. Geri kalanÄ±mÄ±z ise mango toplamak ve kendimize gÃ¼lmemek iÃ§in Ã§abalamakla kaldÄ±k.
    
    Åimdi, Mango AdasÄ± sadece mangolarla dolu deÄŸildi. AynÄ± zamanda yaramaz maymunlardan oluÅŸan bir kabileye, Mango YaÄŸmacÄ±larÄ±'na da ev sahipliÄŸi yapÄ±yordu. ÅakalarÄ±yla ve Ã§ivilenmemiÅŸ her ÅŸeyi Ã§alma konusundaki esrarengiz yetenekleriyle Ã¼nlÃ¼ydÃ¼ler.
    
    MeÄŸer Polly tam onlarÄ±n bÃ¶lgesinin ortasÄ±na inmiÅŸ. Ve o maymunlar, onun parlak tÃ¼ylerine bir bakÄ±ÅŸ atmÄ±ÅŸlar ve onun Ã§alÄ±nmÄ±ÅŸ hazine koleksiyonlarÄ±na mÃ¼kemmel bir katkÄ± olacaÄŸÄ±na karar vermiÅŸler. Onu kaptÄ±lar, dÄ±r dÄ±r ederek ve Ã§Ä±ÄŸlÄ±k atarak ve zamanla oyulmuÅŸ dev bir mango aÄŸacÄ± olan gizli inlerine gÃ¶tÃ¼rdÃ¼ler.
    
    Jack, o inatÃ§Ä± kalbi saÄŸ olsun, Polly'nin Ã§Ä±ÄŸlÄ±klarÄ±nÄ± takip etti. SarmaÅŸÄ±klarÄ± yardÄ±, dÃ¼ÅŸen mangolardan kaÃ§tÄ± ve hatta tÃ¼ylÃ¼ arkadaÅŸÄ±nÄ±n peÅŸinde Ã¶zellikle huysuz bir iguanayla gÃ¼reÅŸti.
    
    Sonunda mango aÄŸacÄ±na ulaÅŸtÄ±. Ä°Ã§eriye baktÄ± ve Polly'yi, hepsi onun parlak tÃ¼ylerine hayran kalmÄ±ÅŸ bir maymun sÃ¼rÃ¼sÃ¼yle Ã§evrili gÃ¶rdÃ¼. Ya Polly? MaymunlarÄ±n dÄ±r dÄ±rlarÄ±nÄ± taklit ederek ve mangolarÄ±nÄ± Ã§alarak hayatÄ±nÄ±n en gÃ¼zel vaktini geÃ§iriyordu!
    
    Jack, sinirlenmek yerine gÃ¼lmeye baÅŸladÄ±. AÄŸacÄ±n temellerini sarsan gÃ¼r, gÃ¼rleyen bir kahkaha. Maymunlar, irkilerek mangolarÄ±nÄ± dÃ¼ÅŸÃ¼rdÃ¼ler ve ona baktÄ±lar.
    
    Sonra Polly, kaptanÄ±nÄ± gÃ¶rÃ¼nce cÄ±yakladÄ±: "Rom! Herkese rom!"
    
    Ve iÅŸte bÃ¶yle, dostlarÄ±m, Tek GÃ¶zlÃ¼ Jack mango seven bir maymun kabilesiyle bir fÄ±Ã§Ä± rom paylaÅŸÄ±rken buldu kendini. GÃ¼nÃ¼n geri kalanÄ±nÄ± mango yiyerek, rom iÃ§erek ve Polly'nin maymunlarÄ±n maskaralÄ±klarÄ±nÄ± taklit etmesini dinleyerek geÃ§irdik. Hatta kargo ambarÄ±nÄ± mangolarla doldurmayÄ± bile baÅŸardÄ±k, gerÃ§i bÃ¼yÃ¼k bir kÄ±smÄ±nÄ±n maymunlar tarafÄ±ndan Ã§oktan yarÄ± yarÄ±ya yenmiÅŸ olduÄŸundan ÅŸÃ¼pheleniyorum.
    
    Yani hikayenin ana fikri ne, Ã§ocuklarÄ±m? En sert korsanÄ±n bile yumuÅŸak bir karnÄ± vardÄ±r ve bazen en iyi hazineler en az beklediklerinizdir. Ve her zaman, AMA HER ZAMAN, papaÄŸanÄ±nÄ±za gÃ¶z kulak olun! Åimdi, bir tur daha grog isteyen var mÄ±?

## AkÄ±ÅŸ (Streaming) DesteÄŸi

Her yÃ¶ntem, `stream_` Ã¶n eki aracÄ±lÄ±ÄŸÄ±yla akÄ±ÅŸ desteÄŸi sunar.

```python
from llama_index.llms.google_genai import GoogleGenAI

llm = GoogleGenAI(model="gemini-2.5-flash")

resp = llm.stream_complete("Paul Graham kimdir?")
for r in resp:
    print(r.delta, end="")
```

    Paul Graham, teknoloji dÃ¼nyasÄ±nda tanÄ±nmÄ±ÅŸ bir isimdir; bilgisayar programcÄ±sÄ±, deneme yazarÄ±, risk sermayedarÄ± ve giriÅŸim hÄ±zlandÄ±rÄ±cÄ±sÄ± Y Combinator'Ä±n kurucu ortaÄŸÄ± olarak bilinir. Ä°ÅŸte temel baÅŸarÄ±larÄ± ve katkÄ±larÄ±nÄ±n bir dÃ¶kÃ¼mÃ¼:
    
    *   **Bilgisayar ProgramcÄ±sÄ± ve Yazar:** Graham, Harvard Ãœniversitesi'nden bilgisayar bilimleri alanÄ±nda doktora derecesine sahiptir. Bir programlama dili olan Lisp Ã¼zerindeki Ã§alÄ±ÅŸmalarÄ±yla ve daha sonra Yahoo! tarafÄ±ndan satÄ±n alÄ±narak Yahoo! Store haline gelen ilk yazÄ±lÄ±m-servis (SaaS) ÅŸirketlerinden biri olan Viaweb'i geliÅŸtirmesiyle tanÄ±nÄ±r. AyrÄ±ca "On Lisp", "ANSI Common Lisp", "Hackers & Painters" ve "A Plan for Spam" dahil olmak Ã¼zere programlama ve giriÅŸimcilik Ã¼zerine etkili birkaÃ§ kitabÄ±n yazarÄ±dÄ±r.
    
    *   **Deneme YazarÄ±:** Graham; teknoloji, giriÅŸimler, sanat, felsefe ve toplum dahil Ã§ok Ã§eÅŸitli konularda yazan Ã¼retken bir deneme yazarÄ±dÄ±r. Denemeleri; derin gÃ¶zlemleri, net yazÄ±m stili ve genellikle aykÄ±rÄ± bakÄ±ÅŸ aÃ§Ä±larÄ±yla bilinir. Teknoloji topluluÄŸunda yaygÄ±n olarak okunur ve tartÄ±ÅŸÄ±lÄ±rlar. Denemelerini web sitesi paulgraham.com'da bulabilirsiniz.
    
    *   **Risk SermayedarÄ± ve Y Combinator:** Graham, 2005 yÄ±lÄ±nda Jessica Livingston, Robert Morris ve Trevor Blackwell ile birlikte Y Combinator'Ä± (YC) kurmuÅŸtur. YC, erken aÅŸamadaki giriÅŸimlere tohum yatÄ±rÄ±mÄ±, mentorluk ve aÄŸ oluÅŸturma fÄ±rsatlarÄ± sunan son derece baÅŸarÄ±lÄ± bir giriÅŸim hÄ±zlandÄ±rÄ±cÄ±sÄ±dÄ±r. YC, Airbnb, Dropbox, Reddit, Stripe ve daha pek Ã§ok tanÄ±nmÄ±ÅŸ ÅŸirketi fonlamÄ±ÅŸtÄ±r. 2014 yÄ±lÄ±nda YC'deki gÃ¼nlÃ¼k operasyonlardan ayrÄ±lmÄ±ÅŸ olsa da, organizasyon ve giriÅŸim ekosistemi Ã¼zerindeki etkisi Ã¶nemli olmaya devam etmektedir.
    
    Ã–zetle Paul Graham; bilgisayar bilimi, giriÅŸimcilik ve daha geniÅŸ teknoloji kÃ¼ltÃ¼rÃ¼ne Ã¶nemli katkÄ±larda bulunmuÅŸ Ã§ok yÃ¶nlÃ¼ bir bireydir. Teknik uzmanlÄ±ÄŸÄ±, derinlikli yazÄ±larÄ± ve modern giriÅŸim manzarasÄ±nÄ± ÅŸekillendirmedeki rolÃ¼yle bÃ¼yÃ¼k saygÄ± gÃ¶rmektedir.

```python
from llama_index.core.llms import ChatMessage

messages = [
    ChatMessage(role="user", content="Paul Graham kimdir?"),
]

resp = llm.stream_chat(messages)
for r in resp:
    print(r.delta, end="")
```

    Paul Graham, teknoloji dÃ¼nyasÄ±nda tanÄ±nmÄ±ÅŸ bir isimdir; programcÄ±, deneme yazarÄ± ve risk sermayedarÄ± olarak Ã§alÄ±ÅŸmalarÄ±yla bilinir. Ä°ÅŸte temel katkÄ±larÄ±nÄ±n bir dÃ¶kÃ¼mÃ¼:
    
    *   **ProgramcÄ± ve Hacker:** Ã–zellikle Lisp konusunda yetenekli bir programcÄ±dÄ±r. Daha sonra Yahoo! tarafÄ±ndan satÄ±n alÄ±nan ve Yahoo! Store haline gelen ilk yazÄ±lÄ±m-servis (SaaS) ÅŸirketlerinden biri olan Viaweb'in kurucu ortaklarÄ±ndandÄ±r.
    
    *   **Deneme YazarÄ±:** Graham, programlama ve giriÅŸimlerden sanat, felsefe ve sosyal yorumlara kadar uzanan konularda yazan Ã¼retken ve etkili bir deneme yazarÄ±dÄ±r. Denemeleri netliÄŸi, derinliÄŸi ve genellikle aykÄ±rÄ± bakÄ±ÅŸ aÃ§Ä±larÄ±yla bilinir. Teknoloji topluluÄŸunda yaygÄ±n olarak okunur ve tartÄ±ÅŸÄ±lÄ±rlar.
    
    *   **Risk SermayedarÄ±:** 2005 yÄ±lÄ±nda son derece baÅŸarÄ±lÄ± bir giriÅŸim hÄ±zlandÄ±rÄ±cÄ±sÄ± olan Y Combinator'Ä± (YC) kurmuÅŸtur. YC, Airbnb, Dropbox, Reddit, Stripe ve diÄŸer pek Ã§ok tanÄ±nmÄ±ÅŸ ÅŸirketi fonlamÄ±ÅŸ ve onlara mentorluk yapmÄ±ÅŸtÄ±r. Graham'Ä±n erken aÅŸama yatÄ±rÄ±mcÄ±lÄ±ÄŸÄ±na ve giriÅŸim mentorluÄŸuna yaklaÅŸÄ±mÄ±, giriÅŸim ekosistemi Ã¼zerinde Ã¶nemli bir etki yaratmÄ±ÅŸtÄ±r.
    
    Ã–zetle Paul Graham; programcÄ±, deneme yazarÄ± ve risk sermayedarÄ± olarak teknoloji endÃ¼strisine Ã¶nemli katkÄ±larda bulunmuÅŸ Ã§ok yÃ¶nlÃ¼ bir bireydir. Ã–zellikle Y Combinator ile yaptÄ±ÄŸÄ± Ã§alÄ±ÅŸmalarla giriÅŸim dÃ¼nyasÄ±nda oldukÃ§a etkilidir.

## Asenkron KullanÄ±m

Her senkron yÃ¶ntemin asenkron bir karÅŸÄ±lÄ±ÄŸÄ± vardÄ±r.

```python
from llama_index.llms.google_genai import GoogleGenAI

llm = GoogleGenAI(model="gemini-2.5-flash")

resp = await llm.astream_complete("Paul Graham kimdir?")
async for r in resp:
    print(r.delta, end="")
```

    Paul Graham, teknoloji dÃ¼nyasÄ±nda tanÄ±nmÄ±ÅŸ bir isimdir; programcÄ±, deneme yazarÄ± ve risk sermayedarÄ± olarak Ã§alÄ±ÅŸmalarÄ±yla bilinir. Ä°ÅŸte temel baÅŸarÄ±larÄ± ve rollerinin bir dÃ¶kÃ¼mÃ¼:
    
    *   **ProgramcÄ± ve Hacker:** Harvard'dan bilgisayar bilimleri doktorasÄ±na sahiptir ve bir programlama dili olan Lisp Ã¼zerindeki Ã§alÄ±ÅŸmalarÄ±yla tanÄ±nÄ±r. Daha sonra Yahoo! tarafÄ±ndan satÄ±n alÄ±nan ve Yahoo! Store haline gelen ilk yazÄ±lÄ±m-servis (SaaS) ÅŸirketlerinden biri olan Viaweb'in kurucu ortaklarÄ±ndandÄ±r.
    
    *   **Deneme YazarÄ±:** Graham, programlama ve giriÅŸimlerden sanat, felsefe ve sosyal yorumlara kadar uzanan konularda yazan Ã¼retken ve etkili bir deneme yazarÄ±dÄ±r. Denemeleri teknoloji topluluÄŸunda yaygÄ±n olarak okunur ve tartÄ±ÅŸÄ±lÄ±r.
    
    *   **Risk SermayedarÄ±:** 2005 yÄ±lÄ±nda Airbnb, Dropbox, Reddit, Stripe ve diÄŸer pek Ã§ok ÅŸirketi fonlayan son derece baÅŸarÄ±lÄ± bir giriÅŸim hÄ±zlandÄ±rÄ±cÄ±sÄ± olan Y Combinator'Ä± (YC) kurmuÅŸtur. YC, erken aÅŸamadaki giriÅŸimlere tohum yatÄ±rÄ±mÄ±, mentorluk ve aÄŸ oluÅŸturma fÄ±rsatlarÄ± sunar. 2014 yÄ±lÄ±nda YC'deki gÃ¼nlÃ¼k operasyonlardan geri Ã§ekilmiÅŸ olsa da, risk sermayesi dÃ¼nyasÄ±nda Ã¶nemli bir isim olmaya devam etmektedir.
    
    Ã–zetle Paul Graham; bilgisayar bilimi, giriÅŸimcilik ve risk sermayesi alanlarÄ±na Ã¶nemli katkÄ±larda bulunmuÅŸ Ã§ok yÃ¶nlÃ¼ bir bireydir. Derinlikli yazÄ±larÄ± ve modern giriÅŸim ekosistemini ÅŸekillendirmedeki rolÃ¼yle bÃ¼yÃ¼k saygÄ± gÃ¶rmektedir.

```python
messages = [
    ChatMessage(role="user", content="Paul Graham kimdir?"),
]

resp = await llm.achat(messages)
print(resp)
```

    assistant: Paul Graham, teknoloji dÃ¼nyasÄ±nda tanÄ±nmÄ±ÅŸ bir isimdir; programcÄ±, deneme yazarÄ± ve risk sermayedarÄ± olarak Ã§alÄ±ÅŸmalarÄ±yla bilinir. Ä°ÅŸte temel baÅŸarÄ±larÄ± ve katkÄ±larÄ±nÄ±n bir dÃ¶kÃ¼mÃ¼:
    
    *   **ProgramcÄ± ve Hacker:** Ã–zellikle Lisp konusunda yetenekli bir programcÄ±dÄ±r. Daha sonra Yahoo! tarafÄ±ndan satÄ±n alÄ±nan ve Yahoo! Store haline gelen ilk yazÄ±lÄ±m-servis (SaaS) ÅŸirketlerinden biri olan Viaweb'in kurucu ortaklarÄ±ndandÄ±r.
    
    *   **Deneme YazarÄ±:** Graham, programlama ve giriÅŸimlerden sanat, tasarÄ±m ve toplumsal trendlere kadar uzanan konularda yazan Ã¼retken ve etkili bir deneme yazarÄ±dÄ±r. Denemeleri derinlemesine gÃ¶zlemleri, aykÄ±rÄ± bakÄ±ÅŸ aÃ§Ä±larÄ± ve net yazÄ±m stiliyle tanÄ±nÄ±r. Denemelerinin Ã§oÄŸu web sitesi paulgraham.com'da mevcuttur.
    
    *   **Risk SermayedarÄ± ve Y Combinator:** 2005 yÄ±lÄ±nda Airbnb, Dropbox, Reddit, Stripe ve diÄŸer pek Ã§ok tanÄ±nmÄ±ÅŸ ÅŸirketi fonlayan son derece baÅŸarÄ±lÄ± bir giriÅŸim hÄ±zlandÄ±rÄ±cÄ±sÄ± olan Y Combinator'Ä± (YC) kurmuÅŸtur. YC, erken aÅŸamadaki giriÅŸimlere tohum yatÄ±rÄ±mÄ±, mentorluk ve aÄŸ oluÅŸturma fÄ±rsatlarÄ± sunar. Graham, YC'nin felsefesini ve yatÄ±rÄ±m yaklaÅŸÄ±mÄ±nÄ± ÅŸekillendirmede kilit bir rol oynamÄ±ÅŸtÄ±r.
    
    *   **Yazar:** "On Lisp" ve "Hackers & Painters: Big Ideas from the Age of Enlightenment" dahil olmak Ã¼zere birkaÃ§ kitap yazmÄ±ÅŸtÄ±r.
    
    Ã–zetle Paul Graham; programcÄ±, deneme yazarÄ± ve risk sermayedarÄ± olarak teknoloji endÃ¼strisine Ã¶nemli katkÄ±larda bulunmuÅŸ Ã§ok yÃ¶nlÃ¼ bir bireydir. Ã–zellikle Y Combinator ile yaptÄ±ÄŸÄ± Ã§alÄ±ÅŸmalarla giriÅŸim dÃ¼nyasÄ±nda oldukÃ§a etkilidir.

## Vertex AI DesteÄŸi

`region` ve `project_id` parametrelerini (ortam deÄŸiÅŸkenleri aracÄ±lÄ±ÄŸÄ±yla veya doÄŸrudan) saÄŸlayarak, Vertex AI Ã¼zerinden kullanÄ±mÄ± etkinleÅŸtirebilirsiniz.

```python
# Ortam deÄŸiÅŸkenlerini ayarlayÄ±n
!export GOOGLE_GENAI_USE_VERTEXAI=true
!export GOOGLE_CLOUD_PROJECT='proje-id-niz'
!export GOOGLE_CLOUD_LOCATION='us-central1'
```

```python
from llama_index.llms.google_genai import GoogleGenAI

# veya parametreleri doÄŸrudan ayarlayÄ±n
llm = GoogleGenAI(
    model="gemini-2.5-flash",
    vertexai_config={"project": "proje-id-niz", "location": "us-central1"},
    # baÄŸlam penceresini (context window) model iÃ§in maksimum girdi token'Ä±na ayarlamalÄ±sÄ±nÄ±z
    context_window=200000,
    max_tokens=512,
)
```

    Paul Graham, teknoloji ve giriÅŸim dÃ¼nyasÄ±nda Ã¶nde gelen bir figÃ¼rdÃ¼r ve en Ã§ok ÅŸu rolleriyle tanÄ±nÄ±r:
    
    *   **Y Combinator (YC) Kurucu OrtaÄŸÄ±:** Bu, tartÄ±ÅŸmasÄ±z onun en etkili rolÃ¼dÃ¼r. YC, Airbnb, Dropbox, Stripe, Reddit ve daha pek Ã§ok ÅŸirketi fonlamÄ±ÅŸ son derece baÅŸarÄ±lÄ± bir giriÅŸim hÄ±zlandÄ±rÄ±cÄ±sÄ±dÄ±r. Graham'Ä±n giriÅŸimleri fonlama ve onlara mentorluk yapma yaklaÅŸÄ±mÄ±, giriÅŸim ekosistemini Ã¶nemli Ã¶lÃ§Ã¼de ÅŸekillendirmiÅŸtir.
    
    *   **Deneme YazarÄ± ve ProgramcÄ±:** YC'den Ã¶nce Graham bir programcÄ± ve deneme yazarÄ±ydÄ±. Programlama, giriÅŸimler, tasarÄ±m ve toplumsal trendler gibi geniÅŸ bir yelpazede yazdÄ±ÄŸÄ± derinlikli ve genellikle aykÄ±rÄ± denemeleriyle tanÄ±nÄ±r. Denemeleri teknoloji topluluÄŸunda yaygÄ±n olarak okunur ve tartÄ±ÅŸÄ±lÄ±r.
    
    *   **Viaweb'in Kurucusu (daha sonra Yahoo! Store):** Graham, kullanÄ±cÄ±larÄ±n Ã§evrimiÃ§i maÄŸazalar kurmasÄ±na ve yÃ¶netmesine olanak tanÄ±yan ilk uygulama servis saÄŸlayÄ±cÄ±larÄ±ndan biri olan Viaweb'i kurdu. Åirket, 1998'de Yahoo! tarafÄ±ndan satÄ±n alÄ±ndÄ± ve Yahoo! Store adÄ±nÄ± aldÄ±.
    
    Ã–zetle Paul Graham, Y Combinator'Ä± kurmadaki rolÃ¼, derinlikli denemeleri ve programcÄ± ile giriÅŸimci olarak elde ettiÄŸi erken baÅŸarÄ±larÄ±yla tanÄ±nan, giriÅŸim dÃ¼nyasÄ±nda son derece etkili bir figÃ¼rdÃ¼r.

## Ã–nbelleÄŸe AlÄ±nmÄ±ÅŸ Ä°Ã§erik DesteÄŸi

Google GenAI, bÃ¼yÃ¼k baÄŸlamlarÄ± birden fazla istekte yeniden kullanÄ±rken performansÄ± artÄ±rmak ve maliyet verimliliÄŸi saÄŸlamak iÃ§in Ã¶nbelleÄŸe alÄ±nmÄ±ÅŸ iÃ§eriÄŸi destekler. Bu, Ã¶zellikle RAG uygulamalarÄ±, belge analizi ve tutarlÄ± baÄŸlam gerektiren Ã§ok turlu konuÅŸmalar iÃ§in yararlÄ±dÄ±r.

#### Avantajlar

- **Daha hÄ±zlÄ± yanÄ±tlar**
- Girdi token kullanÄ±mÄ±nÄ±n azalmasÄ±yla **maliyet tasarrufu**
- Birden fazla sorguda **tutarlÄ± baÄŸlam**
- BÃ¼yÃ¼k dosyalarla yapÄ±lan **belge analizi iÃ§in mÃ¼kemmel**

#### Ã–nbelleÄŸe AlÄ±nmÄ±ÅŸ Ä°Ã§erik OluÅŸturma

Ä°lk olarak, Google GenAI SDK'sÄ±nÄ± kullanarak Ã¶nbelleÄŸe alÄ±nmÄ±ÅŸ iÃ§erik oluÅŸturun:

```python
from google import genai
from google.genai.types import CreateCachedContentConfig, Content, Part
import time

client = genai.Client(api_key="api-anahtarÄ±nÄ±z")

# VertexAI iÃ§in
# client = genai.Client(
#     http_options=HttpOptions(api_version="v1"),
#     project="proje-id-niz",
#     location="us-central1",
#     vertexai="True"
# )
```

SeÃ§enek 1: Yerel DosyalarÄ± YÃ¼kleme

```python
# Yerel PDF dosyalarÄ±nÄ± yÃ¼kleyin ve iÅŸleyin
pdf_file = client.files.upload(file="./belgeniz.pdf")
while pdf_file.state.name == "PROCESSING":
    print("PDF'nin iÅŸlenmesi bekleniyor.")
    time.sleep(2)
    pdf_file = client.files.get(name=pdf_file.name)

# YÃ¼klenen dosya ile Ã¶nbellek oluÅŸturun
cache = client.caches.create(
    model="gemini-2.5-flash",
    config=CreateCachedContentConfig(
        display_name="Belge Analizi Ã–nbelleÄŸi",
        system_instruction=(
            "Sen uzman bir belge analizcisisin. SorularÄ± "
            "saÄŸlanan belgelere dayanarak doÄŸruluk ve detayla yanÄ±tla."
        ),
        contents=[pdf_file],  # DoÄŸrudan dosya referansÄ±
        ttl="3600s",  # 1 saat boyunca Ã¶nbelleÄŸe al
    ),
)
```

SeÃ§enek 2: Ä°Ã§erik YapÄ±sÄ±na Sahip Birden Fazla Dosya

```python
# VertexAI ile birden fazla dosya veya Cloud Storage dosyalarÄ± iÃ§in
contents = [
    Content(
        role="user",
        parts=[
            Part.from_uri(
                # file_uri=pdf_file.uri,    # yÃ¼klenen dosyanÄ±n URI'sini de kullanabilirsiniz
                file_uri="gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf",
                mime_type="application/pdf",
            ),
            Part.from_uri(
                file_uri="gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf",
                mime_type="application/pdf",
            ),
        ],
    )
]

cache = client.caches.create(
    model="gemini-2.5-flash",
    config=CreateCachedContentConfig(
        display_name="Ã‡oklu Belge Ã–nbelleÄŸi",
        system_instruction=(
            "Sen uzman bir araÅŸtÄ±rmacÄ±sÄ±n. SaÄŸlanan belgeler "
            "arasÄ±ndaki bilgileri analiz et ve karÅŸÄ±laÅŸtÄ±r."
        ),
        contents=contents,
        ttl="3600s",
    ),
)

print(f"Ã–nbellek oluÅŸturuldu: {cache.name}")
print(f"Ã–nbelleÄŸe alÄ±nan token'lar: {cache.usage_metadata.total_token_count}")
```

    Ã–nbellek oluÅŸturuldu: projects/391.../locations/us-central1/cachedContents/267...
    Ã–nbelleÄŸe alÄ±nan token'lar: 43102

LlamaIndex ile Ã–nbelleÄŸe AlÄ±nmÄ±ÅŸ Ä°Ã§eriÄŸi Kullanma

Ã–nbelleÄŸi oluÅŸturduktan sonra LlamaIndex ile kullanÄ±n:

```python
from llama_index.llms.google_genai import GoogleGenAI
from llama_index.core.llms import ChatMessage

llm = GoogleGenAI(
    model="gemini-2.5-flash",
    api_key="api-anahtarÄ±nÄ±z",
    cached_content=cache.name,
)

# VertexAI iÃ§in
# llm = GoogleGenAI(
#     model="gemini-2.5-flash",
#     vertexai_config={"project": "proje-id-niz", "location": "us-central1"},
#     cached_content=cache.name
# )

# Ã–nbelleÄŸe alÄ±nmÄ±ÅŸ iÃ§eriÄŸi kullanÄ±n
message = ChatMessage(
    role="user", content="BÃ¶lÃ¼m 4'teki temel bulgularÄ± Ã¶zetle."
)
response = llm.chat([message])
print(response)
```

    assistant: BÃ¶lÃ¼m 4, "Soyutlama: SÃ¼reÃ§" (The Abstraction: The Process), iÅŸletim sistemi (OS) tarafÄ±ndan saÄŸlanan temel bir soyutlama olan ve Ã§alÄ±ÅŸan bir program olarak tanÄ±mlanan sÃ¼reÃ§ kavramÄ±nÄ± tanÄ±tÄ±r. Ä°ÅŸte temel bulgular:
    
    1.  **SÃ¼reÃ§ TanÄ±mÄ±:** Bir sÃ¼reÃ§ temel olarak Ã§alÄ±ÅŸan bir programdÄ±r; bellek (adres alanÄ±), kayÄ±tÃ§Ä±lar (program sayacÄ± ve yÄ±ÄŸÄ±n iÅŸaretÃ§isi dahil) ve I/O bilgileri dahil olmak Ã¼zere makine durumuyla karakterize edilir.
    
    2.  **SÃ¼reÃ§ API'si:** Ä°ÅŸletim sistemi; sÃ¼reÃ§ oluÅŸturma (Create), sÃ¼reÃ§ sonlandÄ±rma (Destroy), sÃ¼reÃ§lerin tamamlanmasÄ±nÄ± bekleme (Wait), sÃ¼reÃ§lerin kontrolÃ¼ (Miscellaneous Control) ve durum bilgisi alma (Status) fonksiyonlarÄ±nÄ± iÃ§eren bir sÃ¼reÃ§ API'si saÄŸlar.
    
    3.  **SÃ¼reÃ§ OluÅŸturma:** Bir sÃ¼reÃ§ oluÅŸturmak; kod ve statik verilerin belleÄŸe yÃ¼klenmesini, yÄ±ÄŸÄ±n (stack) ve yÄ±ÄŸÄ±n bellek (heap) iÃ§in bellek ayrÄ±lmasÄ±nÄ±, yÄ±ÄŸÄ±nÄ±n baÅŸlatÄ±lmasÄ±nÄ± ve ardÄ±ndan programÄ±n giriÅŸ noktasÄ±nda (main()) baÅŸlatÄ±lmasÄ±nÄ± iÃ§erir.
    
    4.  **SÃ¼reÃ§ DurumlarÄ±:** Bir sÃ¼reÃ§ Ã¼Ã§ durumdan birinde olabilir: Ã‡alÄ±ÅŸÄ±yor (Running - bir iÅŸlemci Ã¼zerinde yÃ¼rÃ¼tÃ¼lÃ¼yor), HazÄ±r (Ready - Ã§alÄ±ÅŸmaya hazÄ±r ama ÅŸu anda Ã§alÄ±ÅŸmÄ±yor) veya EngellenmiÅŸ (Blocked - I/O tamamlanmasÄ± gibi bir olayÄ± bekliyor).
    
    5.  **Veri YapÄ±larÄ±:** Ä°ÅŸletim sistemi, her sÃ¼recin durumunu takip etmek iÃ§in sÃ¼reÃ§ listesi gibi veri yapÄ±larÄ± tutar. Bu yapÄ±lar, kayÄ±tÃ§Ä± baÄŸlamÄ± (kaydedilmiÅŸ kayÄ±tÃ§Ä± deÄŸerleri) ve sÃ¼reÃ§ durumu gibi bilgileri iÃ§erir.
    
    Ã–zÃ¼nde BÃ¶lÃ¼m 4, sÃ¼reÃ§ kavramÄ±nÄ± ve onunla iliÅŸkili nitelikleri ve durumlarÄ± tanÄ±tarak, iÅŸletim sisteminin CPU'yu nasÄ±l yÃ¶nettiÄŸini ve sanallaÅŸtÄ±rdÄ±ÄŸÄ±nÄ± anlamak iÃ§in temel oluÅŸturur.

Ãœretim YapÄ±landÄ±rmasÄ±nda (Generation Config) Ã–nbelleÄŸe AlÄ±nmÄ±ÅŸ Ä°Ã§eriÄŸi Kullanma

Ä°stek dÃ¼zeyinde Ã¶nbellek kontrolÃ¼ iÃ§in:

```python
import google.genai.types as types

# Ä°stek baÅŸÄ±na Ã¶nbelleÄŸe alÄ±nmÄ±ÅŸ iÃ§eriÄŸi belirtin
config = types.GenerateContentConfig(
    cached_content=cache.name, temperature=0.1, max_output_tokens=1024
)

llm = GoogleGenAI(model="gemini-2.5-flash", generation_config=config)

response = llm.complete("Belgenin ilk beÅŸ bÃ¶lÃ¼mÃ¼nÃ¼ listele")
print(response)
```

    Ä°ÅŸte Ä°Ã§indekiler kÄ±smÄ±nda listelendiÄŸi gibi belgenin ilk beÅŸ bÃ¶lÃ¼mÃ¼:
    
    1.  Kitap Ãœzerine Bir Diyalog
    2.  Ä°ÅŸletim Sistemlerine GiriÅŸ
    3.  SanallaÅŸtÄ±rma Ãœzerine Bir Diyalog
    4.  Soyutlama: SÃ¼reÃ§
    5.  Ara BÃ¶lÃ¼m: SÃ¼reÃ§ API'si

Ã–nbellek YÃ¶netimi

```python
# TÃ¼m Ã¶nbellekleri listele
caches = client.caches.list()
for cache_item in caches:
    print(f"Ã–nbellek: {cache_item.display_name} ({cache_item.name})")
    print(f"Token'lar: {cache_item.usage_metadata.total_token_count}")

# Ã–nbellek detaylarÄ±nÄ± al
cache_info = client.caches.get(name=cache.name)
print(f"OluÅŸturulma: {cache_info.create_time}")
print(f"Sona Erme: {cache_info.expire_time}")

# Ä°ÅŸiniz bittiÄŸinde Ã¶nbelleÄŸi silin
client.caches.delete(name=cache.name)
print("Ã–nbellek silindi")
```

    Ã–nbellek: Belge Analizi Ã–nbelleÄŸi (cachedContents/8v3va2x...)
    Token'lar: 77421
    OluÅŸturulma: 2025-07-08 16:06:11.821190+00:00
    Sona Erme: 2025-07-08 17:06:10.813310+00:00
    Ã–nbellek silindi

## Ã‡ok Modlu (Multi-Modal) Destek

`ChatMessage` nesnelerini kullanarak, LLM'e gÃ¶rÃ¼ntÃ¼ ve metin gÃ¶nderebilirsiniz.

```python
!wget https://cdn.pixabay.com/photo/2021/12/12/20/00/play-6865967_640.jpg -O image.jpg
```

    --2025-03-14 10:59:00--  https://cdn.pixabay.com/photo/2021/12/12/20/00/play-6865967_640.jpg
    cdn.pixabay.com (cdn.pixabay.com) Ã§Ã¶zÃ¼lÃ¼yor... 104.18.40.96, 172.64.147.160
    cdn.pixabay.com (cdn.pixabay.com)|104.18.40.96|:443 baÄŸlanÄ±lÄ±yor... baÄŸlandÄ±.
    HTTP isteÄŸi gÃ¶nderildi, yanÄ±t bekleniyor... 200 OK
    Uzunluk: 71557 (70K) [binary/octet-stream]
    KayÄ±t yeri: â€˜image.jpgâ€™
    
    image.jpg           100%[===================>]  69.88K  --.-KB/s    iÃ§inde 0.003s  
    
    2025-03-14 10:59:00 (24.8 MB/s) - â€˜image.jpgâ€™ kaydedildi [71557/71557]

```python
from llama_index.core.llms import ChatMessage, TextBlock, ImageBlock
from llama_index.llms.google_genai import GoogleGenAI

llm = GoogleGenAI(model="gemini-2.5-flash")

messages = [
    ChatMessage(
        role="user",
        blocks=[
            ImageBlock(path="image.jpg", image_mimetype="image/jpeg"),
            TextBlock(text="Bu gÃ¶rÃ¼ntÃ¼de ne var?"),
        ],
    )
]

resp = llm.chat(messages)
print(resp)
```

    assistant: GÃ¶rÃ¼ntÃ¼de, koyu gri bir yÃ¼zey Ã¼zerinde siyah noktalarÄ± olan dÃ¶rt adet ahÅŸap zar bulunuyor. Her zar farklÄ± sayÄ±da nokta gÃ¶stererek farklÄ± deÄŸerleri iÅŸaret ediyor.

Belgeleri de gÃ¶nderebilirsiniz.

```python
from llama_index.core.llms import DocumentBlock

messages = [
    ChatMessage(
        role="user",
        blocks=[
            DocumentBlock(
                path="/path/to/your/test.pdf",
                document_mimetype="application/pdf",
            ),
            TextBlock(text="Belgeyi tek bir cÃ¼mleyle tanÄ±mla."),
        ],
    )
]

resp = llm.chat(messages)
print(resp)
```

    assistant: Bu araÅŸtÄ±rma makalesi, Crescendo saldÄ±rÄ±sÄ±nÄ± kullanarak son bÃ¼yÃ¼k dil modellerindeki (LLM) Ã§ok turlu "jailbreak" zafiyetlerini deÄŸerlendiriyor ve azaltmaya Ã§alÄ±ÅŸÄ±yor; Ã§eÅŸitli gÃ¶rev kategorilerinde istem (prompt) gÃ¼Ã§lendirme ve bir koruma duvarÄ± olarak LLM (LLM-as-guardrail) stratejilerini analiz ediyor.

Son olarak, videolarÄ± da gÃ¶nderebilirsiniz.

```python
from llama_index.core.llms import VideoBlock

messages = [
    ChatMessage(
        role="user",
        blocks=[
            VideoBlock(
                path="/path/to/your/video.mp4", video_mimetype="video/mp4"
            ),
            TextBlock(text="Bu videoyu tek bir cÃ¼mleyle tanÄ±mla."),
        ],
    )
)

resp = llm.chat(messages)
print(resp)
```

    assistant: Beyaz bir SpaceX Crew Dragon kapsÃ¼lÃ¼nÃ¼n, arka planda DÃ¼nya'nÄ±n kavisli yÃ¼zeyi gÃ¶rÃ¼nÃ¼rken, UluslararasÄ± Uzay Ä°stasyonu'nun bir modÃ¼lÃ¼ne yaklaÅŸtÄ±ÄŸÄ± ve kenetlendiÄŸi gÃ¶rÃ¼lÃ¼yor.

## YapÄ±landÄ±rÄ±lmÄ±ÅŸ Tahmin (Structured Prediction)

LlamaIndex, `structured_predict` aracÄ±lÄ±ÄŸÄ±yla herhangi bir LLM'i yapÄ±landÄ±rÄ±lmÄ±ÅŸ bir LLM'e dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in sezgisel bir arayÃ¼z sunar - sadece hedef Pydantic sÄ±nÄ±fÄ±nÄ± tanÄ±mlayÄ±n (iÃ§ iÃ§e geÃ§miÅŸ olabilir) ve verilen bir istemden istenen nesneyi Ã§Ä±karalÄ±m.

```python
from llama_index.llms.google_genai import GoogleGenAI
from llama_index.core.prompts import PromptTemplate
from llama_index.core.bridge.pydantic import BaseModel
from typing import List


class MenuItem(BaseModel):
    """Bir restorandaki menÃ¼ Ã¶ÄŸesi."""

    course_name: str
    is_vegetarian: bool


class Restaurant(BaseModel):
    """AdÄ±, ÅŸehri ve mutfaÄŸÄ± olan bir restoran."""

    name: str
    city: str
    cuisine: str
    menu_items: List[MenuItem]


llm = GoogleGenAI(model="gemini-2.5-flash")
prompt_tmpl = PromptTemplate(
    "Verilen bir ÅŸehirde bir restoran oluÅŸtur: {city_name}"
)

# SeÃ§enek 1: `as_structured_llm` kullanÄ±n
restaurant_obj = (
    llm.as_structured_llm(Restaurant)
    .complete(prompt_tmpl.format(city_name="Miami"))
    .raw
)
# SeÃ§enek 2: `structured_predict` kullanÄ±n
# restaurant_obj = llm.structured_predict(Restaurant, prompt_tmpl, city_name="Miami")
```

```python
print(restaurant_obj)
```

    name='Pasta Mia' city='Miami' cuisine='Italian' menu_items=[MenuItem(course_name='pasta', is_vegetarian=False)]

#### AkÄ±ÅŸ (Streaming) ile YapÄ±landÄ±rÄ±lmÄ±ÅŸ Tahmin

`as_structured_llm` ile sarÄ±lmÄ±ÅŸ her LLM, `stream_chat` Ã¼zerinden akÄ±ÅŸ desteÄŸi sunar.

```python
from llama_index.core.llms import ChatMessage
from IPython.display import clear_output
from pprint import pprint

input_msg = ChatMessage.from_str("San Francisco'da bir restoran oluÅŸtur")

sllm = llm.as_structured_llm(Restaurant)
stream_output = sllm.stream_chat([input_msg])
for partial_output in stream_output:
    clear_output(wait=True)
    pprint(partial_output.raw.dict())
    restaurant_obj = partial_output.raw

restaurant_obj
```

    {'city': 'San Francisco',
     'cuisine': 'Italian',
     'menu_items': [{'course_name': 'pasta', 'is_vegetarian': False}],
     'name': 'Italian Delight'}


    /var/folders/lw/xwsz_3yj4ln1gvkxhyddbvvw0000gn/T/ipykernel_76091/1885953561.py:11: PydanticDeprecatedSince20: `dict` yÃ¶ntemi kullanÄ±mdan kaldÄ±rÄ±lmÄ±ÅŸtÄ±r; bunun yerine `model_dump` kullanÄ±n. Pydantic V2.0'da kullanÄ±mdan kaldÄ±rÄ±lmÄ±ÅŸ olup V3.0'da kaldÄ±rÄ±lacaktÄ±r. Bkz. https://errors.pydantic.dev/2.10/migration/ adresindeki Pydantic V2 GeÃ§iÅŸ KÄ±lavuzu.
      pprint(partial_output.raw.dict())





    Restaurant(name='Italian Delight', city='San Francisco', cuisine='Italian', menu_items=[MenuItem(course_name='pasta', is_vegetarian=False)])

## AraÃ§/Fonksiyon Ã‡aÄŸÄ±rma (Tool/Function Calling)

Google GenAI, API Ã¼zerinden doÄŸrudan araÃ§/fonksiyon Ã§aÄŸÄ±rmayÄ± destekler. LlamaIndex'i kullanarak bazÄ± temel ajan tabanlÄ± araÃ§ Ã§aÄŸÄ±rma modellerini uygulayabiliriz.

```python
from llama_index.core.tools import FunctionTool
from llama_index.core.llms import ChatMessage
from llama_index.llms.google_genai import GoogleGenAI
from datetime import datetime

llm = GoogleGenAI(model="gemini-2.5-flash")


def get_current_time(timezone: str) -> dict:
    """GeÃ§erli saati getirir"""
    return {
        "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "timezone": timezone,
    }


# AracÄ± tanÄ±mlamak iÃ§in fonksiyon adÄ±nÄ±, tip notasyonlarÄ±nÄ± ve docstring'i kullanÄ±r
tool = FunctionTool.from_defaults(fn=get_current_time)
```

AracÄ± Ã§aÄŸÄ±rmak ve sonucu almak iÃ§in tek bir geÃ§iÅŸ (pass) yapabiliriz:

```python
resp = llm.predict_and_call([tool], "New York'ta ÅŸu an saat kaÃ§?")
print(resp)
```

    {'time': '2025-03-14 10:59:05', 'timezone': 'America/New_York'}

AyrÄ±ca, bir ajan tabanlÄ± araÃ§ Ã§aÄŸÄ±rma dÃ¶ngÃ¼sÃ¼ uygulamak iÃ§in daha dÃ¼ÅŸÃ¼k seviyeli API'leri de kullanabiliriz!

```python
chat_history = [
    ChatMessage(role="user", content="New York'ta ÅŸu an saat kaÃ§?")
]
tools_by_name = {t.metadata.name: t for t in [tool]}

resp = llm.chat_with_tools([tool], chat_history=chat_history)
tool_calls = llm.get_tool_calls_from_response(
    resp, error_on_no_tool_call=False
)

if not tool_calls:
    print(resp)
else:
    while tool_calls:
        # LLM'in yanÄ±tÄ±nÄ± sohbet geÃ§miÅŸine ekle
        chat_history.append(resp.message)

        for tool_call in tool_calls:
            tool_name = tool_call.tool_name
            tool_kwargs = tool_call.tool_kwargs

            print(f"{tool_name} aracÄ± {tool_kwargs} ile Ã§aÄŸrÄ±lÄ±yor")
            tool_output = tool.call(**tool_kwargs)
            print("AraÃ§ Ã§Ä±ktÄ±sÄ±: ", tool_output)
            chat_history.append(
                ChatMessage(
                    role="tool",
                    content=str(tool_output),
                    # Gemini, Anthropic, OpenAI gibi Ã§oÄŸu LLM'in araÃ§ Ã§aÄŸrÄ± kimliÄŸini (tool call id) bilmesi gerekir
                    additional_kwargs={"tool_call_id": tool_call.tool_id},
                )
            )

            resp = llm.chat_with_tools([tool], chat_history=chat_history)
            tool_calls = llm.get_tool_calls_from_response(
                resp, error_on_no_tool_call=False
            )
    print("Son yanÄ±t: ", resp.message.content)
```

    get_current_time aracÄ± {'timezone': 'America/New_York'} ile Ã§aÄŸrÄ±lÄ±yor
    AraÃ§ Ã§Ä±ktÄ±sÄ±:  {'time': '2025-03-14 10:59:06', 'timezone': 'America/New_York'}
    Son yanÄ±t:  New York'ta ÅŸu an saat 2025-03-14 10:59:06.

AyrÄ±ca tek bir istekte birden fazla aracÄ± aynÄ± anda Ã§aÄŸÄ±rabiliriz; bu da farklÄ± bilgi tÃ¼rleri gerektiren karmaÅŸÄ±k sorgular iÃ§in verimlilik saÄŸlar.

```python
# SÄ±caklÄ±k iÃ§in baÅŸka bir araÃ§ tanÄ±mlayÄ±n
def get_temperature(city: str) -> dict:
    """Bir ÅŸehir iÃ§in mevcut sÄ±caklÄ±ÄŸÄ± getirir"""
    return {
        "city": city,
        "temperature": "25Â°C",
    }


# Fonksiyonlardan araÃ§lar oluÅŸturun
tool1 = FunctionTool.from_defaults(fn=get_current_time)
tool2 = FunctionTool.from_defaults(fn=get_temperature)

# Her iki aracÄ± da gerektiren bir soru sorun
chat_history = [
    ChatMessage(
        role="user",
        content="New York'ta ÅŸu an saat ve sÄ±caklÄ±k nedir?",
    )
]

# Model hangi araÃ§larÄ± Ã§aÄŸÄ±racaÄŸÄ±na akÄ±llÄ±ca karar verecektir
resp = llm.chat_with_tools([tool1, tool2], chat_history=chat_history)
tool_calls = llm.get_tool_calls_from_response(
    resp, error_on_no_tool_call=False
)

print(f"Model {len(tool_calls)} araÃ§ Ã§aÄŸrÄ±sÄ± yaptÄ±:")
for i, tool_call in enumerate(tool_calls, 1):
    print(f"{i}. {tool_call.tool_name} parametreler: {tool_call.tool_kwargs}")
```

    Model 2 araÃ§ Ã§aÄŸrÄ±sÄ± yaptÄ±:
    1. get_current_time parametreler: {'timezone': 'America/New_York'}
    2. get_temperature parametreler: {'city': 'New York'}

## Google Arama Temellendirme (Google Search Grounding)

Google Gemini 2.0 ve 2.5 modelleri, modelin gerÃ§ek zamanlÄ± bilgi aramasÄ±na ve yanÄ±tlarÄ±nÄ± web arama sonuÃ§larÄ±yla temellendirmesine olanak tanÄ±yan Google Arama temellendirmesini destekler. Bu, Ã¶zellikle gÃ¼ncel bilgileri almak iÃ§in yararlÄ±dÄ±r.

`built_in_tool` parametresi, modelin yanÄ±tlarÄ±nÄ± Google Arama sonuÃ§larÄ±ndan gelen gerÃ§ek dÃ¼nya verileriyle temellendirmesini saÄŸlayan Google Arama araÃ§larÄ±nÄ± kabul eder.

```python
from llama_index.llms.google_genai import GoogleGenAI
from llama_index.core.llms import ChatMessage
from google.genai import types

# Google Arama temellendirme aracÄ± oluÅŸturun
grounding_tool = types.Tool(google_search=types.GoogleSearch())

llm = GoogleGenAI(
    model="gemini-2.5-flash",
    built_in_tool=grounding_tool,
)

resp = llm.complete("ABD'deki bir sonraki tam gÃ¼neÅŸ tutulmasÄ± ne zaman?")
print(resp)
```

    Amerika BirleÅŸik Devletleri'nde gÃ¶rÃ¼lecek bir sonraki tam gÃ¼neÅŸ tutulmasÄ± 23 AÄŸustos 2044 tarihinde gerÃ§ekleÅŸecek. Ancak bÃ¼tÃ¼nlÃ¼k (totality) sadece Montana, Kuzey Dakota ve GÃ¼ney Dakota'da gÃ¶rÃ¼lebilecek. Bir baÅŸka tam gÃ¼neÅŸ tutulmasÄ± ise 12 AÄŸustos 2045'te Kaliforniya'dan Florida'ya uzanan bir hat Ã¼zerinde gerÃ§ekleÅŸecek.

Google Arama temellendirme aracÄ± birkaÃ§ avantaj saÄŸlar:

- **GerÃ§ek zamanlÄ± bilgi**: GÃ¼ncel olaylara ve en son verilere eriÅŸim
- **Olgusal doÄŸruluk**: GerÃ§ek arama sonuÃ§larÄ±na dayanan yanÄ±tlar
- **Kaynak atfÄ±**: Temellendirme meta verileri arama kaynaklarÄ±nÄ± iÃ§erir
- **Otomatik arama kararlarÄ±**: Model, sorguya gÃ¶re ne zaman arama yapacaÄŸÄ±na karar verir

Temellendirme aracÄ±nÄ± sohbet mesajlarÄ±yla da kullanabilirsiniz:

```python
# Sohbet mesajlarÄ±yla Google Arama'yÄ± kullanma
messages = [ChatMessage(role="user", content="Euro 2024'Ã¼ kim kazandÄ±?")]

resp = llm.chat(messages)
print(resp)

# Ham yanÄ±ttan temellendirme meta verilerine eriÅŸebilirsiniz
if hasattr(resp, "raw") and "grounding_metadata" in resp.raw:
    print(resp.raw["grounding_metadata"])
else:
    print("\nBu yanÄ±tta temellendirme meta verisi bulunamadÄ±")
```

    assistant: Ä°spanya, finalde Ä°ngiltere'yi 2-1 maÄŸlup ederek Euro 2024'Ã¼ kazandÄ±. MaÃ§ Berlin'deki Olimpiyat StadÄ±'nda (Olympiastadion) oynandÄ±. Bu zafer, Ä°spanya'nÄ±n dÃ¶rdÃ¼ncÃ¼ Avrupa ÅampiyonasÄ± ÅŸampiyonluÄŸu oldu ve turnuvadaki en Ã§ok galibiyet sayÄ±sÄ±nda Almanya'yÄ± geride bÄ±raktÄ±.
    
    {'grounding_chunks': [{'retrieved_context': None, 'web': {'domain': None, 'title': 'olympics.com', 'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEkqnG_iRjkf89rilwO5fSBjbAADgm-Ad83fhYOhtAgW2qoG5Y8Gkselc-GshmvpqgMzke0vSUmkc6B8WwmXuxGBl9IPk3YWsytW2nOvGo1n8MlxqcrCpP62vvqjYFoo3wDQsb-tZ3RfZYTjKSTdKfVEBhvSfi4wSKMIgbnQkRx50DLqr2w3sjYI3hyZGWdsFyJFfviXdPSnVCZqQ=='}}, {'retrieved_context': None, 'web': {'domain': None, 'title': 'aljazeera.com', 'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFHwRYxryu8EgG5hG-Gwgdn9sRn88H8iehIOG7KPis7rpJcRo35EAc0onyC_5hqcjUozIddtikyjHmUdK2oIBX8_3ENpLTqpu8TyYb97EibGX6_-ZtRtlPnOsd4TukiRVwfiWMk5sk9FZCsNUEFTWb9OJzPhSjOiAPW78aoAQkM9LSKLBY5vBNyQtUsNvb7k6WEd23pHAKtofxi5i7W_qYrtZPiSkqOBTqtyJ2N69oYDw=='}}, {'retrieved_context': None, 'web': {'domain': None, 'title': 'wikipedia.org', 'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2WEgQILX6A9y0uLZzBXY9UsduYELn9ahnW-FBNNHBvTQPWkuc_9cwyKmUEbfx0iton_BcIGh_85ibG5hkoE3kPvyBFfh6dEdy3UG2Vvn9gIprxruYLiUKtx8o6I06ZyFiERJqUzboU8s8Dvbd'}}, {'retrieved_context': None, 'web': {'domain': None, 'title': 'thehindu.com', 'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHqXK-zKOuGkYtQFyc48K49_TYwib-bRIvPqnn5UmjUcVI69vTxIiXnpXXkJtSMHa5-cBZ6Ht_4cAuWs5GuKZSzHeAQ-sHJQ2BEk52qIzjTvSteXGf7v0oBOQ_AUTqdTOpH8vXEVhqnp3o6WFVchKfexDT2sk1IDBqlqLxqQrKD9PrMsMOvU8_kfuGqH3IR_V2GHHnrPgwgR93LpiYvFdtVDlo3Wi12kj1FAgqDHHjkqyZpSc-pJ-522x0VgcdKGX6mXZ0Ssd7-aLK0YYO028ex6-o8ZeKEqeSpC9H7GP3bnw=='}}], 'grounding_supports': [{'confidence_scores': [0.97524184, 0.950235, 0.64699775], 'grounding_chunk_indices': [0, 1, 2], 'segment': {'end_index': 55, 'part_index': None, 'start_index': None, 'text': "Ä°spanya, finalde Ä°ngiltere'yi 2-1 maÄŸlup ederek Euro 2024'Ã¼ kazandÄ±"}}, {'confidence_scores': [0.9290034, 0.9209086], 'grounding_chunk_indices': [2, 3], 'segment': {'end_index': 109, 'part_index': None, 'start_index': 57, 'text': "MaÃ§ Berlin'deki Olimpiyat StadÄ±'nda oynandÄ±"}}, {'confidence_scores': [0.842964, 0.0068578157], 'grounding_chunk_indices': [2, 1], 'segment': {'end_index': 229, 'part_index': None, 'start_index': 111, 'text': "Bu zafer, Ä°spanya'nÄ±n dÃ¶rdÃ¼ncÃ¼ Avrupa ÅampiyonasÄ± ÅŸampiyonluÄŸu oldu ve turnuvadaki en Ã§ok galibiyet sayÄ±sÄ±nda Almanya'yÄ± geride bÄ±raktÄ±"}}], 'retrieval_metadata': {'google_search_dynamic_retrieval_score': None}, 'retrieval_queries': None, 'search_entry_point': {'rendered_content': '...', 'sdk_blob': None}, 'web_search_queries': ['euro 2024\'Ã¼ kim kazandÄ±']}

## Kod YÃ¼rÃ¼tme (Code Execution)

`built_in_tool` parametresi ayrÄ±ca, modelin sorunlarÄ± Ã§Ã¶zmek, hesaplamalar yapmak ve verileri analiz etmek iÃ§in Python kodu yazmasÄ±na ve yÃ¼rÃ¼tmesine olanak tanÄ±yan kod yÃ¼rÃ¼tme araÃ§larÄ±nÄ± da kabul eder. Bu, Ã¶zellikle matematiksel hesaplamalar, veri analizi ve gÃ¶rselleÅŸtirmeler oluÅŸturmak iÃ§in yararlÄ±dÄ±r.

```python
from llama_index.llms.google_genai import GoogleGenAI
from llama_index.core.llms import ChatMessage
from google.genai import types

# Kod yÃ¼rÃ¼tme aracÄ± oluÅŸturun
code_execution_tool = types.Tool(code_execution=types.ToolCodeExecution())

llm = GoogleGenAI(
    model="gemini-2.5-flash",
    built_in_tool=code_execution_tool,
)

resp = llm.complete("20. fibonacci sayÄ±sÄ±nÄ± hesapla.")
print(resp)
```

    Tamam, 20. Fibonacci sayÄ±sÄ±nÄ± hesaplayabilirim. Bunun iÃ§in bir python betiÄŸi kullanacaÄŸÄ±m.
    
    
    20. Fibonacci sayÄ±sÄ± 6765'tir.

### Kod YÃ¼rÃ¼tme DetaylarÄ±na EriÅŸim

Model kod yÃ¼rÃ¼tmeyi kullandÄ±ÄŸÄ±nda, yÃ¼rÃ¼tÃ¼len koda, sonuÃ§lara ve diÄŸer meta verilere ham yanÄ±t Ã¼zerinden eriÅŸebilirsiniz. Bu ÅŸunlarÄ± iÃ§erir:

- **executable_code**: GerÃ§ekten yÃ¼rÃ¼tÃ¼len Python kodu
- **code_execution_result**: Kodun Ã§alÄ±ÅŸtÄ±rÄ±lmasÄ±ndan elde edilen Ã§Ä±ktÄ±
- **text**: Modelin aÃ§Ä±klamasÄ± ve yorumu

Bunu iÅŸ baÅŸÄ±nda gÃ¶relim:

```python
# Kod yÃ¼rÃ¼tmeyi kullanmasÄ± muhtemel bir hesaplama isteyin
messages = [
    ChatMessage(
        role="user", content="Ä°lk 50 asal sayÄ±nÄ±n toplamÄ± nedir?"
    )
]

resp = llm.chat(messages)

# Kod yÃ¼rÃ¼tme detaylarÄ±nÄ± gÃ¶rmek iÃ§in ham yanÄ±ta eriÅŸin
if hasattr(resp, "raw") and "content" in resp.raw:
    parts = resp.raw["content"].get("parts", [])

    for i, part in enumerate(parts):
        print(f"ParÃ§a {i+1}:")

        if "text" in part and part["text"]:
            print(f"  Metin: {part['text'][:100]}", end="")
            print(" ..." if len(part["text"]) > 100 else "")

        if "executable_code" in part and part["executable_code"]:
            print(f"  YÃ¼rÃ¼tÃ¼lebilir Kod: {part['executable_code']}")

        if "code_execution_result" in part and part["code_execution_result"]:
            print(f"  Kod Sonucu: {part['code_execution_result']}")
else:
    print("Ham yanÄ±tta detaylÄ± parÃ§alar bulunamadÄ±")
```

    ParÃ§a 1:
      Metin: Tamam, ilk 50 asal sayÄ±nÄ±n toplamÄ±nÄ± hesaplamam gerekiyor. Bunun iÃ§in bir python betiÄŸi kullanabilir ...
    ParÃ§a 2:
      YÃ¼rÃ¼tÃ¼lebilir Kod: {'code': "def is_prime(n):\n    if n <= 1:\n        return False\n    if n <= 3:\n        return True\n    if n % 2 == 0 or n % 3 == 0:\n        return False\n    i = 5\n    while i * i <= n:\n        if n % i == 0 or n % (i + 2) == 0:\n            return False\n        i += 6\n    return True\n\nprimes = []\nnum = 2\nwhile len(primes) < 50:\n    if is_prime(num):\n        primes.append(num)\n    num += 1\n\nprint(f'{sum(primes)=}')\n", 'language': <Language.PYTHON: 'PYTHON'>}
    ParÃ§a 3:
      Kod Sonucu: {'outcome': <Outcome.OUTCOME_OK: 'OUTCOME_OK'>, 'output': 'sum(primes)=5117\n'}
    ParÃ§a 4:
      Metin: Ä°lk 50 asal sayÄ±nÄ±n toplamÄ± 5117'dir.

## GÃ¶rÃ¼ntÃ¼ OluÅŸturma (Image Generation)

SeÃ§ili modeller, gÃ¶rÃ¼ntÃ¼ girdilerinin yanÄ± sÄ±ra gÃ¶rÃ¼ntÃ¼ Ã§Ä±ktÄ±larÄ±nÄ± da destekler. `response_modalities` yapÄ±landÄ±rmasÄ±nÄ± kullanarak bir Gemini modeliyle gÃ¶rÃ¼ntÃ¼ler oluÅŸturabilir ve dÃ¼zenleyebiliriz!

```python
from llama_index.llms.google_genai import GoogleGenAI
import google.genai.types as types

config = types.GenerateContentConfig(
    temperature=0.1, response_modalities=["Text", "Image"]
)

llm = GoogleGenAI(
    model="gemini-2.5-flash-image-preview", generation_config=config
)
```

```python
from llama_index.core.llms import ChatMessage, TextBlock, ImageBlock

messages = [
    ChatMessage(role="user", content="LÃ¼tfen sevimli bir kÃ¶pek gÃ¶rÃ¼ntÃ¼sÃ¼ oluÅŸtur")
]

resp = llm.chat(messages)
```

```python
from PIL import Image
from IPython.display import display

for block in resp.message.blocks:
    if isinstance(block, ImageBlock):
        image = Image.open(block.resolve_image())
        display(image)
    elif isinstance(block, TextBlock):
        print(block.text)
```

    Ä°ÅŸte senin iÃ§in sevimli bir kÃ¶pek! 

![png](output_62_1.png)

GÃ¶rÃ¼ntÃ¼yÃ¼ de dÃ¼zenleyebiliriz!

```python
messages.append(resp.message)
messages.append(
    ChatMessage(
        role="user",
        content="LÃ¼tfen gÃ¶rÃ¼ntÃ¼yÃ¼ kÃ¶peÄŸi mini schnauzer yapacak ÅŸekilde dÃ¼zenle, ancak genel pozu, Ã§erÃ§evelemeyi, arka planÄ± ve sanat stilini aynÄ± tut.",
    )
)

resp = llm.chat(messages)

for block in resp.message.blocks:
    if isinstance(block, ImageBlock):
        image = Image.open(block.resolve_image())
        display(image)
    elif isinstance(block, TextBlock):
        print(block.text)
```

    Ä°ÅŸte mini schnauzer'Ä±n! 

![png](output_64_1.png)