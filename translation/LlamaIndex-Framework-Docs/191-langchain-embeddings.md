# LangChain Embedding'leri

LlamaIndex, `LangchainEmbedding` sarmalay覺c覺s覺 (wrapper) arac覺l覺覺yla t羹m LangChain embedding modellerini kullanman覺za olanak tan覺r.

Bu not defterini Colab'da a癟覺yorsan覺z, muhtemelen LlamaIndex'i  kurman覺z gerekecektir.

```python
%pip install llama-index-embeddings-langchain
%pip install langchain
```

```python
!pip install llama-index
```

## LangChain Embedding Modelini Kurma

Aa覺daki 繹rnekte, LangChain'in `HuggingFaceEmbeddings` modelini LlamaIndex ile nas覺l kullanaca覺n覺z覺 g繹rebilirsiniz.

```python
from llama_index.embeddings.langchain import LangchainEmbedding
from langchain_community.embeddings import HuggingFaceEmbeddings

# LangChain modelini oluturun
lc_embed_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2"
)

# LlamaIndex sarmalay覺c覺s覺 ile paketleyin
embed_model = LangchainEmbedding(lc_embed_model)
```

## Kullan覺m

```python
embeddings = embed_model.get_text_embedding("Bu bir test metnidir.")
print(len(embeddings))
print(embeddings[:5])
```

## RAG Boru Hatt覺nda Kullan覺m

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings

# Global ayar olarak atay覺n
Settings.embed_model = embed_model

# 襤ndeks oluturun
documents = SimpleDirectoryReader("./data").load_data()
index = VectorStoreIndex.from_documents(documents)

# Sorgulama yap覺n
query_engine = index.as_query_engine()
response = query_engine.query("Metin ne hakk覺nda?")
print(response)
```