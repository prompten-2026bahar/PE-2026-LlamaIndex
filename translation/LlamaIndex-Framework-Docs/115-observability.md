# GÃ¶zlemlenebilirlik (Observability)

LlamaIndex, Ã¼retim ortamÄ±nda ilkeli (principled) LLM uygulamalarÄ± oluÅŸturmanÄ±za olanak tanÄ±mak iÃ§in **tek tÄ±kla gÃ¶zlemlenebilirlik** ğŸ”­ saÄŸlar.

Verileriniz Ã¼zerinde geliÅŸtirilen LLM uygulamalarÄ±nÄ±n (RAG sistemleri, ajanlar) ilkeli bir ÅŸekilde geliÅŸtirilmesi iÃ§in temel gereksinim; hem sistemin tamamÄ±nÄ± hem de her bir bileÅŸeni gÃ¶zlemleyebilmek, hata ayÄ±klayabilmek ve deÄŸerlendirebilmektir.

Bu Ã¶zellik, LlamaIndex kÃ¼tÃ¼phanesini ortaklarÄ±mÄ±zÄ±n sunduÄŸu gÃ¼Ã§lÃ¼ gÃ¶zlemlenebilirlik/deÄŸerlendirme araÃ§larÄ±yla sorunsuz bir ÅŸekilde entegre etmenize olanak tanÄ±r. Bir deÄŸiÅŸkeni bir kez yapÄ±landÄ±rdÄ±ÄŸÄ±nÄ±zda, aÅŸaÄŸÄ±dakiler gibi iÅŸlemleri yapabileceksiniz:

-   LLM/istem (prompt) giriÅŸlerini/Ã§Ä±kÄ±ÅŸlarÄ±nÄ± gÃ¶rÃ¼ntÃ¼leme
-   Herhangi bir bileÅŸenin (LLM'ler, embedding'ler) Ã§Ä±ktÄ±larÄ±nÄ± beklenen ÅŸekilde performans gÃ¶sterdiÄŸinden emin olma
-   Hem indeksleme hem de sorgulama iÃ§in Ã§aÄŸrÄ± izlerini (call traces) gÃ¶rÃ¼ntÃ¼leme

Her saÄŸlayÄ±cÄ±nÄ±n benzerlikleri ve farklÄ±lÄ±klarÄ± vardÄ±r. Her biri iÃ§in tam kÄ±lavuz setine aÅŸaÄŸÄ±dan gÃ¶z atÄ±n!

**NOT:**

GÃ¶zlemlenebilirlik artÄ±k [`instrumentation` modÃ¼lÃ¼](/python/framework/module_guides/observability/instrumentation) (v0.10.20 ve sonrasÄ± sÃ¼rÃ¼mlerde mevcuttur) aracÄ±lÄ±ÄŸÄ±yla yÃ¶netilmektedir.

Bu sayfada bahsedilen araÃ§larÄ±n ve entegrasyonlarÄ±n Ã§oÄŸu eski `CallbackManager` yapÄ±mÄ±zÄ± kullanÄ±r veya `set_global_handler` kullanmaz. Bu entegrasyonlarÄ± buna gÃ¶re iÅŸaretledik!

## KullanÄ±m KalÄ±bÄ± (Usage Pattern)

EtkinleÅŸtirmek iÃ§in genellikle sadece aÅŸaÄŸÄ±dakini yapmanÄ±z yeterlidir:

```python
from llama_index.core import set_global_handler

# genel kullanÄ±m
set_global_handler("<handler_adi>", **kwargs)
```

`set_global_handler` fonksiyonuna verilen tÃ¼m `kwargs` argÃ¼manlarÄ±nÄ±n altta yatan geri arama iÅŸleyicisine (callback handler) aktarÄ±ldÄ±ÄŸÄ±nÄ± unutmayÄ±n.

Ä°ÅŸte bu kadar! Ã‡alÄ±ÅŸtÄ±rma iÅŸlemleri sorunsuz bir ÅŸekilde ilgili hizmete aktarÄ±lacak ve uygulamanÄ±zÄ±n Ã§alÄ±ÅŸtÄ±rma izlerini gÃ¶rÃ¼ntÃ¼leme gibi Ã¶zelliklere eriÅŸebileceksiniz.

## Entegrasyonlar

### OpenTelemetry

[OpenTelemetry](https://opentelemetry.io), Ã§ok sayÄ±da arka uÃ§ entegrasyonuna (Jaeger, Zipkin veya Prometheus gibi) sahip, izleme (tracing) ve gÃ¶zlemlenebilirlik iÃ§in yaygÄ±n olarak kullanÄ±lan bir aÃ§Ä±k kaynak hizmettir.

OpenTelemetry entegrasyonumuz; LLM'ler, Ajanlar, RAG boru hattÄ± bileÅŸenleri ve Ã§ok daha fazlasÄ± dahil olmak Ã¼zere LlamaIndex kodu tarafÄ±ndan Ã¼retilen tÃ¼m olaylarÄ± izler: LlamaIndex yerel enstrÃ¼mantasyonuyla elde edebileceÄŸiniz her ÅŸeyi OpenTelemetry formatÄ±nda dÄ±ÅŸa aktarabilirsiniz!

KÃ¼tÃ¼phaneyi ÅŸu komutla yÃ¼kleyebilirsiniz:

```bash
pip install llama-index-observability-otel
```

Ve bir RAG boru hattÄ± iÃ§eren bu Ã¶rnekte olduÄŸu gibi, varsayÄ±lan ayarlarla kodunuzda kullanabilirsiniz:

```python
from llama_index.observability.otel import LlamaIndexOpenTelemetry
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import Settings

# enstrÃ¼mantasyon nesnesini baÅŸlat
instrumentor = LlamaIndexOpenTelemetry()

if __name__ == "__main__":
    embed_model = OpenAIEmbedding(model_name="text-embedding-3-small")
    llm = OpenAI(model="gpt-4o-mini")

    # dinlemeye baÅŸla!
    instrumentor.start_registering()

    # olaylarÄ± kaydet
    documents = SimpleDirectoryReader(
        input_dir="./data/paul_graham/"
    ).load_data()

    index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)
    query_engine = index.as_query_engine(llm=llm)

    query_result_one = query_engine.query("Paul kimdir?")
    query_result_two = query_engine.query("Paul ne yaptÄ±?")
```

Veya aÅŸaÄŸÄ±daki Ã¶rnekte olduÄŸu gibi daha karmaÅŸÄ±k ve Ã¶zelleÅŸtirilmiÅŸ bir kurulum kullanabilirsiniz:

```python
import json
from pydantic import BaseModel, Field
from typing import List

from llama_index.observability.otel import LlamaIndexOpenTelemetry
from opentelemetry.exporter.otlp.proto.http.trace_exporter import (
    OTLPSpanExporter,
)

# Ã¶zel bir span (iz parÃ§asÄ±) dÄ±ÅŸa aktarÄ±cÄ± tanÄ±mla
span_exporter = OTLPSpanExporter("http://0.0.0.0:4318/v1/traces")

# enstrÃ¼mantasyon nesnesini baÅŸlat
instrumentor = LlamaIndexOpenTelemetry(
    service_name_or_resource="benim.test.servisim.1",
    span_exporter=span_exporter,
    debug=True,
)


if __name__ == "__main__":
    instrumentor.start_registering()
    # ... kodunuz buraya
```

AyrÄ±ca agentic iÅŸ akÄ±ÅŸlarÄ±nÄ±n nasÄ±l izleneceÄŸini ve kayÄ±tlÄ± izlerin bir Postgres veritabanÄ±na nasÄ±l aktarÄ±lacaÄŸÄ±nÄ± gÃ¶sterdiÄŸimiz bir [demo depomuz](https://github.com/run-llama/agents-observability-demo) da bulunmaktadÄ±r.

### LlamaTrace (BarÄ±ndÄ±rÄ±lan Arize Phoenix)

Arize ile, LlamaIndex aÃ§Ä±k kaynak kullanÄ±cÄ±larÄ±yla yerel olarak Ã§alÄ±ÅŸan ve LlamaCloud ile entegrasyonlarÄ± olan barÄ±ndÄ±rÄ±lan bir izleme, gÃ¶zlemlenebilirlik ve deÄŸerlendirme platformu olan [LlamaTrace](https://llamatrace.com/) Ã¼zerinde ortaklÄ±k kurduk.

Bu, aÃ§Ä±k kaynaklÄ± Arize [Phoenix](https://github.com/Arize-ai/phoenix) projesi Ã¼zerine inÅŸa edilmiÅŸtir. Phoenix, modellerinizi ve LLM uygulamalarÄ±nÄ±zÄ± izlemek iÃ§in "Ã¶nce not defteri" (notebook-first) bir deneyim sunar:

-   **LLM Ä°zleri (Traces)**: LLM uygulamanÄ±zÄ±n dahili iÅŸleyiÅŸini anlamak; getirme (retrieval) ve araÃ§ Ã§alÄ±ÅŸtÄ±rma gibi konulardaki sorunlarÄ± gidermek iÃ§in uygulamanÄ±zÄ±n yÃ¼rÃ¼tÃ¼lmesini izleyin.
-   **LLM DeÄŸerlendirmeleri (Evals)**: Ãœretken modelinizin veya uygulamanÄ±zÄ±n uygunluÄŸunu, toksisitesini ve daha fazlasÄ±nÄ± deÄŸerlendirmek iÃ§in bÃ¼yÃ¼k dil modellerinin gÃ¼cÃ¼nden yararlanÄ±n.

#### KullanÄ±m KalÄ±bÄ±

Entegrasyon paketini yÃ¼klemek iÃ§in `pip install -U llama-index-callbacks-arize-phoenix` komutunu kullanÄ±n.

ArdÄ±ndan LlamaTrace Ã¼zerinde bir hesap oluÅŸturun: https://llamatrace.com/login. Bir API anahtarÄ± oluÅŸturun ve bunu aÅŸaÄŸÄ±daki `PHOENIX_API_KEY` deÄŸiÅŸkenine yerleÅŸtirin.

ArdÄ±ndan aÅŸaÄŸÄ±daki kodu Ã§alÄ±ÅŸtÄ±rÄ±n:

```python
# Phoenix, LlamaIndex uygulamanÄ±zdan otomatik olarak toplanan izleri 
# gerÃ§ek zamanlÄ± olarak gÃ¶rÃ¼ntÃ¼leyebilir.
# TÃ¼m LlamaIndex uygulamalarÄ±nÄ±zÄ± her zamanki gibi Ã§alÄ±ÅŸtÄ±rÄ±n; izler 
# toplanacak ve Phoenix'te gÃ¶rÃ¼ntÃ¼lenecektir.

# gÃ¼nlÃ¼kleme/gÃ¶zlemlenebilirlik iÃ§in Arize Phoenix'i kurun
import llama_index.core
import os

PHOENIX_API_KEY = "<PHOENIX_API_KEY>"
os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"api_key={PHOENIX_API_KEY}"
llama_index.core.set_global_handler(
    "arize_phoenix", endpoint="https://llamatrace.com/v1/traces"
)

...
```

#### KÄ±lavuzlar

-   [LlamaTrace ile LlamaCloud AjanÄ±](https://github.com/run-llama/llamacloud-demo/blob/main/examples/tracing/llamacloud_tracing_phoenix.ipynb)

![](./../../_static/integrations/arize_phoenix.png)

### SigNoz

[SigNoz](https://signoz.io/), aÃ§Ä±k kaynaklÄ± bir gÃ¶zlemlenebilirlik Ã§erÃ§evesidir. Yerel olarak OpenTelemetry Ã¼zerinden inÅŸa edilmiÅŸtir; izleri, gÃ¼nlÃ¼kleri ve metrikleri tek bir panelde sunar; hem kendi kendine barÄ±ndÄ±rma (self-hosted) hem de bulut daÄŸÄ±tÄ±m seÃ§eneklerine sahiptir. LlamaIndex ile SigNoz kullanarak, tÃ¼m RAG ve Ajan iÅŸ akÄ±ÅŸlarÄ±nÄ±n ayrÄ±ntÄ±lÄ± izlerini gÃ¶rÃ¼ntÃ¼leyebilir; token kullanÄ±mÄ±, gecikme, hata oranlarÄ±, LLM model daÄŸÄ±lÄ±mÄ± ve Ã§ok daha fazlasÄ± gibi Ã¶nemli metrikleri takip edebilirsiniz.

#### KullanÄ±m KalÄ±bÄ±

AÅŸaÄŸÄ±daki baÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kleyin:

```bash
pip install \
  opentelemetry-distro \
  opentelemetry-exporter-otlp \
  opentelemetry-instrumentation-httpx \
  opentelemetry-instrumentation-system-metrics \
  llama-index \
  openinference-instrumentation-llama-index
```

ArdÄ±ndan, otomatik enstrÃ¼mantasyonu ekleyin:

```bash
opentelemetry-bootstrap --action=install
```

SonrasÄ±nda, LlamaIndex uygulamanÄ±zÄ± otomatik enstrÃ¼mantasyon ile Ã§alÄ±ÅŸtÄ±rÄ±n:

```bash
OTEL_RESOURCE_ATTRIBUTES="service.name=<servis_adi>" \
OTEL_EXPORTER_OTLP_ENDPOINT="https://ingest.<bolge>.signoz.cloud:443" \
OTEL_EXPORTER_OTLP_HEADERS="signoz-ingestion-key=<ingestion_anahtariniz>" \
OTEL_EXPORTER_OTLP_PROTOCOL=grpc \
OTEL_TRACES_EXPORTER=otlp \
OTEL_METRICS_EXPORTER=otlp \
OTEL_LOGS_EXPORTER=otlp \
OTEL_PYTHON_LOG_CORRELATION=true \
OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED=true \
opentelemetry-instrument <calistirma_komutunuz>
```

-   `<servis_adi>` servisinizin adÄ±dÄ±r.
-   `<bolge>` kÄ±smÄ±nÄ± SigNoz Cloud [bÃ¶lgenizle](https://signoz.io/docs/ingestion/signoz-cloud/overview/#endpoint) eÅŸleÅŸecek ÅŸekilde ayarlayÄ±n.
-   `<ingestion_anahtariniz>` kÄ±smÄ±nÄ± SigNoz [ingestion anahtarÄ±nÄ±zla](https://signoz.io/docs/ingestion/signoz-cloud/keys/) deÄŸiÅŸtirin.
-   `<calistirma_komutunuz>` kÄ±smÄ±nÄ± uygulamanÄ±zÄ± Ã§alÄ±ÅŸtÄ±rmak iÃ§in kullanacaÄŸÄ±nÄ±z gerÃ§ek komutla deÄŸiÅŸtirin. Ã–rneÄŸin: `python main.py`

> ğŸ“Œ Not: Kendi kendine barÄ±ndÄ±rÄ±lan (self-hosted) SigNoz mu kullanÄ±yorsunuz? Ã‡oÄŸu adÄ±m Ã¶zdeÅŸtir. Bu kÄ±lavuzu uyarlamak iÃ§in, [Bulut â†’ Kendi Kendine BarÄ±ndÄ±rma](https://signoz.io/docs/ingestion/cloud-vs-self-hosted/#cloud-to-self-hosted) kÄ±smÄ±nda gÃ¶sterildiÄŸi gibi uÃ§ noktayÄ± gÃ¼ncelleyin ve ingestion anahtarÄ± baÅŸlÄ±ÄŸÄ±nÄ± kaldÄ±rÄ±n.

ArtÄ±k LlamaIndex uygulamanÄ±z tarafÄ±ndan otomatik veya manuel olarak dÄ±ÅŸa aktarÄ±lan tÃ¼m izleri, gÃ¼nlÃ¼kleri ve metrikleri gÃ¶rebileceksiniz.

![SigNoz AyrÄ±ntÄ±lÄ± Ä°z GÃ¶rÃ¼nÃ¼mÃ¼](https://signoz.io/img/docs/llm/llamaindex/llamaindex-detailed-trace-view.webp)

#### Ã–rnek KÄ±lavuzlar

-   [SigNoz LlamaIndex Entegrasyon DÃ¶kÃ¼manlarÄ±](https://signoz.io/docs/llamaindex-observability/)
-   [SigNoz LlamaIndex Soru-Cevap RAG Demosu](https://github.com/SigNoz/llamaindex-rag-opentelemetry-demo)

### Weights and Biases (W&B) Weave

[W&B Weave](https://weave-docs.wandb.ai/), LLM uygulamalarÄ±nÄ± izlemek, denemek, deÄŸerlendirmek, yayÄ±na almak ve iyileÅŸtirmek iÃ§in kullanÄ±lan bir Ã§erÃ§evedir. Ã–lÃ§eklenebilirlik ve esneklik iÃ§in tasarlanan Weave, uygulama geliÅŸtirme iÅŸ akÄ±ÅŸÄ±nÄ±zÄ±n her aÅŸamasÄ±nÄ± destekler.

#### KullanÄ±m KalÄ±bÄ±

Entegrasyon, span'larÄ±/olaylarÄ± Weave Ã§aÄŸrÄ±larÄ± olarak kaydetmek iÃ§in LlamaIndex'in [`instrumentation` modÃ¼lÃ¼nÃ¼](/python/framework/module_guides/observability/instrumentation) kullanÄ±r. VarsayÄ±lan olarak Weave, [yaygÄ±n LLM kÃ¼tÃ¼phanelerine ve Ã§erÃ§evelerine](https://weave-docs.wandb.ai/guides/integrations/) yapÄ±lan Ã§aÄŸrÄ±larÄ± otomatik olarak yamalar (patch) ve izler.

`weave` kÃ¼tÃ¼phanesini yÃ¼kleyin:

```bash
pip install weave
```

Bir W&B API AnahtarÄ± edinin:

HenÃ¼z bir W&B hesabÄ±nÄ±z yoksa, [https://wandb.ai](https://wandb.ai) adresini ziyaret ederek bir tane oluÅŸturun ve API anahtarÄ±nÄ±zÄ± [https://wandb.ai/authorize](https://wandb.ai/authorize) adresinden kopyalayÄ±n. Kimlik doÄŸrulamasÄ± istendiÄŸinde API anahtarÄ±nÄ± girin.

```python
import weave
from llama_index.llms.openai import OpenAI

# Proje adÄ±nÄ±zla Weave'i baÅŸlatÄ±n
weave.init("llamaindex-demo")

# TÃ¼m LlamaIndex iÅŸlemleri artÄ±k otomatik olarak izleniyor
llm = OpenAI(model="gpt-4o-mini")
response = llm.complete("William Shakespeare ÅŸÃ¶yledir: ")
print(response)
```

![weave baÅŸlangÄ±Ã§](./../../_static/integrations/weave/weave_quickstart.png)

Ä°zler; yÃ¼rÃ¼tme sÃ¼resini, token kullanÄ±mÄ±nÄ±, maliyeti, giriÅŸleri/Ã§Ä±kÄ±ÅŸlarÄ±, hatalarÄ±, iÃ§ iÃ§e geÃ§miÅŸ iÅŸlemleri ve akÄ±ÅŸ verilerini iÃ§erir. Weave izleme konusunda yeniyseniz, nasÄ±l gezinileceÄŸi hakkÄ±nda daha fazlasÄ±nÄ± [buradan](https://weave-docs.wandb.ai/guides/tracking/trace-tree) Ã¶ÄŸrenebilirsiniz.

Ä°zlenmeyen Ã¶zel bir fonksiyonunuz varsa, onu [`@weave.op()`](https://weave-docs.wandb.ai/guides/tracking/ops) ile dekore edin.

`weave.init` iÃ§indeki `autopatch_settings` argÃ¼manÄ±nÄ± kullanarak yama davranÄ±ÅŸÄ±nÄ± da kontrol edebilirsiniz. Ã–rneÄŸin bir kÃ¼tÃ¼phaneyi/Ã§erÃ§eveyi izlemek istemiyorsanÄ±z ÅŸu ÅŸekilde kapatabilirsiniz:

```python
weave.init(..., autopatch_settings={"openai": {"enabled": False}})
```

Herhangi bir ek LlamaIndex yapÄ±landÄ±rmasÄ± gerekmez; izleme `weave.init()` Ã§aÄŸrÄ±ldÄ±ÄŸÄ± anda baÅŸlar.

#### KÄ±lavuzlar

LlamaIndex ile entegrasyon, LlamaIndex'in neredeyse her bileÅŸenini (akÄ±ÅŸ/asenkron, tamamlamalar, sohbet, araÃ§ Ã§aÄŸÄ±rma, ajanlar, iÅŸ akÄ±ÅŸlarÄ± ve RAG desteÄŸi) destekler. Resmi [W&B Weave Ã— LlamaIndex](https://weave-docs.wandb.ai/guides/integrations/llamaindex) dÃ¶kÃ¼mantasyonunda daha fazlasÄ±nÄ± Ã¶ÄŸrenebilirsiniz.

### MLflow

[MLflow](https://mlflow.org/docs/latest/llms/tracing/index.html), makine Ã¶ÄŸrenimi projeleri iÃ§in her aÅŸamanÄ±n yÃ¶netilebilir, izlenebilir ve yeniden Ã¼retilebilir olmasÄ±nÄ± saÄŸlayan, tam yaÅŸam dÃ¶ngÃ¼sÃ¼ne odaklanan aÃ§Ä±k kaynaklÄ± bir MLOps/LLMOps platformudur.
**MLflow Tracing**, OpenTelemetry tabanlÄ± bir izleme Ã¶zelliÄŸidir ve LlamaIndex uygulamalarÄ± iÃ§in tek tÄ±kla enstrÃ¼mantasyonu destekler.

#### KullanÄ±m KalÄ±bÄ±

MLflow aÃ§Ä±k kaynaklÄ± olduÄŸu iÃ§in, herhangi bir hesap oluÅŸturmadan veya API anahtarÄ± kurulumu yapmadan kullanmaya baÅŸlayabilirsiniz. MLflow paketini yÃ¼kledikten sonra doÄŸrudan koda geÃ§in!

```python
import mlflow

mlflow.llama_index.autolog()  # MLflow izlemeyi etkinleÅŸtir
```

![](./../../_static/integrations/mlflow/mlflow.gif)

#### KÄ±lavuzlar

MLflow LlamaIndex entegrasyonu ayrÄ±ca deney takibi, deÄŸerlendirme, baÄŸÄ±mlÄ±lÄ±k yÃ¶netimi ve daha fazlasÄ±nÄ± sunar. Daha fazla detay iÃ§in [MLflow dÃ¶kÃ¼mantasyonuna](https://mlflow.org/docs/latest/llms/llama-index/index.html) gÃ¶z atÄ±n.

#### Destek Tablosu

MLflow Tracing, LlamaIndex Ã¶zelliklerinin tamamÄ±nÄ± destekler. [AgentWorkflow](https://www.llamaindex.ai/blog/introducing-agentworkflow-a-powerful-system-for-building-ai-agent-systems) gibi bazÄ± yeni Ã¶zellikler MLflow >= 2.18.0 gerektirir.

| AkÄ±ÅŸ (Streaming) | Asenkron (Async) | Motor (Engine) | Ajanlar | Ä°ÅŸ AkÄ±ÅŸÄ± (Workflow) | AgentWorkflow |
| :--- | :--- | :--- | :--- | :--- | :--- |
| âœ… | âœ… | âœ… | âœ… | âœ… (>= 2.18) | âœ… (>= 2.18) |

### OpenLLMetry

[OpenLLMetry](https://github.com/traceloop/openllmetry), LLM uygulamalarÄ±nÄ± izlemek iÃ§in OpenTelemetry tabanlÄ± aÃ§Ä±k kaynaklÄ± bir objedir. [TÃ¼m bÃ¼yÃ¼k gÃ¶zlemlenebilirlik platformlarÄ±na](https://www.traceloop.com/docs/openllmetry/integrations/introduction) baÄŸlanÄ±r ve dakikalar iÃ§inde kurulur.

#### KullanÄ±m KalÄ±bÄ±

```python
from traceloop.sdk import Traceloop

Traceloop.init()
```

#### KÄ±lavuzlar

-   [OpenLLMetry](/python/examples/observability/openllmetry)

![](./../../_static/integrations/openllmetry.png)

### Arize Phoenix (yerel)

AÃ§Ä±k kaynaklÄ± proje aracÄ±lÄ±ÄŸÄ±yla Phoenix'in **yerel** bir Ã¶rneÄŸini kullanmayÄ± da seÃ§ebilirsiniz.

Bu durumda LlamaTrace'te bir hesap oluÅŸturmanÄ±za veya Phoenix iÃ§in bir API anahtarÄ± ayarlamanÄ±za gerek yoktur. Phoenix sunucusu yerel olarak baÅŸlatÄ±lacaktÄ±r.

#### KullanÄ±m KalÄ±bÄ±

Entegrasyon paketini yÃ¼klemek iÃ§in `pip install -U llama-index-callbacks-arize-phoenix` komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.

ArdÄ±ndan aÅŸaÄŸÄ±daki kodu Ã§alÄ±ÅŸtÄ±rÄ±n:

```python
# Phoenix, LlamaIndex uygulamanÄ±zdan otomatik olarak toplanan izleri 
# gerÃ§ek zamanlÄ± olarak gÃ¶rÃ¼ntÃ¼leyebilir.
# TÃ¼m LlamaIndex uygulamalarÄ±nÄ±zÄ± her zamanki gibi Ã§alÄ±ÅŸtÄ±rÄ±n; izler 
# toplanacak ve Phoenix'te gÃ¶rÃ¼ntÃ¼lenecektir.

import phoenix as px

# UygulamayÄ± tarayÄ±cÄ±da aÃ§mak iÃ§in Ã§Ä±ktÄ±daki URL'ye bakÄ±n.
px.launch_app()
# Uygulama baÅŸlangÄ±Ã§ta boÅŸtur, ancak aÅŸaÄŸÄ±daki adÄ±mlarla devam ettikÃ§e, 
# LlamaIndex uygulamanÄ±z Ã§alÄ±ÅŸtÄ±kÃ§a izler otomatik olarak gÃ¶rÃ¼necektir.

import llama_index.core

llama_index.core.set_global_handler("arize_phoenix")
...
```

#### Ã–rnek KÄ±lavuzlar

-   [Pinecone ve Arize Phoenix ile Otomatik Getirme (Auto-Retrieval) KÄ±lavuzu](https://docs.llamaindex.ai/en/latest/examples/vector_stores/pinecone_auto_retriever/?h=phoenix)
-   [Arize Phoenix Ä°zleme EÄŸitimi](https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb)

### Langfuse ğŸª¢

[Langfuse](https://langfuse.com/docs), ekiplerin LLM uygulamalarÄ± Ã¼zerinde iÅŸ birliÄŸi iÃ§inde hata ayÄ±klamasÄ±na, analiz etmesine ve yineleme yapmasÄ±na yardÄ±mcÄ± olan aÃ§Ä±k kaynaklÄ± bir LLM mÃ¼hendislik platformudur. Langfuse entegrasyonu ile LlamaIndex uygulamanÄ±zÄ±n performansÄ±nÄ±, izlerini ve metriklerini takip edebilir ve izleyebilirsiniz. BaÄŸlam zenginleÅŸtirme (context augmentation) ve LLM sorgulama sÃ¼reÃ§lerinin ayrÄ±ntÄ±lÄ± [izleri](https://langfuse.com/docs/tracing) yakalanÄ±r ve doÄŸrudan Langfuse kullanÄ±cÄ± arayÃ¼zÃ¼nde incelenebilir.

#### KullanÄ±m KalÄ±bÄ±

Hem `llama-index` hem de `langfuse` paketlerinin yÃ¼klÃ¼ olduÄŸundan emin olun.

```bash
pip install llama-index langfuse openinference-instrumentation-llama-index
```

ArdÄ±ndan, Langfuse API anahtarlarÄ±nÄ±zÄ± ayarlayÄ±n. Bu anahtarlarÄ± Ã¼cretsiz bir [Langfuse Bulut](https://cloud.langfuse.com/) hesabÄ± oluÅŸturarak veya [kendiniz barÄ±ndÄ±rarak (self-hosting)](https://langfuse.com/self-hosting) alabilirsiniz. Bu ortam deÄŸiÅŸkenleri, Langfuse istemcisinin kimlik doÄŸrulamasÄ± yapmasÄ± ve Langfuse projenize veri gÃ¶ndermesi iÃ§in gereklidir.

```python
import os

# Projeniz iÃ§in anahtarlarÄ± proje ayarlarÄ± sayfasÄ±ndan alÄ±n: https://cloud.langfuse.com

os.environ["LANGFUSE_PUBLIC_KEY"] = "pk-lf-..."
os.environ["LANGFUSE_SECRET_KEY"] = "sk-lf-..."
os.environ["LANGFUSE_HOST"] = "https://cloud.langfuse.com"  # ğŸ‡ªğŸ‡º AB bÃ¶lgesi
# os.environ["LANGFUSE_HOST"] = "https://us.cloud.langfuse.com" # ğŸ‡ºğŸ‡¸ ABD bÃ¶lgesi
```

Ortam deÄŸiÅŸkenleri ayarlandÄ±ktan sonra Langfuse istemcisini baÅŸlatabiliriz. `get_client()`, ortam deÄŸiÅŸkenlerinde saÄŸlanan kimlik bilgilerini kullanarak Langfuse istemcisini baÅŸlatÄ±r.

```python
from langfuse import get_client

langfuse = get_client()

# BaÄŸlantÄ±yÄ± doÄŸrula
if langfuse.auth_check():
    print("Langfuse istemcesi doÄŸrulandÄ± ve hazÄ±r!")
else:
    print("Kimlik doÄŸrulamasÄ± baÅŸarÄ±sÄ±z oldu. LÃ¼tfen bilgilerinizi ve host adresini kontrol edin.")
```

Åimdi, [OpenInference LlamaIndex enstrÃ¼mantasyonunu](https://docs.arize.com/phoenix/tracing/integrations-tracing/llamaindex) baÅŸlatÄ±yoruz. Bu Ã¼Ã§Ã¼ncÃ¼ taraf enstrÃ¼mantasyon, LlamaIndex iÅŸlemlerini otomatik olarak yakalar ve OpenTelemetry (OTel) span'larÄ±nÄ± Langfuse'a aktarÄ±r.

```python
from openinference.instrumentation.llama_index import LlamaIndexInstrumentor

# LlamaIndex enstrÃ¼mantasyonunu baÅŸlat
LlamaIndexInstrumentor().instrument()
```

ArtÄ±k LlamaIndex uygulamanÄ±zÄ±n gÃ¼nlÃ¼klerini Langfuse'da gÃ¶rebilirsiniz:

[LlamaIndex Ã¶rnek iz](https://langfuse.com/images/cookbook/integration-llamaindex-workflows/llamaindex-trace.gif)

#### Ã–rnek KÄ±lavuzlar

-   [Langfuse DÃ¶kÃ¼mantasyonu](https://langfuse.com/docs/integrations/llama-index/get-started)
-   [LlamaIndex AjanlarÄ±nÄ± Ä°zleme](https://langfuse.com/docs/integrations/llama-index/workflows)

### Literal AI

[Literal AI](https://literalai.com/), mÃ¼hendislik ve Ã¼rÃ¼n ekiplerinin LLM uygulamalarÄ±nÄ± gÃ¼venilir, daha hÄ±zlÄ± ve Ã¶lÃ§eklenebilir ÅŸekilde sunmalarÄ±nÄ± saÄŸlayan bir LLM deÄŸerlendirme ve gÃ¶zlemlenebilirlik Ã§Ã¶zÃ¼mÃ¼dÃ¼r. Bu, istem mÃ¼hendisliÄŸi, LLM gÃ¶zlemlenebilirliÄŸi, LLM deÄŸerlendirmesi ve LLM izlemeyi iÃ§eren iÅŸ birlikÃ§i bir geliÅŸtirme dÃ¶ngÃ¼sÃ¼ ile mÃ¼mkÃ¼ndÃ¼r. KonuÅŸma AkÄ±ÅŸlarÄ± (Threads) ve Ajan Ã‡alÄ±ÅŸtÄ±rmalarÄ± Literal AI Ã¼zerinde otomatik olarak gÃ¼nlÃ¼klenebilir.

Literal AI'yÄ± denemenin en kolay yolu [bulut Ã¶rneÄŸimize](https://cloud.getliteral.ai/) kaydolmaktÄ±r. ArdÄ±ndan **Ayarlar**'a gidip API anahtarÄ±nÄ±zÄ± alabilir ve gÃ¼nlÃ¼klemeye baÅŸlayabilirsiniz!

#### KullanÄ±m KalÄ±bÄ±

-   `pip install literalai` ile Literal AI Python SDK'sÄ±nÄ± yÃ¼kleyin.
-   Literal AI projenizde **Ayarlar**'a gidin ve API anahtarÄ±nÄ±zÄ± alÄ±n.
-   Kendi kendine barÄ±ndÄ±rÄ±lan bir Literal AI Ã¶rneÄŸi kullanÄ±yorsanÄ±z, temel URL'sini de not edin.

ArdÄ±ndan uygulama kodunuza aÅŸaÄŸÄ±daki satÄ±rlarÄ± ekleyin:

```python
from llama_index.core import set_global_handler

# Literal AI API anahtarÄ±nÄ±zÄ± ve temel URL'nizi aÅŸaÄŸÄ±daki ortam deÄŸiÅŸkenlerini kullanarak saÄŸlamalÄ±sÄ±nÄ±z:
# LITERAL_API_KEY, LITERAL_API_URL
set_global_handler("literalai")
```

#### Ã–rnek KÄ±lavuzlar

-   [Literal AI ile Llama Index Entegrasyonu](https://docs.getliteral.ai/integrations/llama-index)
-   [LlamaIndex ile bir Soru-Cevap uygulamasÄ± oluÅŸturun ve bunu Literal AI ile izleyin](https://github.com/Chainlit/literal-cookbook/blob/main/python/llamaindex-integration)

### Comet Opik

[Opik](https://www.comet.com/docs/opik/?utm_source=llama-index&utm_medium=docs&utm_campaign=opik&utm_content=home_page), Comet tarafÄ±ndan geliÅŸtirilen aÃ§Ä±k kaynaklÄ± uÃ§tan uca bir LLM DeÄŸerlendirme Platformudur.

BaÅŸlamak iÃ§in sadece [Comet](https://www.comet.com/signup?from=llm&utm_medium=github&utm_source=llama-index&utm_campaign=opik) Ã¼zerinde bir hesap oluÅŸturun ve API anahtarÄ±nÄ±zÄ± alÄ±n.

#### KullanÄ±m KalÄ±bÄ±

-   `pip install opik` ile Opik Python SDK'sÄ±nÄ± yÃ¼kleyin.
-   Opik'te kullanÄ±cÄ± menÃ¼sÃ¼nden API anahtarÄ±nÄ±zÄ± alÄ±n.
-   Kendi kendine barÄ±ndÄ±rÄ±lan bir Opik Ã¶rneÄŸi kullanÄ±yorsanÄ±z, temel URL'sini de not edin.

[Kendi kendine barÄ±ndÄ±rÄ±lan bir Ã¶rnek](https://www.comet.com/docs/opik/self-host/self_hosting_opik) kullanÄ±yorsanÄ±z `OPIK_API_KEY`, `OPIK_WORKSPACE` ve `OPIK_URL_OVERRIDE` ortam deÄŸiÅŸkenlerini kullanarak Opik'i yapÄ±landÄ±rabilirsiniz:

```bash
export OPIK_API_KEY="<OPIK_API_KEY>"
export OPIK_WORKSPACE="<OPIK_WORKSPACE - Genellikle API anahtarÄ±nÄ±zla aynÄ±dÄ±r>"

# Ä°steÄŸe baÄŸlÄ±
#export OPIK_URL_OVERRIDE="<OPIK_URL_OVERRIDE>"
```

ArtÄ±k kÃ¼resel iÅŸleyiciyi ayarlayarak Opik entegrasyonunu LlamaIndex ile kullanabilirsiniz:

```python
from llama_index.core import Document, VectorStoreIndex, set_global_handler

# OPIK API anahtarÄ±nÄ±zÄ± ve Ã‡alÄ±ÅŸma AlanÄ±nÄ±zÄ± (Workspace) aÅŸaÄŸÄ±daki ortam deÄŸiÅŸkenleri ile saÄŸlamalÄ±sÄ±nÄ±z:
# OPIK_API_KEY, OPIK_WORKSPACE
set_global_handler(
    "opik",
)

# Bu Ã¶rnek varsayÄ±lan olarak OpenAI kullanÄ±r, bu yÃ¼zden bir OPENAI_API_KEY ayarlamayÄ± unutmayÄ±n
index = VectorStoreIndex.from_documents([Document.example()])
query_engine = index.as_query_engine()

questions = [
    "Bana LLM'lerden bahset",
    "Bir sinir aÄŸÄ± nasÄ±l ince ayar (fine-tune) yapÄ±lÄ±r?",
    "RAG nedir?",
]

for question in questions:
    print(f"> \033[92m{question}\033[0m")
    response = query_engine.query(question)
    print(response)
```

Opik'te ÅŸu izleri gÃ¶receksiniz:

![Opik LlamaIndex Entegrasyonu](./../../_static/integrations/opik.png)

#### Ã–rnek KÄ±lavuzlar

-   [Llama-index + Opik dÃ¶kÃ¼mantasyon sayfasÄ±](https://www.comet.com/docs/opik/tracing/integrations/llama_index?utm_source=llamaindex&utm_medium=docs&utm_campaign=opik)
-   [Llama-index entegrasyon tarif kitabÄ± (cookbook)](https://www.comet.com/docs/opik/cookbook/llama-index?utm_source=llama-index&utm_medium=docs&utm_campaign=opik)

### Argilla

[Argilla](https://github.com/argilla-io/argilla), projeleri iÃ§in yÃ¼ksek kaliteli veri kÃ¼meleri oluÅŸturmasÄ± gereken AI mÃ¼hendisleri ve alan uzmanlarÄ± iÃ§in bir iÅŸ birliÄŸi aracÄ±dÄ±r.

BaÅŸlamak iÃ§in Argilla sunucusunu kurmanÄ±z gerekir. HenÃ¼z yapmadÄ±ysanÄ±z, bu [kÄ±lavuzu](https://docs.argilla.io/latest/getting_started/quickstart/) izleyerek kolayca kurabilirsiniz.

#### KullanÄ±m KalÄ±bÄ±

-   `pip install argilla-llama-index` ile Argilla LlamaIndex entegrasyon paketini yÃ¼kleyin.
-   ArgillaHandler'Ä± baÅŸlatÄ±n. `<api_key>` Argilla AlanÄ±nÄ±zÄ±n `My Settings` sayfasÄ±ndadÄ±r, ancak AlanÄ± oluÅŸturmak iÃ§in kullandÄ±ÄŸÄ±nÄ±z `owner` hesabÄ±yla giriÅŸ yaptÄ±ÄŸÄ±nÄ±zdan emin olun. `<api_url>` tarayÄ±cÄ±nÄ±zda gÃ¶sterilen URL'dir.
-   ArgillaHandler'Ä± dispatcher'a ekleyin.

```python
from llama_index.core.instrumentation import get_dispatcher
from argilla_llama_index import ArgillaHandler

argilla_handler = ArgillaHandler(
    dataset_name="query_llama_index",
    api_url="http://localhost:6900",
    api_key="argilla.apikey",
    number_of_retrievals=2,
)
root_dispatcher = get_dispatcher()
root_dispatcher.add_span_handler(argilla_handler)
root_dispatcher.add_event_handler(argilla_handler)
```

#### Ã–rnek KÄ±lavuzlar

-   [Argilla LlamaIndex Entegrasyonuna BaÅŸlarken](https://github.com/argilla-io/argilla-llama-index/blob/main/docs/tutorials/getting_started.ipynb)
-   [DiÄŸer Ã¶rnek eÄŸitimler](https://github.com/argilla-io/argilla-llama-index/tree/main/docs/tutorials)

![Argilla LlamaIndex Entegrasyonu](./../../_static/integrations/argilla.png)

### Agenta

[Agenta](https://agenta.ai), geliÅŸtiricilerin ve Ã¼rÃ¼n ekiplerinin LLM'ler tarafÄ±ndan desteklenen saÄŸlam AI uygulamalarÄ± oluÅŸturmasÄ±na yardÄ±mcÄ± olan **aÃ§Ä±k kaynaklÄ±** bir LLMOps platformudur. **GÃ¶zlemlenebilirlik**, **istem yÃ¶netimi ve mÃ¼hendisliÄŸi** ile **LLM deÄŸerlendirmesi** iÃ§in tÃ¼m araÃ§larÄ± sunar.

#### KullanÄ±m KalÄ±bÄ±

Entegrasyon iÃ§in gerekli baÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kleyin:

```bash
pip install agenta llama-index openinference-instrumentation-llama-index
```

API kimlik bilgilerinizi ayarlayÄ±n ve Agenta'yÄ± baÅŸlatÄ±n:

```python
import os
import agenta as ag
from openinference.instrumentation.llama_index import LlamaIndexInstrumentor

# Agenta kimlik bilgilerinizi ayarlayÄ±n
os.environ["AGENTA_API_KEY"] = "agenta_api_anahtariniz"
os.environ["AGENTA_HOST"] = "https://cloud.agenta.ai"  # Varsa kendi barÄ±ndÄ±rdÄ±ÄŸÄ±nÄ±z URL'yi kullanÄ±n

# Agenta SDK'yÄ± baÅŸlat
ag.init()

# LlamaIndex enstrÃ¼mantasyonunu etkinleÅŸtir
LlamaIndexInstrumentor().instrument()
```

EnstrÃ¼mante edilmiÅŸ uygulamanÄ±zÄ± oluÅŸturun:

```python
@ag.instrument()
def document_search_app(user_query: str):
    """
    LlamaIndex kullanan dÃ¶kÃ¼man arama uygulamasÄ±.
    DÃ¶kÃ¼manlarÄ± yÃ¼kler, aranabilir bir indeks oluÅŸturur ve kullanÄ±cÄ± sorgularÄ±nÄ± yanÄ±tlar.
    """
    # Yerel dizinden dÃ¶kÃ¼manlarÄ± yÃ¼kle
    docs = SimpleDirectoryReader("data").load_data()

    # VektÃ¶r arama indeksi oluÅŸtur
    search_index = VectorStoreIndex.from_documents(docs)

    # Sorgu iÅŸlemciyi baÅŸlat
    query_processor = search_index.as_query_engine()

    # KullanÄ±cÄ± sorgusunu iÅŸle
    answer = query_processor.query(user_query)

    return answer
```

Bu kurulum yapÄ±ldÄ±ktan sonra Agenta tÃ¼m yÃ¼rÃ¼tme adÄ±mlarÄ±nÄ± otomatik olarak yakalayacaktÄ±r. ArdÄ±ndan uygulamanÄ±zda hata ayÄ±klamak, bunlarÄ± belirli yapÄ±landÄ±rmalara ve istemlere baÄŸlamak, performanslarÄ±nÄ± deÄŸerlendirmek, verileri sorgulamak ve temel metrikleri izlemek iÃ§in Agenta'daki izleri gÃ¶rÃ¼ntÃ¼leyebilirsiniz.

![Agenta LlamaIndex Entegrasyonu](./../../_static/integrations/agenta.png)

#### Ã–rnek KÄ±lavuzlar

-   [Agenta ile LlamaIndex iÃ§in GÃ¶zlemlenebilirlik DÃ¶kÃ¼mantasyonu](https://docs.agenta.ai/observability/integrations/llamaindex)
-   [Agenta ile LlamaIndex iÃ§in Notebook GÃ¶zlemlenebilirliÄŸi](https://github.com/agenta-ai/agenta/blob/main/examples/jupyter/integrations/observability-openinference-llamaindex.ipynb)

### Deepeval

[DeepEval (Confident AI tarafÄ±ndan)](https://github.com/confident-ai/deepeval), LLM uygulamalarÄ± iÃ§in aÃ§Ä±k kaynaklÄ± bir deÄŸerlendirme Ã§erÃ§evesidir. LLM uygulamanÄ±zÄ± DeepEval'in sunduÄŸu 14'ten fazla varsayÄ±lan metrik (Ã¶zetleme, halÃ¼sinasyon, yanÄ±t uygunluÄŸu, sadakat, RAGAS vb.) kullanarak "birim testine" tabi tutarken, LlamaIndex ile bu izleme entegrasyonu aracÄ±lÄ±ÄŸÄ±yla baÅŸarÄ±sÄ±z test durumlarÄ±nda hata ayÄ±klayabilir veya DeepEval'in Ã¼retimde referanssÄ±z deÄŸerlendirmeler yapan barÄ±ndÄ±rÄ±lan deÄŸerlendirme platformu [Confident AI](https://documentation.confident-ai.com/docs) aracÄ±lÄ±ÄŸÄ±yla **Ã¼retimdeki** yetersiz deÄŸerlendirmeleri inceleyebilirsiniz.

#### KullanÄ±m KalÄ±bÄ±

```bash
pip install -U deepeval llama-index
```

```python
import deepeval
from deepeval.integrations.llama_index import instrument_llama_index

import llama_index.core.instrumentation as instrument

# GiriÅŸ yap
deepeval.login("<confident-api-anahtariniz>")

# DeepEval'in izleri toplamasÄ±nÄ± saÄŸla
instrument_llama_index(instrument.get_dispatcher())
```

![tracing](https://confident-bucket.s3.us-east-1.amazonaws.com/llama-index%3Atrace.gif)

#### KÄ±lavuzlar

-   [Llama Index AjanlarÄ±nÄ± DeÄŸerlendirme](https://deepeval.com/integrations/frameworks/langchain)
-   [Llama Index AjanlarÄ±nÄ± Ä°zleme](https://documentation.confident-ai.com/docs/llm-tracing/integrations/llamaindex)

### Maxim AI

[Maxim AI](https://www.getmaxim.ai/), geliÅŸtiricilerin LLM uygulamalarÄ±nÄ± oluÅŸturmalarÄ±na, izlemelerine ve iyileÅŸtirmelerine yardÄ±mcÄ± olan bir Ajan SimÃ¼lasyonu, DeÄŸerlendirme ve GÃ¶zlemlenebilirlik platformudur. Maxim'in LlamaIndex ile entegrasyonu; RAG sistemleriniz, ajanlarÄ±nÄ±z ve diÄŸer LLM iÅŸ akÄ±ÅŸlarÄ±nÄ±z iÃ§in kapsamlÄ± izleme, takip ve deÄŸerlendirme yetenekleri saÄŸlar.

#### KullanÄ±m KalÄ±bÄ±

Gerekli paketleri yÃ¼kleyin:

```bash
pip install maxim-py
```

Ortam deÄŸiÅŸkenlerinizi ayarlayÄ±n:

```python
import os
from dotenv import load_dotenv

# .env dosyasÄ±ndan ortam deÄŸiÅŸkenlerini yÃ¼kle
load_dotenv()

# Ortam deÄŸiÅŸkenlerini al
MAXIM_API_KEY = os.getenv("MAXIM_API_KEY")
MAXIM_LOG_REPO_ID = os.getenv("MAXIM_LOG_REPO_ID")

# Gerekli deÄŸiÅŸkenlerin ayarlandÄ±ÄŸÄ±nÄ± doÄŸrula
if not MAXIM_API_KEY:
    raise ValueError("MAXIM_API_KEY ortam deÄŸiÅŸkeni gereklidir")
if not MAXIM_LOG_REPO_ID:
    raise ValueError("MAXIM_LOG_REPO_ID ortam deÄŸiÅŸkeni gereklidir")
```

Maxim'i baÅŸlatÄ±n ve LlamaIndex'i enstrÃ¼mante edin:

```python
from maxim import Config, Maxim
from maxim.logger import LoggerConfig
from maxim.logger.llamaindex import instrument_llamaindex

# Maxim logger'Ä± baÅŸlat
maxim = Maxim(Config(api_key=os.getenv("MAXIM_API_KEY")))
logger = maxim.logger(LoggerConfig(id=os.getenv("MAXIM_LOG_REPO_ID")))

# LlamaIndex'i Maxim gÃ¶zlemlenebilirliÄŸi ile enstrÃ¼mante et
# GeliÅŸtirme sÄ±rasÄ±nda ayrÄ±ntÄ±lÄ± gÃ¼nlÃ¼kleri gÃ¶rmek iÃ§in debug=True yapÄ±n
instrument_llamaindex(logger, debug=True)

print("âœ… Maxim enstrÃ¼mantasyonu LlamaIndex iÃ§in etkinleÅŸtirildi")
```

ArtÄ±k LlamaIndex uygulamalarÄ±nÄ±z Maxim'e otomatik olarak iz gÃ¶nderecektir:

```python
from llama_index.core.agent import FunctionAgent
from llama_index.core.tools import FunctionTool
from llama_index.llms.openai import OpenAI


# AraÃ§larÄ± tanÄ±mla ve ajan oluÅŸtur
def add_numbers(a: float, b: float) -> float:
    """Ä°ki sayÄ±yÄ± topla."""
    return a + b


add_tool = FunctionTool.from_defaults(fn=add_numbers)
llm = OpenAI(model="gpt-4o-mini", temperature=0)

agent = FunctionAgent(
    tools=[add_tool],
    llm=llm,
    verbose=True,
    system_prompt="Siz yardÄ±mcÄ± bir hesap makinesi asistanÄ±sÄ±nÄ±z.",
)

# Bu iÅŸlem Maxim enstrÃ¼mantasyonu tarafÄ±ndan otomatik olarak gÃ¼nlÃ¼klenecektir
import asyncio

response = await agent.run("15 + 25 kaÃ§tÄ±r?")
print(f"YanÄ±t: {response}")
```

#### KÄ±lavuzlar

-   [Maxim EnstrÃ¼mantasyon Tarif KitabÄ±](/python/examples/observability/maxim-instrumentation)
-   [Maxim AI DÃ¶kÃ¼mantasyonu](https://www.getmaxim.ai/docs/sdk/python/integrations/llamaindex/llamaindex)

![tracing](https://cdn.getmaxim.ai/public/images/llamaindex.gif)

## DiÄŸer Ä°ÅŸ OrtaÄŸÄ± `Tek TÄ±kla` EntegrasyonlarÄ± (Eski ModÃ¼ller)

Bu iÅŸ ortaÄŸÄ± entegrasyonlarÄ± eski `CallbackManager` yapÄ±mÄ±zÄ± veya Ã¼Ã§Ã¼ncÃ¼ taraf Ã§aÄŸrÄ±larÄ±nÄ± kullanÄ±r.

### Langfuse

Bu entegrasyon kullanÄ±mdan kaldÄ±rÄ±lmÄ±ÅŸtÄ±r (deprecated). Langfuse ile [burada](https://langfuse.com/docs/integrations/llama-index/get-started) aÃ§Ä±klandÄ±ÄŸÄ± gibi enstrÃ¼mantasyon tabanlÄ± yeni entegrasyonu kullanmanÄ±zÄ± Ã¶neririz.

#### KullanÄ±m KalÄ±bÄ±

```python
from llama_index.core import set_global_handler

# 'llama-index-callbacks-langfuse' entegrasyon paketini yÃ¼klediÄŸinizden emin olun.

# NOT: 'LANGFUSE_SECRET_KEY', 'LANGFUSE_PUBLIC_KEY' ve 'LANGFUSE_HOST' ortam 
# deÄŸiÅŸkenlerinizi langfuse.com proje ayarlarÄ±nÄ±zda gÃ¶sterildiÄŸi gibi ayarlayÄ±n.

set_global_handler("langfuse")
```

#### KÄ±lavuzlar

-   [Langfuse Callback Ä°ÅŸleyicisi](/python/examples/observability/langfusecallbackhandler)
-   [PostHog ile Langfuse Ä°zleme](/python/examples/observability/langfusemistralposthog)

![langfuse-tracing](https://static.langfuse.com/llamaindex-langfuse-docs.gif)

### OpenInference

[OpenInference](https://github.com/Arize-ai/open-inference-spec), AI model Ã§Ä±karÄ±mlarÄ±nÄ± yakalamak ve depolamak iÃ§in aÃ§Ä±k bir standarttÄ±r. [Phoenix](https://github.com/Arize-ai/phoenix) gibi LLM gÃ¶zlemlenebilirlik Ã§Ã¶zÃ¼mlerini kullanarak LLM uygulamalarÄ±nÄ±n denenmesini, gÃ¶rselleÅŸtirilmesini ve deÄŸerlendirilmesini saÄŸlar.

#### KullanÄ±m KalÄ±bÄ±

```python
import llama_index.core

llama_index.core.set_global_handler("openinference")

# NOT: AÅŸaÄŸÄ±dakileri yapmanÄ±za gerek yoktur
from llama_index.callbacks.openinference import OpenInferenceCallbackHandler
from llama_index.core.callbacks import CallbackManager
from llama_index.core import Settings

# callback_handler = OpenInferenceCallbackHandler()
# Settings.callback_manager = CallbackManager([callback_handler])

# LlamaIndex uygulamanÄ±zÄ± burada Ã§alÄ±ÅŸtÄ±rÄ±n...
for query in queries:
    query_engine.query(query)

# LLM uygulama verilerinizi OpenInference formatÄ±nda bir dataframe olarak gÃ¶rÃ¼ntÃ¼leyin.
from llama_index.core.callbacks.open_inference_callback import as_dataframe

query_data_buffer = llama_index.core.global_handler.flush_query_data_buffer()
query_dataframe = as_dataframe(query_data_buffer)
```

**NOT**: Phoenix yeteneklerini aÃ§mak iÃ§in, sorgu/baÄŸlam dataframe'lerini beslemek Ã¼zere ek adÄ±mlar tanÄ±mlamanÄ±z gerekecektir. AÅŸaÄŸÄ±ya bakÄ±n!

#### KÄ±lavuzlar

-   [OpenInference Callback Ä°ÅŸleyicisi](/python/examples/observability/openinferencecallback)
-   [Arize Phoenix ile Arama ve Getirme Ä°ÅŸlemini DeÄŸerlendirme](https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/llama_index_search_and_retrieval_tutorial.ipynb)

### TruEra TruLens

TruLens; geri bildirim fonksiyonlarÄ± ve izleme gibi Ã¶zellikler aracÄ±lÄ±ÄŸÄ±yla kullanÄ±cÄ±larÄ±n LlamaIndex uygulamalarÄ±nÄ± enstrÃ¼mante etmelerine/deÄŸerlendirmelerine olanak tanÄ±r.

#### KullanÄ±m KalÄ±bÄ± + KÄ±lavuzlar

```python
# trulens kullan
from trulens_eval import TruLlama

tru_query_engine = TruLlama(query_engine)

# sorgula
tru_query_engine.query("Yazar bÃ¼yÃ¼rken ne yaptÄ±?")
```

![](./../../_static/integrations/trulens.png)

#### KÄ±lavuzlar

-   [Trulens KÄ±lavuzu](/python/framework/community/integrations/trulens)
-   [LlamaIndex + TruLens ile HÄ±zlÄ± BaÅŸlangÄ±Ã§ KÄ±lavuzu](https://github.com/truera/trulens/blob/trulens-eval-0.20.3/trulens_eval/examples/quickstart/llama_index_quickstart.ipynb)

### HoneyHive

HoneyHive, kullanÄ±cÄ±larÄ±n herhangi bir LLM iÅŸ akÄ±ÅŸÄ±nÄ±n yÃ¼rÃ¼tme akÄ±ÅŸÄ±nÄ± izlemesine olanak tanÄ±r. KullanÄ±cÄ±lar daha sonra izlerinde hata ayÄ±klayabilir ve analiz edebilir veya Ã¼retimden deÄŸerlendirme/ince ayar veri kÃ¼meleri oluÅŸturmak iÃ§in belirli iz olaylarÄ±ndaki geri bildirimleri Ã¶zelleÅŸtirebilirler.

#### KullanÄ±m KalÄ±bÄ±

```python
from llama_index.core import set_global_handler

set_global_handler(
    "honeyhive",
    project="Benim HoneyHive Projem",
    name="LLM Ä°ÅŸ AkÄ±ÅŸÄ± AdÄ±m",
    api_key="HONEYHIVE API ANAHTARIM",
)

# NOT: AÅŸaÄŸÄ±dakileri yapmanÄ±za gerek yoktur
from llama_index.core.callbacks import CallbackManager
from llama_index.core import Settings

# hh_tracer = HoneyHiveLlamaIndexTracer(
#     project="Benim HoneyHive Projem",
#     name="LLM Ä°ÅŸ AkÄ±ÅŸÄ± AdÄ±m",
#     api_key="HONEYHIVE API ANAHTARIM",
# )
# Settings.callback_manager = CallbackManager([hh_tracer])
```

![](./../../_static/integrations/honeyhive.png)
![](./../../_static/integrations/perfetto.png)
_HoneyHive izlerinizi hata ayÄ±klamak ve analiz etmek iÃ§in Perfetto'yu kullanÄ±n_

#### KÄ±lavuzlar

-   [HoneyHive Callback Ä°ÅŸleyicisi](/python/examples/observability/honeyhivellamaindextracer)

### PromptLayer

PromptLayer; LLM Ã§aÄŸrÄ±larÄ±, etiketleme, Ã§eÅŸitli kullanÄ±m durumlarÄ± iÃ§in istemleri analiz etme ve deÄŸerlendirme genelinde analitikleri izlemenize olanak tanÄ±r. RAG istemlerinizin performansÄ±nÄ± ve daha fazlasÄ±nÄ± izlemek iÃ§in LlamaIndex ile birlikte kullanÄ±n.

#### KullanÄ±m KalÄ±bÄ±

```python
import os

os.environ["PROMPTLAYER_API_KEY"] = "api_anahtariniz"

from llama_index.core import set_global_handler

# pl_tags opsiyoneldir, istemlerinizi ve uygulamalarÄ±nÄ±zÄ± dÃ¼zenlemenize yardÄ±mcÄ± olur
set_global_handler("promptlayer", pl_tags=["paul graham", "essay"])
```

#### KÄ±lavuzlar

-   [PromptLayer](/python/examples/observability/promptlayerhandler)

### Langtrace

[Langtrace](https://github.com/Scale3-Labs/langtrace), OpenTelemetry'yi destekleyen ve LLM uygulamalarÄ±nÄ± sorunsuz bir ÅŸekilde izlemek, deÄŸerlendirmek ve yÃ¶netmek iÃ§in tasarlanmÄ±ÅŸ saÄŸlam bir aÃ§Ä±k kaynaklÄ± araÃ§tÄ±r. Langtrace, LlamaIndex ile doÄŸrudan entegre olur; doÄŸruluk, deÄŸerlendirmeler ve gecikme gibi performans metrikleri hakkÄ±nda ayrÄ±ntÄ±lÄ±, gerÃ§ek zamanlÄ± iÃ§gÃ¶rÃ¼ler sunar.

#### YÃ¼kleme

```shell
pip install langtrace-python-sdk
```

#### KullanÄ±m KalÄ±bÄ±

```python
from langtrace_python_sdk import (
    langtrace,
)  # Herhangi bir llm modÃ¼lÃ¼ iÃ§e aktarmasÄ±ndan Ã¶nce gelmelidir

langtrace.init(api_key="<LANGTRACE_API_KEY>")
```

#### KÄ±lavuzlar

-   [Langtrace](https://docs.langtrace.ai/supported-integrations/llm-frameworks/llamaindex)

### OpenLIT

[OpenLIT](https://github.com/openlit/openlit), OpenTelemetry yerel bir Ãœretken Yapay Zeka (GenAI) ve LLM Uygulama GÃ¶zlemlenebilirlik aracÄ±dÄ±r. GÃ¶zlemlenebilirliÄŸin GenAI projelerine entegrasyon sÃ¼recini tek bir satÄ±r kodla gerÃ§ekleÅŸtirmek iÃ§in tasarlanmÄ±ÅŸtÄ±r. OpenLIT; Ã§eÅŸitli LLM'ler, VektÃ¶r VeritabanlarÄ± ve LlamaIndex gibi Ã‡erÃ§eveler iÃ§in OpenTelemetry otomatik enstrÃ¼mantasyonu saÄŸlar. OpenLIT; LLM uygulamalarÄ±nÄ±zÄ±n performansÄ±, isteklerin izlenmesi, maliyetler, tokenlar gibi kullanÄ±m metriklerine dair iÃ§gÃ¶rÃ¼ler ve Ã§ok daha fazlasÄ±nÄ± sunar.

#### YÃ¼kleme

```shell
pip install openlit
```

#### KullanÄ±m KalÄ±bÄ±

```python
import openlit

openlit.init()
```

#### KÄ±lavuzlar

-   [OpenLIT Resmi DÃ¶kÃ¼mantasyonu](https://docs.openlit.io/latest/integrations/llama-index)

### AgentOps

[AgentOps](https://github.com/AgentOps-AI/agentops), geliÅŸtiricilerin AI ajanlarÄ± oluÅŸturmasÄ±na, deÄŸerlendirmesine ve izlemesine yardÄ±mcÄ± olur. AgentOps; prototipten Ã¼retime ajanlar oluÅŸturmaya yardÄ±mcÄ± olur, ajan izleme, LLM maliyet takibi, kÄ±yaslama (benchmarking) ve daha fazlasÄ±nÄ± saÄŸlar.

#### YÃ¼kleme

```shell
pip install llama-index-instrumentation-agentops
```

#### KullanÄ±m KalÄ±bÄ±

```python
from llama_index.core import set_global_handler

# NOT: AgentOps ortam deÄŸiÅŸkenlerinizi (Ã¶rneÄŸin 'AGENTOPS_API_KEY') AgentOps 
# dÃ¶kÃ¼mantasyonunda belirtildiÄŸi gibi ayarlayabilir veya set_global_handler 
# iÃ§indeki **eval_params olarak geÃ§ebilirsiniz.

set_global_handler("agentops")
```

### Basit (Simple - LLM GiriÅŸleri/Ã‡Ä±kÄ±ÅŸlarÄ±)

Bu basit gÃ¶zlemlenebilirlik aracÄ±, her LLM giriÅŸ/Ã§Ä±kÄ±ÅŸ Ã§iftini terminale yazdÄ±rÄ±r. En Ã§ok LLM uygulamanÄ±zda hÄ±zlÄ±ca hata ayÄ±klama (debug) gÃ¼nlÃ¼klerini etkinleÅŸtirmeniz gerektiÄŸinde yararlÄ±dÄ±r.

#### KullanÄ±m KalÄ±bÄ±

```python
import llama_index.core

llama_index.core.set_global_handler("simple")
```

#### KÄ±lavuzlar

-   [MLflow](https://mlflow.org/docs/latest/llms/llama-index/index.html)

## Daha fazla gÃ¶zlemlenebilirlik

-   [Geri Aramalar (Callbacks) KÄ±lavuzu](/python/framework/module_guides/observability/callbacks)